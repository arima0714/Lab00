{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修正したモデルから卒論時に集計したデータを作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 表\n",
    "\n",
    "| ベンチマーク名 | 平均誤差率(%) | コスト比(%) |\n",
    "|---------|----------|---------|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 表\n",
    "\n",
    "| ベンチマーク名 | 採用割合(最大MAPE(%), 最小MAPE(%)) |\n",
    "|---------|----------------------------|\n",
    "|         | モデル(1), モデル(2), ...        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 平均誤差率：大規模実行時の関数コール回数との比較\n",
    "* MAPE：トレーニングデータとの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.xlabel(\"使用したプロファイル数\")\n",
    "plt.ylabel(\"平均誤差率(%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_option(\"display.max_columns\")\n",
    "# pd.get_option(\"display.max_rows\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "benchmark = \"cg\"\n",
    "fix = \"Class\"\n",
    "if benchmark == \"bt\" or benchmark == \"sp\":\n",
    "    processes = processes_onlyBTSP\n",
    "else:\n",
    "    processes = processes_excludeBTSP\n",
    "benchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "targetNumOfProcess = 256\n",
    "fixedBenchmarkClass = \"B\"\n",
    "fixedProcess = 64\n",
    "\n",
    "# 引数の条件に合った生の実験データを取得する\n",
    "DF = returnRawDFperBenchmark(Benchmark=benchmark, fix=fix, benchmarkClass=benchmarkClasses,\n",
    "                             FixedProcess=fixedProcess, Processes=processes, FixedBenchmarkClass=fixedBenchmarkClass)\n",
    "# 取得した生の実験データから NaN が含まれる関数の実験データを削除\n",
    "noNaNDF = DF.dropna(how='any')\n",
    "# noNaNDF\n",
    "\n",
    "returnedCalculatedDF = return_calculatedDF(benchmark=benchmark, noNaNDF=noNaNDF,\n",
    "                                           targetNumOfProcess=targetNumOfProcess, targetProblemSize=fixedBenchmarkClass, fix=fix)\n",
    "# returnedCalculatedDF\n",
    "\n",
    "# numOfData列の要素一覧を作成し、ソートされたリストを、listOfNumDataに格納する\n",
    "listOfNumOfData = returnedCalculatedDF['numOfData'].tolist()\n",
    "listOfNumOfData = sorted(list(set(listOfNumOfData)))\n",
    "# listOfNumOfData\n",
    "\n",
    "\n",
    "# 使用したプロファイル数をキー・最適モデルでの相対誤差の平均をバリューとした辞書を作成する\n",
    "dictAverageRelativeErrorOfBestModel = {}\n",
    "x = []\n",
    "y = []\n",
    "for numOfData in listOfNumOfData:\n",
    "    # 使用したプロファイル数で抽出\n",
    "    extractedPerNumOfProfileDF = returnedCalculatedDF[returnedCalculatedDF['numOfData'] == numOfData]\n",
    "    meanDF = extractedPerNumOfProfileDF.mean()\n",
    "    data = meanDF.at['relativeErrorOfBestModel']\n",
    "    dictAverageRelativeErrorOfBestModel[numOfData] = data\n",
    "\n",
    "x = list(dictAverageRelativeErrorOfBestModel.keys())\n",
    "x\n",
    "y = [dictAverageRelativeErrorOfBestModel[key] for key in x]\n",
    "y\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, marker='o')\n",
    "plt.xlabel(\"使用したプロファイル数[％]\")\n",
    "plt.ylabel(\"平均絶対誤差率\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマーク名・関数名・プロセス数・問題サイズを指定することで、その条件での関数コール回数を取得する関数\n",
    "\n",
    "def returnSpecificData(benchmarkName=\"cg\", functionName=\".TAU_application\", process=256, benchmarkClass=\"D\"):\n",
    "    targetRawDF = returnRawDF(Benchmark=benchmarkName, functionName=functionName, benchmarkClass=[\n",
    "                              benchmarkClass], FixedProcess=process, Processes=[process], FixedBenchmarkClass=benchmarkClass)\n",
    "    return targetRawDF.iat[0, 0]\n",
    "# returnSpecificData(benchmarkName=\"mg\", functionName=\"BUBBLE\", process=256, benchmarkClass=\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarksからbt, spを除外する\n",
    "benchmarks = [benchmark for benchmark in benchmarks if benchmark !=\n",
    "              'bt' and benchmark != 'sp']\n",
    "# pandasのDFをprintした時の幅を広げる\n",
    "pd.set_option('display.width', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictTmp = returnDictForPlotPerNumOfUsedData(Benchmark=benchmarks, fix=\"Class\", benchmarkClass=[\n",
    "    \"A\", \"B\", \"C\", \"D\"], FixedProcess=64, Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4g}'.format\n",
    "\n",
    "tmpDF = pd.DataFrame()\n",
    "for benchmark in benchmarks:\n",
    "    listToLearn = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    listToPredict = [256]\n",
    "    benchmark_x = dictTmp[benchmark][\"x\"]\n",
    "    benchmark_y = dictTmp[benchmark][\"y\"]\n",
    "    index = benchmark_x.index(len(listToLearn))\n",
    "    MAPE = benchmark_y[index]\n",
    "    relativeCost = returnRelativeCost(benchmark=benchmark, variablesToLearn=listToLearn,\n",
    "                                      variablesToPredict=listToPredict, fixedClassOrProcess=\"Class\", fixed=\"C\")\n",
    "    dictRowData = {\"ベンチマーク名\": benchmark.upper(\n",
    "    ), \"平均絶対誤差率[％]\": MAPE, \"相対コスト[％]\": relativeCost}\n",
    "    iDF = pd.DataFrame.from_dict(dictRowData, orient='index').T\n",
    "    tmpDF = tmpDF.append(iDF)\n",
    "tmpDFMean = tmpDF.mean()\n",
    "type(tmpDFMean)\n",
    "print(tmpDF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictTmp\n",
    "\n",
    "plt.figure(figsize=(5.72, 4), dpi=200)\n",
    "for benchmark in list(dictTmp.keys()):\n",
    "    x = dictTmp[benchmark][\"x\"]\n",
    "    y = dictTmp[benchmark][\"y\"]\n",
    "    plt.plot(x, y, marker='o', label=benchmark.upper())\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"使用したプロファイル数\")\n",
    "    plt.ylabel(\"平均絶対誤差率[％]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.72, 4), dpi=200)\n",
    "\n",
    "# Extra-PでfixProcessデータを入力して出力したモデルの図時\n",
    "plot_x = np.linspace(0.8, 256, 500)\n",
    "# -3590464.6990329633 + 3759195.349891038 * p^(1/4)\n",
    "plot_y = []\n",
    "for x in plot_x:\n",
    "    plot_y.append(2286768.3333333326 + 301997.61904761934 * math.log2(x)**(1))\n",
    "plt.plot(plot_x, plot_y, label=\"ExtraP\")\n",
    "\n",
    "x = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "y = [1984770.0, 2263540.0, 2821070.0, 3936140.0,\n",
    "     3936140.0, 3936140.0, 3936140.0, 3936140.0]\n",
    "x = np.array(x).reshape(-1, 1)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "plt.scatter(x, y, marker=\"o\", label=\"予測に用いた関数コール回数\")\n",
    "plot_x = np.array(plot_x).reshape(-1, 1)\n",
    "x_target = [256]\n",
    "y_target = [3936140]\n",
    "plt.scatter(x_target, y_target, marker=\"o\", label=\"予測したい関数コール回数の実測値\")\n",
    "\n",
    "benchmarkName = \"CG\"\n",
    "functionName = \"ICNVRT\"\n",
    "\n",
    "# 線形モデル\n",
    "# 対数モデル\n",
    "\n",
    "# 反比例モデル\n",
    "modelIpMk2 = ModelIp_mk2(train_x=x, train_y=y, target_x=x_target, target_y=y_target,\n",
    "                         benchmark_name=benchmarkName, function_name=functionName)\n",
    "modelIpMk2.calc_lr()\n",
    "plot_y_IpMk2 = modelIpMk2.predict(plot_x)\n",
    "plt.plot(plot_x, plot_y_IpMk2, label=\"反比例モデル\")\n",
    "# 線形飽和モデル\n",
    "modelBranchMk2 = ModelBranch_mk2(train_x=x, train_y=y, target_x=x_target,\n",
    "                                 target_y=y_target, benchmark_name=benchmarkName, function_name=functionName)\n",
    "modelBranchMk2.calc_lr()\n",
    "plot_y_BranchMk2 = modelBranchMk2.predict(plot_x)\n",
    "plt.plot(plot_x, plot_y_BranchMk2, label=\"線形飽和モデル\")\n",
    "# # 線形モデル\n",
    "# model_lin = ModelLin(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_lin.calc_lr()\n",
    "# plot_y_lin = model_lin.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_lin, label=\"線形モデル\")\n",
    "# # 対数モデル\n",
    "# model_log10 = ModelLog10(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_log10.calc_lr()\n",
    "# plot_y_log10 = model_log10.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_log10, label=\"対数モデル\")\n",
    "# # 反比例モデル\n",
    "# model_ip = ModelIP(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_ip.calc_lr()\n",
    "# plot_y_ip = model_ip.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_ip, label=\"反比例モデル\")\n",
    "# # 線形飽和モデル\n",
    "# model_branch = ModelBranch(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_branch.calc_lr()\n",
    "# plot_y_branch = model_branch.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_branch, label=\"線形飽和モデル\")\n",
    "# 凡例の表示\n",
    "plt.legend()\n",
    "# 軸ラベルの設定\n",
    "plt.ylabel(\"関数コール回数\")\n",
    "plt.xlabel(\"実行コア数\")\n",
    "\n",
    "plt.scatter(x, y, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 実際にプロットする\n",
    "\n",
    "\n",
    "# print(f\"fix={fix}, benchmarkClasses={benchmarkClasses}, fixedProcess={fixedProcess}, Processes={processes}, FixedBenchmarkClass={fixedBenchmarkClass}\")\n",
    "# print(f\"targetNumOfProcess={targetNumOfProcess}, targetProblemSize={fixedBenchmarkClass}, fix={fix}\")\n",
    "\n",
    "# DF = returnRawDFperBenchmark(Benchmark=\"mg\", fix=\"Process\", benchmarkClass=[\"A\", \"B\", \"C\", \"D\"], Processes=[\n",
    "#                              1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"B\", FixedProcess=64)\n",
    "# DF.dropna(how='any')\n",
    "# DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmarkNamesExcludeBTSP = [\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"]\n",
    "# classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "classes = [\"B\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "targetIndex = -1\n",
    "csvDirPath = \"./csv_files/\"\n",
    "\n",
    "dfByDatumExcludeBTSP = returnDFSummarizedData(\n",
    "    benchmarkNames=benchmarkNamesExcludeBTSP, classes=classes, processes=processes, targetIndex=targetIndex, csvDirPath=csvDirPath)\n",
    "# dfByDatumExcludeBTSP\n",
    "\n",
    "dictForLatexTable = {}\n",
    "numOfData = 0\n",
    "for benchmarkName in benchmarkNamesExcludeBTSP:\n",
    "    dictForLatexTable[benchmarkName] = dfByDatumExcludeBTSP[dfByDatumExcludeBTSP[\"benchmarkName\"] == benchmarkName]\n",
    "    numOfData += len(\n",
    "        dfByDatumExcludeBTSP[dfByDatumExcludeBTSP[\"benchmarkName\"] == benchmarkName])\n",
    "\n",
    "numOfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "listForDF = []\n",
    "\n",
    "for benchmarkName in benchmarkNamesExcludeBTSP:\n",
    "    listForDF.append(returnSeriesOfDatumPerBenchmark(\n",
    "        inputDF=dictForLatexTable[benchmarkName]))\n",
    "DF = pd.DataFrame(listForDF)\n",
    "print(DF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_returnSeriesOfDatumPerBenchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultIs = dictForLatexTable[\"is\"]\n",
    "# resultIs\n",
    "resultIsAtModelBranch = resultIs[resultIs[\"objectBestModelName\"]\n",
    "                                 == \"ModelBranch\"]\n",
    "datumX = resultIsAtModelBranch[\"usedDataX\"].tolist()\n",
    "datumY = resultIsAtModelBranch[\"usedDataY\"].tolist()\n",
    "\n",
    "datumX\n",
    "datumY\n",
    "\n",
    "# returnSeriesOfData(benchmarkName=\"is\", functionName=\"double_randlc(double_*_double_*)\", rawX=dataX, rawY=dataY, fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultIs = dictForLatexTable[\"ft\"]\n",
    "# resultIs\n",
    "resultIsAtModelBranch = resultIs[resultIs[\"objectBestModelName\"]\n",
    "                                 == \"ModelBranch\"]\n",
    "resultIsAtModelBranchOfNotLowMAPE = resultIsAtModelBranch[\n",
    "    resultIsAtModelBranch[\"MAPEOfBestModel\"] > 1]\n",
    "resultIsAtModelBranchOfNotLowMAPE\n",
    "datumX = resultIsAtModelBranchOfNotLowMAPE[\"usedDataX\"].tolist()\n",
    "datumY = resultIsAtModelBranchOfNotLowMAPE[\"usedDataY\"].tolist()\n",
    "\n",
    "datumX\n",
    "datumY\n",
    "\n",
    "for dataIndex in range(len(datumX)):\n",
    "    plt.figure()\n",
    "    plt.scatter(datumX[dataIndex], datumY[dataIndex])\n",
    "\n",
    "# returnSeriesOfData(benchmarkName=\"is\", functionName=\"double_randlc(double_*_double_*)\", rawX=dataX, rawY=dataY, fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 実験用に小規模なリスト\n",
    "benchmarkNames = [\"cg\"]\n",
    "classes = [\"B\"]\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 生データの取得\n",
    "cgDF = returnCollectedExistingData(benchmarkNames=[\"cg\"], classes=[\"A\", \"B\", \"C\", \"D\"], processes=[\n",
    "                                   1, 2, 4, 8, 16, 32, 64, 128, 256], csvDirPath=\"./csv_files/\")\n",
    "cgDF\n",
    "# ベンチマーククラスがAの情報を取得\n",
    "cgDFfixedA = cgDF[cgDF[\"benchmarkClass\"] == \"A\"]\n",
    "cgDFfixedA\n",
    "# 関数名のリストを取得\n",
    "functionNames = sorted(list(set(cgDFfixedA[\"functionName\"])))\n",
    "print(functionNames)\n",
    "\n",
    "# 関数名を関数名のリストから抽出\n",
    "functionNameCG = cgDFfixedA[cgDFfixedA[\"functionName\"] == \"CG\"]\n",
    "functionNameCG\n",
    "\n",
    "# 説明変数と目的変数とをリスト化したものを抽出\n",
    "# プロセス数\n",
    "raw_x = functionNameCG['process'].tolist()\n",
    "# 関数コール回数\n",
    "raw_y = functionNameCG['functionCallNum'].tolist()\n",
    "\n",
    "print(f\"raw_x={raw_x}\")\n",
    "print(f\"raw_y={raw_y}\")\n",
    "\n",
    "bencmarkName = \"CG\"\n",
    "functionName = \"CG\"\n",
    "fixProcessOrClass = \"Class\"\n",
    "fixed = \"A\"\n",
    "targetProcess = 256\n",
    "targetBenchmarkClass = fixed\n",
    "targetFunctionCallNum = raw_y[-1]\n",
    "returnSeriesOfData(benchmarkName=\"benhmarkName\", functionName=\"functionName\", rawX=[1, 2, 3], rawY=[\n",
    "                   1, 2, 3], fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブック中で変数のみを記述することでデータフレームをきれいに表示させる設定の有効化\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題サイズD, コア数256での関数コール回数を予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO：BT, SP以外のベンチマーク名を入れる\n",
    "benchmarkNames = ['cg', 'ep', 'ft', 'is', 'lu', 'mg']\n",
    "classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "dictForSummarizedResult = {}\n",
    "columnName = [\"benchmarkName\", \"functionName\", \"score\", \"relativeErrorRate\"]\n",
    "dfForSummarizedResult = pd.DataFrame(columns=columnName)\n",
    "for benchmarkName in benchmarkNames:\n",
    "    # ベンチマークごとにscoreを保持するためのリスト\n",
    "    listForSummarizedResultPerBenchmarkName = []\n",
    "    # 学習用生データ\n",
    "    DF = returnCollectedExistingData(benchmarkNames=[\n",
    "                                     benchmarkName], classes=classes, processes=processes, csvDirPath=\"./csv_files/\")\n",
    "    # 重複のない関数名のリスト\n",
    "    functionNames = list(set(DF[\"functionName\"]))\n",
    "    usefulFunctionNames = []\n",
    "    # このループで関数ごとのデータが問題サイズパターン数xコア数パターン数 分だけ存在する関数名のリストを作成する\n",
    "    for functionName in functionNames:\n",
    "        # 関数ごとに生データを集計\n",
    "        dfPerFunction = DF[DF[\"functionName\"] == functionName]\n",
    "        if len(classes) * len(processes) == len(dfPerFunction):\n",
    "            usefulFunctionNames.append(functionName)\n",
    "    if len(usefulFunctionNames) == 0:\n",
    "        continue\n",
    "    # 関数ごとのデータを抽出\n",
    "    for functionName in usefulFunctionNames:\n",
    "        # 問題サイズを数値化したカラムを追加\n",
    "        listBenchmarkClass = DF[\"benchmarkClass\"].tolist()\n",
    "        DFWithNumInBenchmarkClass = DF.assign(\n",
    "            benchmarkClassInNum=convertBenchmarkClasses_problemSizeInNPB(listBenchmarkClass))\n",
    "        # 学習用データ\n",
    "        dfPerFunctionForTrain = DFWithNumInBenchmarkClass[(\n",
    "            DFWithNumInBenchmarkClass[\"functionName\"] == functionName)]\n",
    "        dfPerFunctionForTest = DFWithNumInBenchmarkClass[(DFWithNumInBenchmarkClass[\"functionName\"] == functionName) & (\n",
    "            DFWithNumInBenchmarkClass[\"benchmarkClass\"] == \"D\") & (DFWithNumInBenchmarkClass[\"process\"] == 256)]\n",
    "\n",
    "        # x:説明変数, t:目的変数\n",
    "        trainX = dfPerFunctionForTrain[[\"process\", \"benchmarkClassInNum\"]]\n",
    "        trainT = dfPerFunctionForTrain[[\"functionCallNum\"]]\n",
    "        testX = dfPerFunctionForTest[[\"process\", \"benchmarkClassInNum\"]]\n",
    "        testT = dfPerFunctionForTest[[\"functionCallNum\"]]\n",
    "        # 重回帰分析する\n",
    "        reg_model = LinearRegression()\n",
    "        reg_model.fit(trainX, trainT)\n",
    "        # 関数ごとの結果をベンチマークごとの結果に入れる\n",
    "        scorePerFunction = reg_model.score(trainX, trainT)\n",
    "        listForSummarizedResultPerBenchmarkName.append(scorePerFunction)\n",
    "        # 予測を実施して、相対誤差を算出\n",
    "        predictedTByTestX = reg_model.predict(testX)\n",
    "        predictedData = predictedTByTestX[0][0]\n",
    "        realData = testT[\"functionCallNum\"].tolist()[0]\n",
    "        relativeErrorPerFunction = abs(predictedData - realData)/realData * 100\n",
    "        ##\n",
    "        dfPerFunction = pd.DataFrame(index=columnName, data=[\n",
    "                                     benchmarkName, functionName, scorePerFunction, relativeErrorPerFunction]).T\n",
    "        dfForSummarizedResult = dfForSummarizedResult.append(dfPerFunction)\n",
    "\n",
    "# ( A ~ D ) * (1 ~ 256) のすべての条件を\n",
    "# 満たしていたら、リストに追加\n",
    "# 満たしていなければ、なにもしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForSummarizedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = dfForSummarizedResult\n",
    "\n",
    "\n",
    "benchmarkNamesInDF = list(set(dfForSummarizedResult[\"benchmarkName\"].tolist()))\n",
    "\n",
    "listForLatexTable = []\n",
    "for benchmarkName in benchmarkNamesInDF:\n",
    "    print(benchmarkName)\n",
    "    inputDFPerBenchmark = inputDF[inputDF[\"benchmarkName\"] == benchmarkName]\n",
    "    meanData = inputDFPerBenchmark.mean()\n",
    "    print(type(meanData))\n",
    "    meanData[\"benchmarkName\"] = f\"{benchmarkName.upper()}({len(inputDFPerBenchmark)})\"\n",
    "    listForLatexTable.append(meanData)\n",
    "DF = pd.DataFrame(listForLatexTable)\n",
    "\n",
    "DF = DF.sort_index(axis='columns')\n",
    "DF\n",
    "# relativeErrorの単位は[%]ではない。scoreの値はscore()で取得できたもの\n",
    "DF.columns = [\"ベンチマーク名(関数の個数)\", \"MAPE(予測対象関数コール回数に対する)\", \"決定係数\"]\n",
    "print(DF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forInputDF = returnDFSummarizedData(\n",
    "    benchmarkNames=[\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"],\n",
    "    classes=[\"C\"],\n",
    "    processes=[2, 4, 8, 16, 32, 64, 128, 256],\n",
    "    targetIndex=-1,\n",
    "    csvDirPath=\"./csv_files/\",\n",
    ")\n",
    "\n",
    "benchmarkNames = list(set(forInputDF[\"benchmarkName\"].tolist()))\n",
    "columnsNames = [\"ベンチマーク名(関数の個数)\", \"MAPE(予測対象関数コール回数に対する)\"]\n",
    "listForRelativeErrorTable = []\n",
    "for benchmarkName in benchmarkNames:\n",
    "    forInputDFPerBenchmark = forInputDF[forInputDF[\"benchmarkName\"]\n",
    "                                        == benchmarkName]\n",
    "    column1 = f\"{benchmarkName.upper()}({len(forInputDFPerBenchmark)})\"\n",
    "    seriesOfMean = forInputDFPerBenchmark.mean()\n",
    "    seriesOfMeanRelativeErrorRate = seriesOfMean[\"RelativeErrorRate\"]\n",
    "    column2 = int(seriesOfMeanRelativeErrorRate * 100) / 100\n",
    "    listForRelativeErrorTable.append([{column1}, {column2}])\n",
    "print(pd.DataFrame(listForRelativeErrorTable,\n",
    "      columns=columnsNames).to_latex(index=False))\n",
    "\n",
    "# forInputDFPerBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(listForRelativeErrorTable, columns=columnsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb\n",
    "\n",
    "\n",
    "benchmarkNames = ['cg', 'ep', 'ft', 'is', 'lu', 'mg']\n",
    "benchmarkName = 'cg'\n",
    "\n",
    "classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "# 学習用生データ\n",
    "DF = returnCollectedExistingData(benchmarkNames=[\n",
    "                                 benchmarkName], classes=classes, processes=processes, csvDirPath=\"./csv_files/\")\n",
    "DFByValidFunction = returnDFwithFunctionsExecUnderAllConditions(\n",
    "    inputDF=DF, classes=classes, processes=processes)\n",
    "# 問題サイズを数値化したカラムを追加\n",
    "listBenchmarkClass = DFByValidFunction[\"benchmarkClass\"].tolist()\n",
    "DFWithNumInBenchmarkClass = DFByValidFunction.assign(\n",
    "    benchmarkClassInNum=convertBenchmarkClasses_problemSizeInNPB(listBenchmarkClass))\n",
    "validFunctionNames = list(\n",
    "    set(DFWithNumInBenchmarkClass[\"functionName\"].tolist()))\n",
    "for validFunctionName in validFunctionNames:\n",
    "    pass\n",
    "\n",
    "# 学習用データを抽出\n",
    "# 予測対象データを抽出\n",
    "\n",
    "classesTarget = classes[-1]\n",
    "processesTarget = processes[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFWithNumInBenchmarkClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modelLin': array([1.46970337e-14]), 'modelIp': array([0.14415395]), 'modelLog': array([0.03990015])}\n",
      "{'modelLin': array([3.35330871]), 'modelIp': array([1.25003187e-14]), 'modelLog': array([0.21768522])}\n",
      "{'modelLin': array([1.07618955]), 'modelIp': array([0.03316237]), 'modelLog': array([2.3043592e-14])}\n",
      "type(realData)=<class 'float'>, type(predictedData)=<class 'float'>\n",
      "realData=284.0\n",
      "predictedData=284.0\n",
      "type(realData)=<class 'float'>, type(predictedData)=<class 'float'>\n",
      "realData=4.15\n",
      "predictedData=3.9965653407923334\n",
      "type(realData)=<class 'float'>, type(predictedData)=<class 'float'>\n",
      "realData=13.839603729470838\n",
      "predictedData=14.030088439648038\n",
      "relativeErrorRateDictAtLin={'modelLin': 0.0, 'modelIp': array([[0.196]]), 'modelLog': array([[0.05]])}\n",
      "relativeErrorRateDictAtIp ={'modelLin': 3.697, 'modelIp': array([[0.]]), 'modelLog': array([[0.269]])}\n",
      "relativeErrorRateDictAtLog={'modelLin': 1.376, 'modelIp': array([[0.044]]), 'modelLog': array([[0.]])}\n",
      "modelsLin.returnFunctionName()=, modelsIp.returnFunctionName()=, modelsLog.returnFunctionName()=\n",
      "modelsLin.returnBenchmarkName()=, modelsIp.returnBenchmarkName()=, modelsLog.returnBenchmarkName()=\n",
      "modelsLin.returnFunctionName()=functionName, modelsIp.returnFunctionName()=functionName, modelsLog.returnFunctionName()=functionName\n",
      "modelsLin.returnBenchmarkName()=, modelsIp.returnBenchmarkName()=, modelsLog.returnBenchmarkName()=\n",
      "modelsLin.returnFunctionName()=functionName, modelsIp.returnFunctionName()=functionName, modelsLog.returnFunctionName()=functionName\n",
      "modelsLin.returnBenchmarkName()=benchmarkName, modelsIp.returnBenchmarkName()=benchmarkName, modelsLog.returnBenchmarkName()=benchmarkName\n"
     ]
    }
   ],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb\n",
    "\n",
    "# class Models\n",
    "# 各モデルのオブジェクトデータを保持している。\n",
    "# 学習用データ・予測対象データを保持している\n",
    "# 引数名とその説明\n",
    "# inputDF：入力データの全てを保持したDF（説明変数・目的変数・ベンチマーク名・関数名を最低限保持している）\n",
    "# expVarColNames：inputDFの列名の中で、説明変数として用いるカラム名のリスト\n",
    "# resVarColNames：inputDFの列名の中で、説明変数として用いるカラム名のリスト\n",
    "# targetDF：inputDFとデータ構成は同じだが、予測対象のデータがセットされている\n",
    "# modelNames：実施するモデル名を指定できる([\"modelLin\", \"modelIp\", \"modelLog\"])\n",
    "\n",
    "class Models:\n",
    "\n",
    "    def __init__(self, inputDF, expVarColNames, resVarColNames, targetDF=None, modelNames=[\"modelLin\", \"modelIp\", \"modelLog\"]):\n",
    "        self.inputDF = inputDF\n",
    "        self.expVarColNames = expVarColNames\n",
    "        if len(resVarColNames) > 1:\n",
    "            warnings.warn(\"説明変数が複数個存在しています\")\n",
    "        self.resVarColNames = resVarColNames\n",
    "        self.targetDF = targetDF\n",
    "        self.functionName = \"\"\n",
    "        self.benchmarkName = \"\"\n",
    "        self.modelNames = modelNames\n",
    "\n",
    "        if (\"modelLin\" in self.modelNames):\n",
    "            self.objectModelLin = ModelLinForMultipleRegression(inputDF, explanatoryVariableColumnNames=expVarColNames, responseVariableColumnNames=resVarColNames, targetDF=targetDF)\n",
    "        if (\"modelIp\" in self.modelNames):\n",
    "            self.objectModelIp = ModelIpForMultipleRegression(inputDF, explanatoryVariableColumnNames=expVarColNames, responseVariableColumnNames=resVarColNames, targetDF=targetDF)\n",
    "        if (\"modelLog\" in self.modelNames):\n",
    "            self.objectModelLog = ModelLogForMultipleRegression(inputDF, explanatoryVariableColumnNames=expVarColNames, responseVariableColumnNames=resVarColNames, targetDF=targetDF)\n",
    "\n",
    "\n",
    "    def setUpDataBeforeCalcLr(self):\n",
    "        if (\"modelLin\" in self.modelNames):\n",
    "            self.objectModelLin.setUpDataBeforeCalcLr()\n",
    "        if (\"modelIp\" in self.modelNames):\n",
    "            self.objectModelIp.setUpDataBeforeCalcLr()\n",
    "        if (\"modelLog\" in self.modelNames):\n",
    "            self.objectModelLog.setUpDataBeforeCalcLr()\n",
    "\n",
    "    def calcLr(self):\n",
    "        if (\"modelLin\" in self.modelNames):\n",
    "            self.objectModelLin.calcLr()\n",
    "        if (\"modelIp\" in self.modelNames):\n",
    "            self.objectModelIp.calcLr()\n",
    "        if (\"modelLog\" in self.modelNames):\n",
    "            self.objectModelLog.calcLr()\n",
    "\n",
    "    # inputDF：__init__()でのinputDFとDF構成は同じ\n",
    "    def predict(self, inputDF):\n",
    "        pass\n",
    "\n",
    "    # 学習用データへの適合度（MAPE[%]）を計算する\n",
    "    def calcMAPE(self):\n",
    "        # MAPEatTrain:辞書\n",
    "        # キーはmodelNamesの要素、バリューは学習データの適合度としてのMAPE\n",
    "        MAPEatTrain = {}\n",
    "        if len(self.resVarColNames) > 1:\n",
    "            warnings.warn(\"説明変数が複数カラムに及んでいるため、正常な動作を期待できません\")\n",
    "        realData = self.inputDF[self.resVarColNames[0]].tolist()\n",
    "        if (\"modelLin\" in self.modelNames):\n",
    "            predictedDataAtLin = self.objectModelLin.predict(self.inputDF[self.expVarColNames])\n",
    "            modelLinMAPEatTrain = returnMapeScore(realData, predictedDataAtLin)\n",
    "            MAPEatTrain[\"modelLin\"] = modelLinMAPEatTrain\n",
    "        if (\"modelIp\" in self.modelNames):\n",
    "            predictedDataAtIp  = self.objectModelIp.predict(self.inputDF[self.expVarColNames])\n",
    "            modelIpMAPEatTrain = returnMapeScore(realData, predictedDataAtIp)\n",
    "            MAPEatTrain[\"modelIp\"] = modelIpMAPEatTrain\n",
    "        if (\"modelLog\" in self.modelNames):\n",
    "            predictedDataAtLog = self.objectModelLog.predict(self.inputDF[self.expVarColNames])\n",
    "            modelLogMAPEatTrain = returnMapeScore(realData, predictedDataAtLog)\n",
    "            MAPEatTrain[\"modelLog\"] = modelLogMAPEatTrain\n",
    "        self.MAPEatTrain = MAPEatTrain\n",
    "        \n",
    "    # calcMAPEで計算した辞書を返す関数\n",
    "    # 返す辞書がない場合は空の辞書を返す\n",
    "    def returnCalculatedMAPE(self):\n",
    "        if self.MAPEatTrain is None:\n",
    "            reutnrn({})\n",
    "        else:\n",
    "            return(self.MAPEatTrain)\n",
    "\n",
    "    # 予測対象データとの相対誤差率を計算する\n",
    "    # 引数 targetDF:本オブジェクト構築時に必要になるinputDFをデータ構造が同じDF\n",
    "    def calcRelativeErrorRate(self, targetDF=None):\n",
    "        # relativeErrorRateDict:辞書\n",
    "        # キーはmodelNamesの要素、バリューは絶対相対誤差率\n",
    "        relativeErrorRateDict = {}\n",
    "        # （すでに予測対象の説明変数データがある or targetDF is not None）なら問題ない。\n",
    "        if ((self.targetDF is None) and (targetDF is None)):\n",
    "            warnings.warn(\"相対誤差率を計算するための真値が与えられていません。\")\n",
    "            return (-1)\n",
    "        \n",
    "        if len(self.resVarColNames) == 0:\n",
    "            warnings.warn(\"説明変数のカラム名が複数設定されています\")\n",
    "        if targetDF is None :\n",
    "            if len(self.targetDF) > 1:\n",
    "                warnings.warn(\"ターゲットとなるDFに要素が2つ以上含まれています。\")\n",
    "            realData = self.targetDF[self.resVarColNames[0]]\n",
    "        else:\n",
    "            if len(targetDF) > 1:\n",
    "                warnings.warn(\"ターゲットとなるDFに要素が2つ以上含まれています。\")\n",
    "            realData = targetDF[self.resVarColNames[0]]\n",
    "        \n",
    "        # realData は DataFrame なので、それをリスト化して、最初の要素のみ保持する\n",
    "        realData = realData.tolist()[0]\n",
    "    \n",
    "        if (\"modelLin\" in self.modelNames):\n",
    "            predictedData = self.objectModelLin.predict(targetDF[self.expVarColNames]).tolist()[0][0]\n",
    "            print(f\"type(realData)={type(realData)}, type(predictedData)={type(predictedData)}\")\n",
    "            print(f\"realData={realData}\")\n",
    "            print(f\"predictedData={predictedData}\")\n",
    "            relativeErrorRateDict[\"modelLin\"] = returnRelativeErrorRate(realNum=realData, predictedNum=predictedData)\n",
    "        if (\"modelIp\" in self.modelNames):\n",
    "            predictedData = self.objectModelIp.predict(targetDF[self.expVarColNames])\n",
    "            relativeErrorRateDict[\"modelIp\"] = returnRelativeErrorRate(realNum=realData, predictedNum=predictedData)\n",
    "        if (\"modelLog\" in self.modelNames):\n",
    "            predictedData = self.objectModelLog.predict(targetDF[self.expVarColNames])\n",
    "            relativeErrorRateDict[\"modelLog\"] = returnRelativeErrorRate(realNum=realData, predictedNum=predictedData)\n",
    "        self.relativeErrorRateDict = relativeErrorRateDict\n",
    "\n",
    "    # calcRelativeErrorRate()で計算した辞書を返す関数\n",
    "    # 返す辞書がない場合は空の辞書を返す\n",
    "    def returnRelativeErrorRateDict(self):\n",
    "        if self.relativeErrorRateDict is None:\n",
    "            return ({})\n",
    "        else:\n",
    "            return(self.relativeErrorRateDict)\n",
    "\n",
    "    # 関数名・ベンチマーク名を更新する\n",
    "    def updateFunctionAndBenchmarkName(self, functionName=None, benchmarkName=None):\n",
    "        # TODO:各種引数が空の場合は更新しない\n",
    "        if functionName is not None:\n",
    "            self.functionName = functionName\n",
    "        if benchmarkName is not None:\n",
    "            self.benchmarkName = benchmarkName\n",
    "\n",
    "    def returnFunctionName(self):\n",
    "        return(self.functionName)\n",
    "    def returnBenchmarkName(self):\n",
    "        return(self.benchmarkName)\n",
    "\n",
    "def test_Models():\n",
    "    # inputDFを準備\n",
    "    # 説明変数\n",
    "    plotX = np.linspace(1, 20, 10)\n",
    "    plotY = np.linspace(21, 40, 10)\n",
    "    plotZ = np.linspace(41, 60, 10)\n",
    "    # 目的変数\n",
    "    plotTforLin = plotX + 2*plotY + 3*plotZ + 4\n",
    "    plotTforIp  = 1/plotX + 2/plotY + 3/plotZ + 4\n",
    "    plotTforLog = np.log10(plotX) + 2*np.log10(plotY) + 3*np.log10(plotZ) + 4\n",
    "    inputDF = pd.DataFrame({\"plotX\":plotX, \"plotY\":plotY, \"plotZ\":plotZ, \"plotTforLin\":plotTforLin, \"plotTforIp\":plotTforIp, \"plotTforLog\":plotTforLog})\n",
    "    \n",
    "    # functionNameを準備\n",
    "    functionName = \"functionName\"\n",
    "    # benchmarkNameを準備\n",
    "    benchmarkName = \"benchmarkName\"\n",
    "    \n",
    "    inputDF[\"functionNames\"] = functionName\n",
    "    inputDF[benchmarkName] = benchmarkName\n",
    "    # targetDFを準備\n",
    "    targetDF = inputDF.tail(1)\n",
    "    \n",
    "    columnNames = inputDF.columns.tolist()\n",
    "    # expVarColNamesを準備\n",
    "    expVarColNames = columnNames[:3]\n",
    "    # resVarColNamesを準備\n",
    "    resVarColNames = columnNames[-3:]\n",
    "    resVarColNamesForLin = [\"plotTforLin\"]\n",
    "    resVarColNamesForIp  = [\"plotTforIp\"]\n",
    "    resVarColNamesForLog = [\"plotTforLog\"]\n",
    "\n",
    "    # インスタンスを作成\n",
    "    modelsLin = Models(inputDF=inputDF, expVarColNames=expVarColNames, resVarColNames=resVarColNamesForLin, targetDF=targetDF)\n",
    "    modelsIp  = Models(inputDF=inputDF, expVarColNames=expVarColNames, resVarColNames=resVarColNamesForIp , targetDF=targetDF)\n",
    "    modelsLog = Models(inputDF=inputDF, expVarColNames=expVarColNames, resVarColNames=resVarColNamesForLog, targetDF=targetDF)\n",
    "    # 予測に必要な初期化作業を開始\n",
    "    modelsLin.setUpDataBeforeCalcLr()\n",
    "    modelsIp.setUpDataBeforeCalcLr()\n",
    "    modelsLog.setUpDataBeforeCalcLr()\n",
    "    # モデル構築を実施\n",
    "    modelsLin.calcLr()\n",
    "    modelsIp.calcLr()\n",
    "    modelsLog.calcLr()\n",
    "    # TODO:予測をして学習データに対するMAPEを計算し、その値がそれぞれ小さいことを確認\n",
    "    modelsLin.calcMAPE()\n",
    "    dictCalcedMAPEatLin = modelsLin.returnCalculatedMAPE()\n",
    "    modelsIp.calcMAPE()\n",
    "    dictCalcedMAPEatIp = modelsIp.returnCalculatedMAPE()\n",
    "    modelsLog.calcMAPE()\n",
    "    dictCalcedMAPEatLog = modelsLog.returnCalculatedMAPE()\n",
    "    print(dictCalcedMAPEatLin)\n",
    "    print(dictCalcedMAPEatIp)\n",
    "    print(dictCalcedMAPEatLog)\n",
    "    # TODO:相対誤差率を計算し、それが小さいことを確認\n",
    "    modelsLin.calcRelativeErrorRate(targetDF=targetDF)\n",
    "    modelsIp.calcRelativeErrorRate(targetDF=targetDF)\n",
    "    modelsLog.calcRelativeErrorRate(targetDF=targetDF)\n",
    "    relativeErrorRateDictAtLin = modelsLin.returnRelativeErrorRateDict()\n",
    "    relativeErrorRateDictAtIp  = modelsIp.returnRelativeErrorRateDict()\n",
    "    relativeErrorRateDictAtLog = modelsLog.returnRelativeErrorRateDict()\n",
    "    print(f\"relativeErrorRateDictAtLin={relativeErrorRateDictAtLin}\")\n",
    "    print(f\"relativeErrorRateDictAtIp ={relativeErrorRateDictAtIp}\")\n",
    "    print(f\"relativeErrorRateDictAtLog={relativeErrorRateDictAtLog}\")\n",
    "    # 関数名・ベンチマーク名を更新する関数のテスト\n",
    "    # updateFunctionAndBenchmarkName()\n",
    "    print(f\"modelsLin.returnFunctionName()={modelsLin.returnFunctionName()}, modelsIp.returnFunctionName()={modelsIp.returnFunctionName()}, modelsLog.returnFunctionName()={modelsLog.returnFunctionName()}\")\n",
    "    print(f\"modelsLin.returnBenchmarkName()={modelsLin.returnBenchmarkName()}, modelsIp.returnBenchmarkName()={modelsIp.returnBenchmarkName()}, modelsLog.returnBenchmarkName()={modelsLog.returnBenchmarkName()}\")\n",
    "    modelsLin.updateFunctionAndBenchmarkName(functionName=functionName)\n",
    "    modelsIp.updateFunctionAndBenchmarkName(functionName=functionName)\n",
    "    modelsLog.updateFunctionAndBenchmarkName(functionName=functionName)\n",
    "    print(f\"modelsLin.returnFunctionName()={modelsLin.returnFunctionName()}, modelsIp.returnFunctionName()={modelsIp.returnFunctionName()}, modelsLog.returnFunctionName()={modelsLog.returnFunctionName()}\")\n",
    "    print(f\"modelsLin.returnBenchmarkName()={modelsLin.returnBenchmarkName()}, modelsIp.returnBenchmarkName()={modelsIp.returnBenchmarkName()}, modelsLog.returnBenchmarkName()={modelsLog.returnBenchmarkName()}\")\n",
    "    modelsLin.updateFunctionAndBenchmarkName(benchmarkName=benchmarkName)\n",
    "    modelsIp.updateFunctionAndBenchmarkName(benchmarkName=benchmarkName)\n",
    "    modelsLog.updateFunctionAndBenchmarkName(benchmarkName=benchmarkName)    \n",
    "    print(f\"modelsLin.returnFunctionName()={modelsLin.returnFunctionName()}, modelsIp.returnFunctionName()={modelsIp.returnFunctionName()}, modelsLog.returnFunctionName()={modelsLog.returnFunctionName()}\")\n",
    "    print(f\"modelsLin.returnBenchmarkName()={modelsLin.returnBenchmarkName()}, modelsIp.returnBenchmarkName()={modelsIp.returnBenchmarkName()}, modelsLog.returnBenchmarkName()={modelsLog.returnBenchmarkName()}\")\n",
    "    # assertする\n",
    "\n",
    "    pass\n",
    "\n",
    "test_Models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
