{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修正したモデルから卒論時に集計したデータを作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 表\n",
    "\n",
    "| ベンチマーク名 | 平均誤差率(%) | コスト比(%) |\n",
    "|---------|----------|---------|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 表\n",
    "\n",
    "| ベンチマーク名 | 採用割合(最大MAPE(%), 最小MAPE(%)) |\n",
    "|---------|----------------------------|\n",
    "|         | モデル(1), モデル(2), ...        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 平均誤差率：大規模実行時の関数コール回数との比較\n",
    "* MAPE：トレーニングデータとの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.xlabel(\"使用したプロファイル数\")\n",
    "plt.ylabel(\"平均誤差率(%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_option(\"display.max_columns\")\n",
    "# pd.get_option(\"display.max_rows\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "benchmark = \"cg\"\n",
    "fix = \"Class\"\n",
    "if benchmark == \"bt\" or benchmark == \"sp\":\n",
    "    processes = processes_onlyBTSP\n",
    "else:\n",
    "    processes = processes_excludeBTSP\n",
    "benchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "targetNumOfProcess = 256\n",
    "fixedBenchmarkClass = \"B\"\n",
    "fixedProcess = 64\n",
    "\n",
    "# 引数の条件に合った生の実験データを取得する\n",
    "DF = returnRawDFperBenchmark(Benchmark=benchmark, fix=fix, benchmarkClass=benchmarkClasses,\n",
    "                             FixedProcess=fixedProcess, Processes=processes, FixedBenchmarkClass=fixedBenchmarkClass)\n",
    "# 取得した生の実験データから NaN が含まれる関数の実験データを削除\n",
    "noNaNDF = DF.dropna(how='any')\n",
    "# noNaNDF\n",
    "\n",
    "returnedCalculatedDF = return_calculatedDF(benchmark=benchmark, noNaNDF=noNaNDF,\n",
    "                                           targetNumOfProcess=targetNumOfProcess, targetProblemSize=fixedBenchmarkClass, fix=fix)\n",
    "# returnedCalculatedDF\n",
    "\n",
    "# numOfData列の要素一覧を作成し、ソートされたリストを、listOfNumDataに格納する\n",
    "listOfNumOfData = returnedCalculatedDF['numOfData'].tolist()\n",
    "listOfNumOfData = sorted(list(set(listOfNumOfData)))\n",
    "# listOfNumOfData\n",
    "\n",
    "\n",
    "# 使用したプロファイル数をキー・最適モデルでの相対誤差の平均をバリューとした辞書を作成する\n",
    "dictAverageRelativeErrorOfBestModel = {}\n",
    "x = []\n",
    "y = []\n",
    "for numOfData in listOfNumOfData:\n",
    "    # 使用したプロファイル数で抽出\n",
    "    extractedPerNumOfProfileDF = returnedCalculatedDF[returnedCalculatedDF['numOfData'] == numOfData]\n",
    "    meanDF = extractedPerNumOfProfileDF.mean()\n",
    "    data = meanDF.at['relativeErrorOfBestModel']\n",
    "    dictAverageRelativeErrorOfBestModel[numOfData] = data\n",
    "\n",
    "x = list(dictAverageRelativeErrorOfBestModel.keys())\n",
    "x\n",
    "y = [dictAverageRelativeErrorOfBestModel[key] for key in x]\n",
    "y\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, marker='o')\n",
    "plt.xlabel(\"使用したプロファイル数[％]\")\n",
    "plt.ylabel(\"平均絶対誤差率\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマーク名・関数名・プロセス数・問題サイズを指定することで、その条件での関数コール回数を取得する関数\n",
    "\n",
    "def returnSpecificData(benchmarkName=\"cg\", functionName=\".TAU_application\", process=256, benchmarkClass=\"D\"):\n",
    "    targetRawDF = returnRawDF(Benchmark=benchmarkName, functionName=functionName, benchmarkClass=[\n",
    "                              benchmarkClass], FixedProcess=process, Processes=[process], FixedBenchmarkClass=benchmarkClass)\n",
    "    return targetRawDF.iat[0, 0]\n",
    "# returnSpecificData(benchmarkName=\"mg\", functionName=\"BUBBLE\", process=256, benchmarkClass=\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarksからbt, spを除外する\n",
    "benchmarks = [benchmark for benchmark in benchmarks if benchmark !=\n",
    "              'bt' and benchmark != 'sp']\n",
    "# pandasのDFをprintした時の幅を広げる\n",
    "pd.set_option('display.width', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictTmp = returnDictForPlotPerNumOfUsedData(Benchmark=benchmarks, fix=\"Class\", benchmarkClass=[\n",
    "    \"A\", \"B\", \"C\", \"D\"], FixedProcess=64, Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4g}'.format\n",
    "\n",
    "tmpDF = pd.DataFrame()\n",
    "for benchmark in benchmarks:\n",
    "    listToLearn = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    listToPredict = [256]\n",
    "    benchmark_x = dictTmp[benchmark][\"x\"]\n",
    "    benchmark_y = dictTmp[benchmark][\"y\"]\n",
    "    index = benchmark_x.index(len(listToLearn))\n",
    "    MAPE = benchmark_y[index]\n",
    "    relativeCost = returnRelativeCost(benchmark=benchmark, variablesToLearn=listToLearn,\n",
    "                                      variablesToPredict=listToPredict, fixedClassOrProcess=\"Class\", fixed=\"C\")\n",
    "    dictRowData = {\"ベンチマーク名\": benchmark.upper(\n",
    "    ), \"平均絶対誤差率[％]\": MAPE, \"相対コスト[％]\": relativeCost}\n",
    "    iDF = pd.DataFrame.from_dict(dictRowData, orient='index').T\n",
    "    tmpDF = tmpDF.append(iDF)\n",
    "tmpDFMean = tmpDF.mean()\n",
    "type(tmpDFMean)\n",
    "print(tmpDF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictTmp\n",
    "\n",
    "plt.figure(figsize=(5.72, 4), dpi=200)\n",
    "for benchmark in list(dictTmp.keys()):\n",
    "    x = dictTmp[benchmark][\"x\"]\n",
    "    y = dictTmp[benchmark][\"y\"]\n",
    "    plt.plot(x, y, marker='o', label=benchmark.upper())\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"使用したプロファイル数\")\n",
    "    plt.ylabel(\"平均絶対誤差率[％]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.72, 4), dpi=200)\n",
    "\n",
    "# Extra-PでfixProcessデータを入力して出力したモデルの図時\n",
    "plot_x = np.linspace(0.8, 256, 500)\n",
    "# -3590464.6990329633 + 3759195.349891038 * p^(1/4)\n",
    "plot_y = []\n",
    "for x in plot_x:\n",
    "    plot_y.append(2286768.3333333326 + 301997.61904761934 * math.log2(x)**(1))\n",
    "plt.plot(plot_x, plot_y, label=\"ExtraP\")\n",
    "\n",
    "x = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "y = [1984770.0, 2263540.0, 2821070.0, 3936140.0,\n",
    "     3936140.0, 3936140.0, 3936140.0, 3936140.0]\n",
    "x = np.array(x).reshape(-1, 1)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "plt.scatter(x, y, marker=\"o\", label=\"予測に用いた関数コール回数\")\n",
    "plot_x = np.array(plot_x).reshape(-1, 1)\n",
    "x_target = [256]\n",
    "y_target = [3936140]\n",
    "plt.scatter(x_target, y_target, marker=\"o\", label=\"予測したい関数コール回数の実測値\")\n",
    "\n",
    "benchmarkName = \"CG\"\n",
    "functionName = \"ICNVRT\"\n",
    "\n",
    "# 線形モデル\n",
    "# 対数モデル\n",
    "\n",
    "# 反比例モデル\n",
    "modelIpMk2 = ModelIp_mk2(train_x=x, train_y=y, target_x=x_target, target_y=y_target,\n",
    "                         benchmark_name=benchmarkName, function_name=functionName)\n",
    "modelIpMk2.calc_lr()\n",
    "plot_y_IpMk2 = modelIpMk2.predict(plot_x)\n",
    "plt.plot(plot_x, plot_y_IpMk2, label=\"反比例モデル\")\n",
    "# 線形飽和モデル\n",
    "modelBranchMk2 = ModelBranch_mk2(train_x=x, train_y=y, target_x=x_target,\n",
    "                                 target_y=y_target, benchmark_name=benchmarkName, function_name=functionName)\n",
    "modelBranchMk2.calc_lr()\n",
    "plot_y_BranchMk2 = modelBranchMk2.predict(plot_x)\n",
    "plt.plot(plot_x, plot_y_BranchMk2, label=\"線形飽和モデル\")\n",
    "# # 線形モデル\n",
    "# model_lin = ModelLin(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_lin.calc_lr()\n",
    "# plot_y_lin = model_lin.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_lin, label=\"線形モデル\")\n",
    "# # 対数モデル\n",
    "# model_log10 = ModelLog10(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_log10.calc_lr()\n",
    "# plot_y_log10 = model_log10.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_log10, label=\"対数モデル\")\n",
    "# # 反比例モデル\n",
    "# model_ip = ModelIP(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_ip.calc_lr()\n",
    "# plot_y_ip = model_ip.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_ip, label=\"反比例モデル\")\n",
    "# # 線形飽和モデル\n",
    "# model_branch = ModelBranch(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_branch.calc_lr()\n",
    "# plot_y_branch = model_branch.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_branch, label=\"線形飽和モデル\")\n",
    "# 凡例の表示\n",
    "plt.legend()\n",
    "# 軸ラベルの設定\n",
    "plt.ylabel(\"関数コール回数\")\n",
    "plt.xlabel(\"実行コア数\")\n",
    "\n",
    "plt.scatter(x, y, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 実際にプロットする\n",
    "\n",
    "\n",
    "# print(f\"fix={fix}, benchmarkClasses={benchmarkClasses}, fixedProcess={fixedProcess}, Processes={processes}, FixedBenchmarkClass={fixedBenchmarkClass}\")\n",
    "# print(f\"targetNumOfProcess={targetNumOfProcess}, targetProblemSize={fixedBenchmarkClass}, fix={fix}\")\n",
    "\n",
    "# DF = returnRawDFperBenchmark(Benchmark=\"mg\", fix=\"Process\", benchmarkClass=[\"A\", \"B\", \"C\", \"D\"], Processes=[\n",
    "#                              1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"B\", FixedProcess=64)\n",
    "# DF.dropna(how='any')\n",
    "# DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmarkNamesExcludeBTSP = [\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"]\n",
    "# classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "classes = [\"B\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "targetIndex = -1\n",
    "csvDirPath = \"./csv_files/\"\n",
    "\n",
    "dfByDatumExcludeBTSP = returnDFSummarizedData(\n",
    "    benchmarkNames=benchmarkNamesExcludeBTSP, classes=classes, processes=processes, targetIndex=targetIndex, csvDirPath=csvDirPath)\n",
    "# dfByDatumExcludeBTSP\n",
    "\n",
    "dictForLatexTable = {}\n",
    "numOfData = 0\n",
    "for benchmarkName in benchmarkNamesExcludeBTSP:\n",
    "    dictForLatexTable[benchmarkName] = dfByDatumExcludeBTSP[dfByDatumExcludeBTSP[\"benchmarkName\"] == benchmarkName]\n",
    "    numOfData += len(\n",
    "        dfByDatumExcludeBTSP[dfByDatumExcludeBTSP[\"benchmarkName\"] == benchmarkName])\n",
    "\n",
    "numOfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "listForDF = []\n",
    "\n",
    "for benchmarkName in benchmarkNamesExcludeBTSP:\n",
    "    listForDF.append(returnSeriesOfDatumPerBenchmark(\n",
    "        inputDF=dictForLatexTable[benchmarkName]))\n",
    "DF = pd.DataFrame(listForDF)\n",
    "print(DF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_returnSeriesOfDatumPerBenchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultIs = dictForLatexTable[\"is\"]\n",
    "# resultIs\n",
    "resultIsAtModelBranch = resultIs[resultIs[\"objectBestModelName\"]\n",
    "                                 == \"ModelBranch\"]\n",
    "datumX = resultIsAtModelBranch[\"usedDataX\"].tolist()\n",
    "datumY = resultIsAtModelBranch[\"usedDataY\"].tolist()\n",
    "\n",
    "datumX\n",
    "datumY\n",
    "\n",
    "# returnSeriesOfData(benchmarkName=\"is\", functionName=\"double_randlc(double_*_double_*)\", rawX=dataX, rawY=dataY, fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultIs = dictForLatexTable[\"ft\"]\n",
    "# resultIs\n",
    "resultIsAtModelBranch = resultIs[resultIs[\"objectBestModelName\"]\n",
    "                                 == \"ModelBranch\"]\n",
    "resultIsAtModelBranchOfNotLowMAPE = resultIsAtModelBranch[\n",
    "    resultIsAtModelBranch[\"MAPEOfBestModel\"] > 1]\n",
    "resultIsAtModelBranchOfNotLowMAPE\n",
    "datumX = resultIsAtModelBranchOfNotLowMAPE[\"usedDataX\"].tolist()\n",
    "datumY = resultIsAtModelBranchOfNotLowMAPE[\"usedDataY\"].tolist()\n",
    "\n",
    "datumX\n",
    "datumY\n",
    "\n",
    "for dataIndex in range(len(datumX)):\n",
    "    plt.figure()\n",
    "    plt.scatter(datumX[dataIndex], datumY[dataIndex])\n",
    "\n",
    "# returnSeriesOfData(benchmarkName=\"is\", functionName=\"double_randlc(double_*_double_*)\", rawX=dataX, rawY=dataY, fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 実験用に小規模なリスト\n",
    "benchmarkNames = [\"cg\"]\n",
    "classes = [\"B\"]\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 生データの取得\n",
    "cgDF = returnCollectedExistingData(benchmarkNames=[\"cg\"], classes=[\"A\", \"B\", \"C\", \"D\"], processes=[\n",
    "                                   1, 2, 4, 8, 16, 32, 64, 128, 256], csvDirPath=\"./csv_files/\")\n",
    "cgDF\n",
    "# ベンチマーククラスがAの情報を取得\n",
    "cgDFfixedA = cgDF[cgDF[\"benchmarkClass\"] == \"A\"]\n",
    "cgDFfixedA\n",
    "# 関数名のリストを取得\n",
    "functionNames = sorted(list(set(cgDFfixedA[\"functionName\"])))\n",
    "print(functionNames)\n",
    "\n",
    "# 関数名を関数名のリストから抽出\n",
    "functionNameCG = cgDFfixedA[cgDFfixedA[\"functionName\"] == \"CG\"]\n",
    "functionNameCG\n",
    "\n",
    "# 説明変数と目的変数とをリスト化したものを抽出\n",
    "# プロセス数\n",
    "raw_x = functionNameCG['process'].tolist()\n",
    "# 関数コール回数\n",
    "raw_y = functionNameCG['functionCallNum'].tolist()\n",
    "\n",
    "print(f\"raw_x={raw_x}\")\n",
    "print(f\"raw_y={raw_y}\")\n",
    "\n",
    "bencmarkName = \"CG\"\n",
    "functionName = \"CG\"\n",
    "fixProcessOrClass = \"Class\"\n",
    "fixed = \"A\"\n",
    "targetProcess = 256\n",
    "targetBenchmarkClass = fixed\n",
    "targetFunctionCallNum = raw_y[-1]\n",
    "returnSeriesOfData(benchmarkName=\"benhmarkName\", functionName=\"functionName\", rawX=[1, 2, 3], rawY=[\n",
    "                   1, 2, 3], fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブック中で変数のみを記述することでデータフレームをきれいに表示させる設定の有効化\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題サイズD, コア数256での関数コール回数を予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO：BT, SP以外のベンチマーク名を入れる\n",
    "benchmarkNames = ['cg', 'ep', 'ft', 'is', 'lu', 'mg']\n",
    "classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "dictForSummarizedResult = {}\n",
    "columnName = [\"benchmarkName\", \"functionName\", \"score\", \"relativeErrorRate\"]\n",
    "dfForSummarizedResult = pd.DataFrame(columns=columnName)\n",
    "for benchmarkName in benchmarkNames:\n",
    "    # ベンチマークごとにscoreを保持するためのリスト\n",
    "    listForSummarizedResultPerBenchmarkName = []\n",
    "    # 学習用生データ\n",
    "    DF = returnCollectedExistingData(benchmarkNames=[\n",
    "                                     benchmarkName], classes=classes, processes=processes, csvDirPath=\"./csv_files/\")\n",
    "    # 重複のない関数名のリスト\n",
    "    functionNames = list(set(DF[\"functionName\"]))\n",
    "    usefulFunctionNames = []\n",
    "    # このループで関数ごとのデータが問題サイズパターン数xコア数パターン数 分だけ存在する関数名のリストを作成する\n",
    "    for functionName in functionNames:\n",
    "        # 関数ごとに生データを集計\n",
    "        dfPerFunction = DF[DF[\"functionName\"] == functionName]\n",
    "        if len(classes) * len(processes) == len(dfPerFunction):\n",
    "            usefulFunctionNames.append(functionName)\n",
    "    if len(usefulFunctionNames) == 0:\n",
    "        continue\n",
    "    # 関数ごとのデータを抽出\n",
    "    for functionName in usefulFunctionNames:\n",
    "        # 問題サイズを数値化したカラムを追加\n",
    "        listBenchmarkClass = DF[\"benchmarkClass\"].tolist()\n",
    "        DFWithNumInBenchmarkClass = DF.assign(\n",
    "            benchmarkClassInNum=convertBenchmarkClasses_problemSizeInNPB(listBenchmarkClass))\n",
    "        # 学習用データ\n",
    "        dfPerFunctionForTrain = DFWithNumInBenchmarkClass[(\n",
    "            DFWithNumInBenchmarkClass[\"functionName\"] == functionName)]\n",
    "        dfPerFunctionForTest = DFWithNumInBenchmarkClass[(DFWithNumInBenchmarkClass[\"functionName\"] == functionName) & (\n",
    "            DFWithNumInBenchmarkClass[\"benchmarkClass\"] == \"D\") & (DFWithNumInBenchmarkClass[\"process\"] == 256)]\n",
    "\n",
    "        # x:説明変数, t:目的変数\n",
    "        trainX = dfPerFunctionForTrain[[\"process\", \"benchmarkClassInNum\"]]\n",
    "        trainT = dfPerFunctionForTrain[[\"functionCallNum\"]]\n",
    "        testX = dfPerFunctionForTest[[\"process\", \"benchmarkClassInNum\"]]\n",
    "        testT = dfPerFunctionForTest[[\"functionCallNum\"]]\n",
    "        # 重回帰分析する\n",
    "        reg_model = LinearRegression()\n",
    "        reg_model.fit(trainX, trainT)\n",
    "        # 関数ごとの結果をベンチマークごとの結果に入れる\n",
    "        scorePerFunction = reg_model.score(trainX, trainT)\n",
    "        listForSummarizedResultPerBenchmarkName.append(scorePerFunction)\n",
    "        # 予測を実施して、相対誤差を算出\n",
    "        predictedTByTestX = reg_model.predict(testX)\n",
    "        predictedData = predictedTByTestX[0][0]\n",
    "        realData = testT[\"functionCallNum\"].tolist()[0]\n",
    "        relativeErrorPerFunction = abs(predictedData - realData)/realData * 100\n",
    "        ##\n",
    "        dfPerFunction = pd.DataFrame(index=columnName, data=[\n",
    "                                     benchmarkName, functionName, scorePerFunction, relativeErrorPerFunction]).T\n",
    "        dfForSummarizedResult = dfForSummarizedResult.append(dfPerFunction)\n",
    "\n",
    "# ( A ~ D ) * (1 ~ 256) のすべての条件を\n",
    "# 満たしていたら、リストに追加\n",
    "# 満たしていなければ、なにもしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForSummarizedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = dfForSummarizedResult\n",
    "\n",
    "\n",
    "benchmarkNamesInDF = list(set(dfForSummarizedResult[\"benchmarkName\"].tolist()))\n",
    "\n",
    "listForLatexTable = []\n",
    "for benchmarkName in benchmarkNamesInDF:\n",
    "    print(benchmarkName)\n",
    "    inputDFPerBenchmark = inputDF[inputDF[\"benchmarkName\"] == benchmarkName]\n",
    "    meanData = inputDFPerBenchmark.mean()\n",
    "    print(type(meanData))\n",
    "    meanData[\"benchmarkName\"] = f\"{benchmarkName.upper()}({len(inputDFPerBenchmark)})\"\n",
    "    listForLatexTable.append(meanData)\n",
    "DF = pd.DataFrame(listForLatexTable)\n",
    "\n",
    "DF = DF.sort_index(axis='columns')\n",
    "DF\n",
    "# relativeErrorの単位は[%]ではない。scoreの値はscore()で取得できたもの\n",
    "DF.columns = [\"ベンチマーク名(関数の個数)\", \"MAPE(予測対象関数コール回数に対する)\", \"決定係数\"]\n",
    "print(DF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forInputDF = returnDFSummarizedData(\n",
    "    benchmarkNames=[\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"],\n",
    "    classes=[\"C\"],\n",
    "    processes=[2, 4, 8, 16, 32, 64, 128, 256],\n",
    "    targetIndex=-1,\n",
    "    csvDirPath=\"./csv_files/\",\n",
    ")\n",
    "\n",
    "benchmarkNames = list(set(forInputDF[\"benchmarkName\"].tolist()))\n",
    "columnsNames = [\"ベンチマーク名(関数の個数)\", \"MAPE(予測対象関数コール回数に対する)\"]\n",
    "listForRelativeErrorTable = []\n",
    "for benchmarkName in benchmarkNames:\n",
    "    forInputDFPerBenchmark = forInputDF[forInputDF[\"benchmarkName\"]\n",
    "                                        == benchmarkName]\n",
    "    column1 = f\"{benchmarkName.upper()}({len(forInputDFPerBenchmark)})\"\n",
    "    seriesOfMean = forInputDFPerBenchmark.mean()\n",
    "    seriesOfMeanRelativeErrorRate = seriesOfMean[\"RelativeErrorRate\"]\n",
    "    column2 = int(seriesOfMeanRelativeErrorRate * 100) / 100\n",
    "    listForRelativeErrorTable.append([{column1}, {column2}])\n",
    "print(pd.DataFrame(listForRelativeErrorTable,\n",
    "      columns=columnsNames).to_latex(index=False))\n",
    "\n",
    "# forInputDFPerBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(listForRelativeErrorTable, columns=columnsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-47f1ffbafe2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mrelativeErrorRateLogResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"modelLog\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m \u001b[0mtest_returnDFtoMakeSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-47f1ffbafe2b>\u001b[0m in \u001b[0;36mtest_returnDFtoMakeSummary\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# returnDFtoMakeSummary()の実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mresultAtLin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturnDFtoMakeSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputDFatLin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetProcess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mresultAtIp\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mreturnDFtoMakeSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputDFatIp\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtargetClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetProcess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mresultAtLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturnDFtoMakeSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputDFatLog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetProcess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-47f1ffbafe2b>\u001b[0m in \u001b[0;36mreturnDFtoMakeSummary\u001b[0;34m(inputDF, benchmarkName, validFunctionName, targetClass, targetProcess, expVarColNames, resVarColNames)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 学習\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetUpDataBeforeCalcLr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcLr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# MAPE・相対誤差率を算出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcMAPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9bed0ba13294>\u001b[0m in \u001b[0;36mcalcLr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalcLr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"modelLin\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectModelLin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcLr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"modelIp\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectModelIp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcLr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-aa9d70902307>\u001b[0m in \u001b[0;36mcalcLr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# 実際にモデルを構築する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataXForPredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataTForPredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Lab00-XpiMESyv/lib/python3.9/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    519\u001b[0m                                    y_numeric=True, multi_output=True)\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Lab00-XpiMESyv/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Lab00-XpiMESyv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Lab00-XpiMESyv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Lab00-XpiMESyv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Lab00-XpiMESyv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb\n",
    "\n",
    "\n",
    "# 引数として渡されたDFから、関数ごとに「関数名 | ベンチマーク名 | 説明変数 | 目的変数 | 集計結果」を保持したDFを作成する関数\n",
    "# 引数として渡されたDFにはベンチマーク・関数はそれぞれ１種類のデータが格納されている\n",
    "def returnDFtoMakeSummary(inputDF, benchmarkName=\"benchmarkName\", validFunctionName=\"validFunctionName\", targetClass=\"D\", targetProcess=256, expVarColNames=[], resVarColNames=[]):\n",
    "    # モデルを一括で作成\n",
    "    targetDF = inputDF[(inputDF[\"benchmarkClass\"]==targetClass) & (inputDF[\"process\"]==targetProcess)]\n",
    "    dropIndex = inputDF.index[(inputDF[\"benchmarkClass\"]==targetClass) | (inputDF[\"process\"]==targetProcess)]\n",
    "    droppedInputDF = inputDF.drop(dropIndex)\n",
    "    models = Models(inputDF=droppedInputDF, expVarColNames=expVarColNames, resVarColNames=resVarColNames, targetDF=targetDF)\n",
    "    # 学習\n",
    "    models.setUpDataBeforeCalcLr()\n",
    "    models.calcLr()\n",
    "    # MAPE・相対誤差率を算出\n",
    "    models.calcMAPE()\n",
    "    models.calcRelativeErrorRate()\n",
    "    # 結果の格納\n",
    "    dictAggregateResult = {\"MAPE\":models.returnCalculatedMAPE(), \"relativeErrorRate\":models.returnRelativeErrorRateDict()}\n",
    "    expVarDict = models.returnExpVarDatumDF().to_dict(orient='list')\n",
    "    resVarDict = models.returnResVarDatumDF().to_dict(orient='list')\n",
    "    modelsName = models.returnModelsName()\n",
    "    dictDatumForDF = {\"functionName\":validFunctionName, \"benchmarkName\":benchmarkName, \"expVarDatumDict\":expVarDict, \"resVarDatumDict\":resVarDict, \"modelsName\":modelsName, \"dictAggregateResult\":dictAggregateResult}\n",
    "    listDatumKeysForDF = dictDatumForDF.keys()\n",
    "    listDatumValuesForDF = dictDatumForDF.values()\n",
    "    returnDF = pd.DataFrame(index=listDatumKeysForDF, data=listDatumValuesForDF).T\n",
    "    pass\n",
    "\n",
    "\n",
    "benchmarkNames = ['cg', 'ep', 'ft', 'is', 'lu', 'mg']\n",
    "benchmarkName = 'cg'\n",
    "\n",
    "classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "targetClass = classes[-1]\n",
    "targetProcess = processes[-1]\n",
    "\n",
    "# 学習用生データ\n",
    "DF = returnCollectedExistingData(benchmarkNames=[\n",
    "                                 benchmarkName], classes=classes, processes=processes, csvDirPath=\"./csv_files/\")\n",
    "DFByValidFunction = returnDFwithFunctionsExecUnderAllConditions(\n",
    "    inputDF=DF, classes=classes, processes=processes)\n",
    "# 問題サイズを数値化したカラムを追加\n",
    "listBenchmarkClass = DFByValidFunction[\"benchmarkClass\"].tolist()\n",
    "DFWithNumInBenchmarkClass = DFByValidFunction.assign(\n",
    "    benchmarkClassInNum=convertBenchmarkClasses_problemSizeInNPB(listBenchmarkClass))\n",
    "\n",
    "# すべての条件で実行された関数名のリスト\n",
    "validFunctionNames = list(\n",
    "    set(DFWithNumInBenchmarkClass[\"functionName\"].tolist()))\n",
    "# データ内にあるベンチマーク名のリスト\n",
    "benchmarkNames = set(DFWithNumInBenchmarkClass[\"benchmarkName\"].tolist())\n",
    "\n",
    "# 説明変数のカラム名のリスト\n",
    "expVarColNames = [\"process\", \"benchmarkClassInNum\"]\n",
    "# 目的変数のカラム名のリスト\n",
    "resVarColNames = [\"functionCallNum\"]\n",
    "\n",
    "for benchmarkName in benchmarkNames:\n",
    "    for validFunctionName in validFunctionNames:\n",
    "        # 3モデルを一気に作成するmodels()を利用\n",
    "        inputDFperFunction = DFWithNumInBenchmarkClass[(DFWithNumInBenchmarkClass[\"functionName\"] == validFunctionName) & (DFWithNumInBenchmarkClass[\"benchmarkName\"] == benchmarkName)].reset_index()\n",
    "        targetDFperFunction = inputDFperFunction[(inputDFperFunction[\"benchmarkClass\"]==targetClass) & (inputDFperFunction[\"process\"]==targetProcess)]\n",
    "        # inputDFperFunctionの中で条件に当てはまるデータを削除\n",
    "        dropIndex = inputDFperFunction.index[(inputDFperFunction[\"benchmarkClass\"] == targetClass) | (inputDFperFunction[\"process\"]==targetProcess)]\n",
    "        inputDFperFunction = inputDFperFunction.drop(dropIndex)\n",
    "        modelsPerFunction = Models(inputDF=inputDFperFunction, expVarColNames=expVarColNames, resVarColNames=resVarColNames, targetDF=targetDFperFunction)\n",
    "        modelsPerFunction.setUpDataBeforeCalcLr()\n",
    "        modelsPerFunction.calcLr()\n",
    "        # 学習データに対するMAPEを算出し、変数に結果を保持した辞書を格納\n",
    "        modelsPerFunction.calcMAPE()\n",
    "        dictCalcedMAPE = modelsPerFunction.returnCalculatedMAPE()\n",
    "        # 予測対象データに対する相対誤差率を算出し、変数に結果を保持した辞書を格納\n",
    "        modelsPerFunction.calcRelativeErrorRate()\n",
    "        dictRelativeErrorRate = modelsPerFunction.returnRelativeErrorRateDict()\n",
    "        \n",
    "        # 関数ごとに「関数名 | ベンチマーク名 | 説明変数 | 目的変数 | 集計結果」を保持したDFを作成\n",
    "        expVarDatumDF = modelsPerFunction.returnExpVarDatumDF()\n",
    "        resVarDatumDF = modelsPerFunction.returnResVarDatumDF()\n",
    "        # 説明変数のDFを辞書にする\n",
    "        expVarDatumDict = expVarDatumDF.to_dict(orient='list')\n",
    "        # 目的変数のDFを辞書にする\n",
    "        resVarDatumDict = resVarDatumDF.to_dict(orient='list')\n",
    "        modelsName = modelsPerFunction.returnModelNames()\n",
    "        # 集計結果（MAPE、相対誤差率）\n",
    "        dictAggregateResult = {\"MAPE\":dictCalcedMAPE, \"relativeErrorRate\":dictRelativeErrorRate}\n",
    "        dictDatumForDF = {\"functionName\":validFunctionName, \"benchmarkName\":benchmarkName, \"expVarDatumDict\":expVarDatumDict, \"resVarDatumDict\":resVarDatumDict, \"modelsName\":modelsName, \"dictAggregateResult\":dictAggregateResult}\n",
    "        listDatumKeysForDF = dictDatumForDF.keys()\n",
    "        listDatumValuesForDF = dictDatumForDF.values()\n",
    "        DFperValidFunction = pd.DataFrame(data=listDatumValuesForDF, index=listDatumKeysForDF).T\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def test_returnDFtoMakeSummary():\n",
    "    # 入力用DF、inputDFを作成する\n",
    "    plotX = np.linspace(1, 20, 10)\n",
    "    plotY = np.linspace(21, 40, 10) \n",
    "    # functionCallNum\n",
    "    functionCallNumLin = plotX + 2*plotY + 4\n",
    "    functionCallNumIp  = 1/plotX + 2/plotY  + 4\n",
    "    functionCallNumLog = np.log10(plotX) + 2*np.log10(plotY) + 4\n",
    "    # processes\n",
    "    process = np.linspace(1, 20, 10)\n",
    "    # benchmarkClassInNum\n",
    "    benchmarkClassInNum = np.linspace(21, 40, 10)\n",
    "    # functionName\n",
    "    functionNameLin = \"functionNameLin\"\n",
    "    functionNameIp  = \"functionNameIp\"\n",
    "    functionNameLog = \"functionNameLog\"\n",
    "    # benchmarkName\n",
    "    benchmarkNameLin = \"benchmarkNameLin\"\n",
    "    benchmarkNameIp = \"benchmarkNameIp\"\n",
    "    benchmarkNameLog = \"benchmarkNameLog\"\n",
    "    # benchmarkClass\n",
    "    benchmarkClass = [\"Z\"]*len(benchmarkClassInNum)\n",
    "    benchmarkClass[-1] = \"X\"\n",
    "    \n",
    "    dictForDFatLin = {\"functionCallNum\":functionCallNumLin, \"process\":process, \"benchmarkClassInNum\":benchmarkClassInNum, \"benchmarkClass\":benchmarkClass}\n",
    "    inputDFatLin = pd.DataFrame(dictForDFatLin)\n",
    "    inputDFatLin[\"functionName\"] = functionNameLin\n",
    "    inputDFatLin[\"benchmarkName\"] = benchmarkNameLin\n",
    "    \n",
    "    dictForDFatIp  = {\"functionCallNum\":functionCallNumLin, \"process\":process, \"benchmarkClassInNum\":benchmarkClassInNum, \"benchmarkClass\":benchmarkClass}\n",
    "    inputDFatIp  = pd.DataFrame(dictForDFatIp)\n",
    "    inputDFatIp[\"functionName\"] = functionNameIp\n",
    "    inputDFatIp[\"benchmarkName\"] = benchmarkNameIp\n",
    "    \n",
    "    dictForDFatLog = {\"functionCallNum\":functionCallNumLin, \"process\":process, \"benchmarkClassInNum\":benchmarkClassInNum, \"benchmarkClass\":benchmarkClass}\n",
    "    inputDFatLog = pd.DataFrame(dictForDFatLog)\n",
    "    inputDFatLog[\"functionName\"] = functionNameLog\n",
    "    inputDFatLog[\"benchmarkName\"] = benchmarkNameLog\n",
    "    \n",
    "    # 関数の実行に必要な引数を作成する\n",
    "    targetClass = benchmarkClass[-1]\n",
    "    targetProcess = processes[-1]\n",
    "    \n",
    "    # returnDFtoMakeSummary()の実行\n",
    "    resultAtLin = returnDFtoMakeSummary(inputDF = inputDFatLin, targetClass=targetClass, targetProcess=targetProcess)\n",
    "    resultAtIp  = returnDFtoMakeSummary(inputDF = inputDFatIp,  targetClass=targetClass, targetProcess=targetProcess)\n",
    "    resultAtLog = returnDFtoMakeSummary(inputDF = inputDFatLog, targetClass=targetClass, targetProcess=targetProcess)\n",
    "    \n",
    "    # linについて\n",
    "    assert len(resultAtLin) == 1\n",
    "    # functionName\n",
    "    assert functionNameAtLin[0] == functionNameLin\n",
    "    # benchmarkName\n",
    "    assert benchmarkNameAtLinResult[0] == benchmarkNameLin\n",
    "    # expVarDatumDict\n",
    "    expVarDictLinResult = resultAtLin.at[resultAtLin.index[0], \"expVarDatumDict\"]\n",
    "    processLinResult = expVarDictLinResult[\"processes\"]\n",
    "    benchmarkClassInNumLinResult = expVarDictLinResult[\"benchmarkClassInNum\"]\n",
    "    assert processLinResult == processes\n",
    "    assert benchmarClassInNumLinResult == benchmarkClassInNum\n",
    "    # resVarDatumDict\n",
    "    resVarDictLinResult = resultAtLin.at[resultAtLin.index[0], \"resVarDatumDict\"]\n",
    "    functionCallNumLinResult = resVarDictLinResult[\"functionCallNum\"]\n",
    "    assert functionCallNumLinResult == functionCallNumLin\n",
    "    # modelsName\n",
    "    modelsNameLinResult = resultAtLin.at[resultAtLin.index[0], \"modelsName\"]\n",
    "    assert \"modelLin\" in modelsNameLinResult\n",
    "    assert \"modelIp\" in modelsNameLinResult\n",
    "    assert \"modelLog\" in modelsNameLinResult\n",
    "    # dictAggregateResult\n",
    "    dictAggregateResult = resultAtlin.at[resultAtLin.index[0], \"dictAggregateResult\"]\n",
    "    MAPELinResult = dictAggregateResult[\"MAPE\"]\n",
    "    assert MapeLinResult[\"modelLin\"] < 1.0\n",
    "    relativeErrorRateLinResult = dictAggregateResult[\"MAPE\"]\n",
    "    assert relativeErrorRateLinResult[\"modelLin\"] < 1.0\n",
    "    \n",
    "    # Ipについて\n",
    "    assert len(resultAtIp) == 1\n",
    "    # functionName\n",
    "    assert functionNameAtIp[0] == functionNameIp\n",
    "    # benchmarkName\n",
    "    assert benchmarkNameAtIpResult[0] == benchmarkNameIp\n",
    "    # expVarDatumDict\n",
    "    expVarDictIpResult = resultAtIp.at[resultAtIp.index[0], \"expVarDatumDict\"]\n",
    "    processIpResult = expVarDictIpResult[\"processes\"]\n",
    "    benchmarkClassInNumIpResult = expVarDictIpResult[\"benchmarkClassInNum\"]\n",
    "    assert processIpResult == processes\n",
    "    assert benchmarClassInNumIpResult == benchmarkClassInNum\n",
    "    # resVarDatumDict\n",
    "    resVarDictIpResult = resultAtIp.at[resultAtIp.index[0], \"resVarDatumDict\"]\n",
    "    functionCallNumIpResult = resVarDictIpResult[\"functionCallNum\"]\n",
    "    assert functionCallNumIpResult == functionCallNumIp\n",
    "    # modelsName\n",
    "    modelsNameIpResult = resultAtIp.at[resultAtIp.index[0], \"modelsName\"]\n",
    "    assert \"modelLin\" in modelsNameIpResult\n",
    "    assert \"modelIp\"  in modelsNameIpResult\n",
    "    assert \"modelLog\" in modelsNameIpResult\n",
    "    # dictAggregateResult\n",
    "    dictAggregateResult = resultAtIp.at[resultAtIp.index[0], \"dictAggregateResult\"]\n",
    "    MAPEIpResult = dictAggregateResult[\"MAPE\"]\n",
    "    assert MapeIpResult[\"modelIp\"] < 1.0\n",
    "    relativeErrorRateIpResult = dictAggregateResult[\"MAPE\"]\n",
    "    assert relativeErrorRateIpResult[\"modelIp\"] < 1.0\n",
    "    \n",
    "    # Logについて\n",
    "    assert len(resultAtLog) == 1\n",
    "    # functionName\n",
    "    assert functionNameAtLog[0] == functionNameLog\n",
    "    # benchmarkName\n",
    "    assert benchmarkNameAtLogResult[0] == benchmarkNameLog\n",
    "    # expVarDatumDict\n",
    "    expVarDictLogResult = resultAtLog.at[resultAtLog.index[0], \"expVarDatumDict\"]\n",
    "    processLogResult = expVarDictLogResult[\"processes\"]\n",
    "    benchmarkClassInNumLogResult = expVarDictLogResult[\"benchmarkClassInNum\"]\n",
    "    assert processLogResult == processes\n",
    "    assert benchmarClassInNumLogResult == benchmarkClassInNum\n",
    "    # resVarDatumDict\n",
    "    resVarDictLogResult = resultAtLog.at[resultAtLog.index[0], \"resVarDatumDict\"]\n",
    "    functionCallNumIpResult = resVarDictLogResult[\"functionCallNum\"]\n",
    "    assert functionCallNumLogResult == functionCallNumLog\n",
    "    # modelsName\n",
    "    modelsNameLogResult = resultAtLog.at[resultAtLog.index[0], \"modelsName\"]\n",
    "    assert \"modelLin\" in modelsNameLogResult\n",
    "    assert \"modelIp\"  in modelsNameLogResult\n",
    "    assert \"modelLog\" in modelsNameLogResult\n",
    "    # dictAggregateResult\n",
    "    dictAggregateResult = resultAtlog.at[resultAtLog.index[0], \"dictAggregateResult\"]\n",
    "    MAPELogResult = dictAggregateResult[\"MAPE\"]\n",
    "    assert MapeLogResult[\"modelLog\"] < 1.0\n",
    "    relativeErrorRateLogResult = dictAggregateResult[\"MAPE\"]\n",
    "    assert relativeErrorRateLogResult[\"modelLog\"] < 1.0\n",
    "    \n",
    "test_returnDFtoMakeSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionName</th>\n",
       "      <th>benchmarkName</th>\n",
       "      <th>expVarDatumDict</th>\n",
       "      <th>resVarDatumDict</th>\n",
       "      <th>modelsName</th>\n",
       "      <th>dictAggregateResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONJ_GRAD</td>\n",
       "      <td>cg</td>\n",
       "      <td>{'process': [2, 4, 8, 16, 32, 64, 128, 2, 4, 8...</td>\n",
       "      <td>{'functionCallNum': [16.0, 16.0, 16.0, 16.0, 1...</td>\n",
       "      <td>[modelLin, modelIp, modelLog]</td>\n",
       "      <td>{'MAPE': {'modelLin': [51.0005706761659], 'mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  functionName benchmarkName  \\\n",
       "0    CONJ_GRAD            cg   \n",
       "\n",
       "                                     expVarDatumDict  \\\n",
       "0  {'process': [2, 4, 8, 16, 32, 64, 128, 2, 4, 8...   \n",
       "\n",
       "                                     resVarDatumDict  \\\n",
       "0  {'functionCallNum': [16.0, 16.0, 16.0, 16.0, 1...   \n",
       "\n",
       "                      modelsName  \\\n",
       "0  [modelLin, modelIp, modelLog]   \n",
       "\n",
       "                                 dictAggregateResult  \n",
       "0  {'MAPE': {'modelLin': [51.0005706761659], 'mod...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFperValidFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionName</th>\n",
       "      <th>functionCallNum</th>\n",
       "      <th>benchmarkName</th>\n",
       "      <th>benchmarkClass</th>\n",
       "      <th>process</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.TAU_application</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CG</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MPI_Finalize()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INITIALIZE_MPI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MPI_Init()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MPI_Reduce()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>D</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SETUP_SUBMATRIX_INFO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>D</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MPI_Comm_size()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>D</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MPI_Comm_rank()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>D</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SETUP_PROC_INFO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cg</td>\n",
       "      <td>D</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            functionName  functionCallNum benchmarkName benchmarkClass  \\\n",
       "0       .TAU_application              1.0            cg              A   \n",
       "1                     CG              1.0            cg              A   \n",
       "2         MPI_Finalize()              1.0            cg              A   \n",
       "3         INITIALIZE_MPI              1.0            cg              A   \n",
       "4             MPI_Init()              1.0            cg              A   \n",
       "..                   ...              ...           ...            ...   \n",
       "17          MPI_Reduce()              1.0            cg              D   \n",
       "18  SETUP_SUBMATRIX_INFO              1.0            cg              D   \n",
       "19       MPI_Comm_size()              1.0            cg              D   \n",
       "20       MPI_Comm_rank()              1.0            cg              D   \n",
       "21       SETUP_PROC_INFO              1.0            cg              D   \n",
       "\n",
       "    process  \n",
       "0         2  \n",
       "1         2  \n",
       "2         2  \n",
       "3         2  \n",
       "4         2  \n",
       "..      ...  \n",
       "17      256  \n",
       "18      256  \n",
       "19      256  \n",
       "20      256  \n",
       "21      256  \n",
       "\n",
       "[704 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'functionCallNumLin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-06e58a484e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0;34m\"functionCallNum\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfunctionCallNumLin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"processes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"benchmarkClassInNum\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbenchmarkClassInNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"benchmarkClass\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbenchmarkClass\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'functionCallNumLin' is not defined"
     ]
    }
   ],
   "source": [
    "{\"functionCallNum\":functionCallNumLin, \"processes\":processes, \"benchmarkClassInNum\":benchmarkClassInNum, \"benchmarkClass\":benchmarkClass}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
