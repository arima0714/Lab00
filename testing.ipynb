{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修正したモデルから卒論時に集計したデータを作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 表\n",
    "\n",
    "| ベンチマーク名 | 平均誤差率(%) | コスト比(%) |\n",
    "|---------|----------|---------|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 表\n",
    "\n",
    "| ベンチマーク名 | 採用割合(最大MAPE(%), 最小MAPE(%)) |\n",
    "|---------|----------------------------|\n",
    "|         | モデル(1), モデル(2), ...        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 平均誤差率：大規模実行時の関数コール回数との比較\n",
    "* MAPE：トレーニングデータとの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.xlabel(\"使用したプロファイル数\")\n",
    "plt.ylabel(\"平均誤差率(%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_option(\"display.max_columns\")\n",
    "# pd.get_option(\"display.max_rows\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = \"cg\"\n",
    "fix = \"Class\"\n",
    "if benchmark == \"bt\" or benchmark == \"sp\":\n",
    "    processes = processes_onlyBTSP\n",
    "else:\n",
    "    processes = processes_excludeBTSP\n",
    "benchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "targetNumOfProcess = 256\n",
    "fixedBenchmarkClass = \"B\"\n",
    "fixedProcess = 64\n",
    "\n",
    "# 引数の条件に合った生の実験データを取得する\n",
    "DF = returnRawDFperBenchmark(Benchmark=benchmark, fix=fix, benchmarkClass=benchmarkClasses,\n",
    "                             FixedProcess=fixedProcess, Processes=processes, FixedBenchmarkClass=fixedBenchmarkClass)\n",
    "# 取得した生の実験データから NaN が含まれる関数の実験データを削除\n",
    "noNaNDF = DF.dropna(how='any')\n",
    "# noNaNDF\n",
    "\n",
    "returnedCalculatedDF = return_calculatedDF(benchmark=benchmark, noNaNDF=noNaNDF,\n",
    "                                           targetNumOfProcess=targetNumOfProcess, targetProblemSize=fixedBenchmarkClass, fix=fix)\n",
    "# returnedCalculatedDF\n",
    "\n",
    "# numOfData列の要素一覧を作成し、ソートされたリストを、listOfNumDataに格納する\n",
    "listOfNumOfData = returnedCalculatedDF['numOfData'].tolist()\n",
    "listOfNumOfData = sorted(list(set(listOfNumOfData)))\n",
    "# listOfNumOfData\n",
    "\n",
    "\n",
    "# 使用したプロファイル数をキー・最適モデルでの相対誤差の平均をバリューとした辞書を作成する\n",
    "dictAverageRelativeErrorOfBestModel = {}\n",
    "x = []\n",
    "y = []\n",
    "for numOfData in listOfNumOfData:\n",
    "    # 使用したプロファイル数で抽出\n",
    "    extractedPerNumOfProfileDF = returnedCalculatedDF[returnedCalculatedDF['numOfData'] == numOfData]\n",
    "    meanDF = extractedPerNumOfProfileDF.mean()\n",
    "    data = meanDF.at['relativeErrorOfBestModel']\n",
    "    dictAverageRelativeErrorOfBestModel[numOfData] = data\n",
    "\n",
    "x = list(dictAverageRelativeErrorOfBestModel.keys())\n",
    "x\n",
    "y = [dictAverageRelativeErrorOfBestModel[key] for key in x]\n",
    "y\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, marker='o')\n",
    "plt.xlabel(\"使用したプロファイル数[％]\")\n",
    "plt.ylabel(\"平均絶対誤差率\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarksからbt, spを除外する\n",
    "benchmarks = [benchmark for benchmark in benchmarks if benchmark !=\n",
    "              'bt' and benchmark != 'sp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictTmp = returnDictForPlotPerNumOfUsedData(Benchmark=benchmarks, fix=\"Class\", benchmarkClass=[\n",
    "    \"A\", \"B\", \"C\", \"D\"], FixedProcess=64, Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDF = pd.DataFrame()\n",
    "for benchmark in benchmarks:\n",
    "    listToLearn = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    listToPredict = [256]\n",
    "    benchmark_x = dictTmp[benchmark][\"x\"]\n",
    "    benchmark_y = dictTmp[benchmark][\"y\"]\n",
    "    index = benchmark_x.index(len(listToLearn))\n",
    "    MAPE = benchmark_y[index]\n",
    "    relativeCost = returnRelativeCost(benchmark=benchmark, variablesToLearn=listToLearn,\n",
    "                                      variablesToPredict=listToPredict, fixedClassOrProcess=\"Class\", fixed=\"C\")\n",
    "    dictRowData = {\"ベンチマーク名\": benchmark, \"平均絶対誤差率\": int(10*MAPE)/10, \"相対コスト\": int(10*relativeCost)/10}\n",
    "    iDF = pd.DataFrame.from_dict(dictRowData, orient='index').T\n",
    "    tmpDF = tmpDF.append(iDF)\n",
    "print(tmpDF.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictTmp\n",
    "\n",
    "plt.figure()\n",
    "for benchmark in list(dictTmp.keys()):\n",
    "    x = dictTmp[benchmark][\"x\"]\n",
    "    y = dictTmp[benchmark][\"y\"]\n",
    "    plt.plot(x, y, marker='o', label=benchmark.upper())\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"使用したプロファイル数\")\n",
    "    plt.ylabel(\"平均絶対誤差率[％]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 実際にプロットする\n",
    "\n",
    "\n",
    "# print(f\"fix={fix}, benchmarkClasses={benchmarkClasses}, fixedProcess={fixedProcess}, Processes={processes}, FixedBenchmarkClass={fixedBenchmarkClass}\")\n",
    "# print(f\"targetNumOfProcess={targetNumOfProcess}, targetProblemSize={fixedBenchmarkClass}, fix={fix}\")\n",
    "\n",
    "# DF = returnRawDFperBenchmark(Benchmark=\"mg\", fix=\"Process\", benchmarkClass=[\"A\", \"B\", \"C\", \"D\"], Processes=[\n",
    "#                              1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"B\", FixedProcess=64)\n",
    "# DF.dropna(how='any')\n",
    "# DF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
