{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362d8d5f-88a4-4b3c-93ff-9d4539b7700a",
   "metadata": {},
   "source": [
    "# lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2dad4-a784-4a5a-8e94-d8556cd66bf7",
   "metadata": {},
   "source": [
    "# 作成した各種関数などをまとめるノート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c41a1a-c5c6-4fe3-86d1-07cd31cc4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import japanize_matplotlib\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as sp\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899e81b-9313-400c-ba40-8092861a14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブック中で変数のみを記述することでデータフレームをきれいに表示させる設定の有効化\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f033b7f-0e64-4072-81e0-0c1d2eeb5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各種値を宣言\n",
    "benchmarks = ['bt', 'cg', 'ep', 'ft', 'is', 'lu', 'mg', 'sp']\n",
    "classes = [\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "fix_process = 4\n",
    "fix_benchmark_class = \"C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcf6aa-6b4e-469e-871b-6d2dab2cd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_fixed_class(BenchMark=\"bt\", Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchMarkClass=\"C\"):\n",
    "    path = './csv_files/'\n",
    "\n",
    "    # fixed_Class にはベンチマーククラスFixedBenchMarkClassで実行プロセス数がProcessesに該当するものの結果が入る\n",
    "    fixed_Class = []\n",
    "\n",
    "    for process in Processes:\n",
    "        file_name = (\"pprof_\"+BenchMark +\n",
    "                     FixedBenchMarkClass+str(process)+\".csv\")\n",
    "        file_path = path+file_name\n",
    "        if (os.path.exists(file_path) and os.stat(file_path).st_size != 0):\n",
    "            data_frame = pd.read_csv(path+file_name)\n",
    "            data_frame = data_frame.set_index(['Name'])\n",
    "            fixed_Class.append(data_frame.rename(\n",
    "                columns={'#Call': process}).sort_index())\n",
    "    return(fixed_Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45aa10b-3f2b-4d1c-a07a-1f1c80380262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fixed_class_graph(BenchMark=\"bt\", Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchMarkClass=\"C\"):\n",
    "\n",
    "    markers = [\".\", \",\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\", \"d\", \"|\", \"_\", \"None\", None, \"\", \"$x$\",\n",
    "               \"$\\\\alpha$\", \"$\\\\beta$\", \"$\\\\gamma$\"]\n",
    "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3',\n",
    "              '#ff7f00', '#ffff33', '#a65628', '#f781bf']\n",
    "    fixed_Class = return_fixed_class(\n",
    "        BenchMark=BenchMark, Processes=Processes, FixedBenchMarkClass=FixedBenchMarkClass)\n",
    "    if(len(fixed_Class) != 0):\n",
    "        summary_fixed_Class = pd.concat(fixed_Class, axis=1)\n",
    "        summary_fixed_Class.sort_index(axis=1, inplace=True)\n",
    "        summary_fixed_Class_title = BenchMark + \\\n",
    "            \"においてベンチマーククラスをCに固定し実行プロセス数を変化させたときの実行された全ての関数のコール回数\"\n",
    "\n",
    "        x_axes = summary_fixed_Class.columns.tolist()\n",
    "\n",
    "        y_axes = summary_fixed_Class.index.tolist()\n",
    "\n",
    "        plt.figure()\n",
    "        for y_axis in y_axes:\n",
    "            color = random.choice(colors)\n",
    "            label = y_axis\n",
    "            marker = random.choice(markers)\n",
    "            plt.plot(\n",
    "                x_axes, summary_fixed_Class.T[y_axis], marker=marker, label=y_axis)\n",
    "        plt.legend()\n",
    "        plt.title(BenchMark+\"_FixedBenchMarkClass=\"+FixedBenchMarkClass)\n",
    "        plt.show()\n",
    "\n",
    "# 使用例\n",
    "# show_fixed_class_graph(BenchMark=\"cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef890ff7-1cd7-43b0-8a08-a38a5da83f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_fixed_process(BenchMark=\"bt\", BenchMarkClasses=[\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"], FixedProcess=32):\n",
    "    path = './csv_files/'\n",
    "\n",
    "    # fixed_process には実行プロセス数が64でベンチマーククラスがA ~ Dまでの結果が入る\n",
    "    fixed_process = list()\n",
    "\n",
    "    for bench_mark_class in BenchMarkClasses:\n",
    "        file_name = (\"pprof_\"+BenchMark+bench_mark_class +\n",
    "                     str(FixedProcess)+\".csv\")\n",
    "        file_path = path+file_name\n",
    "        if (os.path.exists(file_path) and os.stat(file_path).st_size != 0):\n",
    "            data_frame = pd.read_csv(path+file_name)\n",
    "            data_frame = data_frame.set_index(['Name'])\n",
    "            fixed_process.append(data_frame.rename(\n",
    "                columns={'#Call': bench_mark_class}).sort_index())\n",
    "    return(fixed_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb050fe-2413-474d-b5ba-a7fdeb77e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_fixed_process_graph(BenchMark=\"bt\", BenchMarkClasses=[\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"], FixedProcess=32):\n",
    "    fixed_process = return_fixed_process(\n",
    "        BenchMark=BenchMark, BenchMarkClasses=BenchMarkClasses, FixedProcess=FixedProcess)\n",
    "    markers = [\".\", \",\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\", \"d\", \"|\", \"_\", \"None\", None, \"\", \"$x$\",\n",
    "               \"$\\\\alpha$\", \"$\\\\beta$\", \"$\\\\gamma$\"]\n",
    "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3',\n",
    "              '#ff7f00', '#ffff33', '#a65628', '#f781bf']\n",
    "    if (len(fixed_process) != 0):\n",
    "        summary_fixed_process = pd.concat(fixed_process, axis=1)\n",
    "\n",
    "        x_axes = summary_fixed_process.columns.tolist()\n",
    "\n",
    "        y_axes = summary_fixed_process.index.tolist()\n",
    "\n",
    "        plt.figure()\n",
    "        for y_axis in y_axes:\n",
    "            color = random.choice(colors)\n",
    "            label = y_axis\n",
    "            marker = random.choice(markers)\n",
    "            plt.plot(\n",
    "                x_axes, summary_fixed_process.T[y_axis], marker=marker, label=y_axis)\n",
    "        plt.legend()\n",
    "        plt.title(BenchMark+\"_FixedProcess=\"+str(FixedProcess))\n",
    "        plt.show()\n",
    "\n",
    "# 使用例\n",
    "# show_fixed_process_graph(BenchMark=\"cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9998d9bc-6e40-4785-a469-cb17d4d6ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(BenchMarks=[], Processes=[], BenchMarkClasses=[], fix_process=4, fix_benchmark_class=\"C\"):\n",
    "\n",
    "    if (BenchMarks == [] or Processes == [] or BenchMarkClasses == []):\n",
    "        print(\"関数の引数となっている配列が空です。\")\n",
    "    else:\n",
    "        for bench_mark in BenchMarks:\n",
    "            show_fixed_class_graph(\n",
    "                BenchMark=bench_mark, Processes=Processes, FixedBenchMarkClass=fix_benchmark_class)\n",
    "            show_fixed_process_graph(\n",
    "                BenchMark=bench_mark, BenchMarkClasses=BenchMarkClasses, FixedProcess=fix_process)\n",
    "\n",
    "\n",
    "bench_marks = ['bt', 'cg', 'ep', 'ft', 'is', 'lu', 'mg', 'sp']\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "classes = [\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# 使用例\n",
    "# show_graph(bench_marks, processes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908d6b8-1c6b-40ee-8e24-b83098d24295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_summarized_Fixed_dataframe(BenchMark_name=\"cg\", fixed=\"class\"):\n",
    "\n",
    "    def evaluate_dataframes(df1, df2):\n",
    "        for i in range(len(df1.values.tolist()[0])):\n",
    "            if(df1.values.tolist()[0][i] != df2.values.tolist()[0][i]):\n",
    "                return False\n",
    "        return True\n",
    "    fixed_df = 0\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_df = return_fixed_class(BenchMark=BenchMark_name)\n",
    "    elif (fixed == \"process\"):\n",
    "        fixed_df = return_fixed_process(BenchMark=BenchMark_name)\n",
    "    summary_fixed_df = pd.concat(fixed_df, axis=1)\n",
    "    dropped_summary_fixed_df = summary_fixed_df.drop_duplicates()\n",
    "    dropped_summary_fixed_df_renamed = dropped_summary_fixed_df\n",
    "\n",
    "    for dropped_index_name in dropped_summary_fixed_df.index.values:\n",
    "        dropped_index_name_data = summary_fixed_df.loc[[dropped_index_name]]\n",
    "        replace_name = dropped_index_name\n",
    "        for all_index_name in summary_fixed_df.index.values:\n",
    "            all_index_name_data = summary_fixed_df.loc[[all_index_name]]\n",
    "            if(dropped_index_name == all_index_name):\n",
    "                pass\n",
    "            elif(evaluate_dataframes(dropped_index_name_data, all_index_name_data)):\n",
    "                replace_name += f\", {all_index_name}\"\n",
    "        dropped_summary_fixed_df_renamed = dropped_summary_fixed_df_renamed.rename(\n",
    "            index={dropped_index_name: replace_name})\n",
    "\n",
    "    return dropped_summary_fixed_df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c98ed0-da6c-4794-b318-0a162e4c0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均絶対パーセント誤差 (MAPE)(Mean Absolute Percent Error (MAPE))を返す関数\n",
    "# 引数として長さの同じ二つのリストをとる\n",
    "# 引数l1: 実測値のリスト\n",
    "# 引数l2: 予測値のリスト\n",
    "\n",
    "def mape_score(l1, l2):\n",
    "    return_num = 0\n",
    "    if(len(l1) != len(l2)):\n",
    "        print(\"引数のリストの長さが異なります\", end=\", \")\n",
    "        return -1\n",
    "    for i in range(len(l1)):\n",
    "        l1_num = l1[i]\n",
    "        l2_num = l2[i]\n",
    "\n",
    "        return_num += abs((l1_num - l2_num)/l1_num)\n",
    "\n",
    "    return_num /= len(l1)\n",
    "    return_num *= 100\n",
    "    return return_num\n",
    "\n",
    "\n",
    "# 使用例：mape_score([1,2,3,4], [4,3,2,1])\n",
    "type(mape_score([1, 2, 3, 4], [4, 3, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f93771-6bb7-41a1-88d9-2844c996cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_ratio(base_list: list, test_ratio: float):\n",
    "    test_index = math.floor(len(base_list) * float(test_ratio))\n",
    "    train_list = base_list[:-test_index]\n",
    "    test_list = base_list[-test_index:]\n",
    "    if(test_ratio == 0):\n",
    "        return base_list, []\n",
    "    return train_list, test_list\n",
    "\n",
    "\n",
    "class ModelBase:\n",
    "    def __init__(self, raw_x, raw_y, benchmark_name=\"benchmark_name\", function_name=\"function_name\", test_ratio=0.3):\n",
    "        self.benchmark_name = benchmark_name\n",
    "        self.function_name = function_name\n",
    "        self.xlabel = \"実行時のプロセス数\"\n",
    "        self.ylabel = \"プロセスごとの関数コール回数\"\n",
    "\n",
    "        self.raw_x = np.reshape(raw_x, (-1, 1))\n",
    "        self.raw_y = np.reshape(raw_y, (-1, 1))\n",
    "        self.train_x, self.test_x = split_by_ratio(self.raw_x, test_ratio)\n",
    "        self.train_y, self.test_y = split_by_ratio(self.raw_y, test_ratio)\n",
    "\n",
    "        if(len(self.train_x) == len(self.test_x) or len(self.train_y) == len(self.test_y)):\n",
    "            print(f\"学習用とテスト用にデータを分割するのに問題が生じています。@{benchmark_name}\")\n",
    "            print(f\"len(self.train_x) == {len(self.train_x)}\")\n",
    "            print(f\"len(self.train_y) ==  {len(self.train_y)}\")\n",
    "            print(f\"len(self.test_x) == {len(self.test_x)}\")\n",
    "            print(f\"len(self.test_y) == {len(self.test_y)}\")\n",
    "\n",
    "        self.x_model_line = np.reshape(\n",
    "            np.arange(start=0.1, stop=self.raw_x.max(), step=0.1), (-1, 1))\n",
    "        self.y_model_line = 0\n",
    "\n",
    "        self.lr = 0\n",
    "        self.r2_score = 0\n",
    "\n",
    "    def calc_lr(self):\n",
    "        self.lr = 0\n",
    "\n",
    "    def calc_r2_score(self):\n",
    "        self.r2_score = 0\n",
    "\n",
    "    def calc_mae_score(self):\n",
    "        self.mae_score = 0\n",
    "\n",
    "    def calc_mse_score(self):\n",
    "        self.mse_score = 0\n",
    "\n",
    "    def calc_rmse_score(self):\n",
    "        self.rmse_score = 0\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        self.mape_score = 0\n",
    "\n",
    "    def plot_graph(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.raw_x, self.raw_y, color=\"red\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee91c25-f09c-4623-bfd0-8a0f340ba49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLin(ModelBase):\n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.train_x, self.train_y)\n",
    "\n",
    "    def calc_r2_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.r2_score = r2_score(self.test_y, test_y_predicted)\n",
    "\n",
    "    def calc_mae_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mae_score = mean_absolute_error(self.test_y, test_y_predicted)\n",
    "\n",
    "    def calc_mse_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mse_score = mean_squared_error(self.test_y, test_y_predicted)\n",
    "\n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted = self.lr.predict(self.train_x)\n",
    "        self.mape_score_InTrain = float(\n",
    "            mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def plot_graph(self, save=False, fileName=\"graph.pdf\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        self.y_model_line = self.lr.predict(self.x_model_line)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if(save):\n",
    "            plt.savefig(fileName)\n",
    "\n",
    "    def predict(self, num):\n",
    "        predicted = self.lr.predict(num)\n",
    "        return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6170e9-2a62-4cf2-8eb4-661cb31f7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverter_log10_func(x):\n",
    "    return 10**x\n",
    "\n",
    "\n",
    "class ModelLog10(ModelBase):\n",
    "\n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.transformer_log10 = sp.FunctionTransformer(\n",
    "            func=np.log10, inverse_func=inverter_log10_func)\n",
    "        x_train_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        y_train_log10 = self.transformer_log10.transform(self.train_y)\n",
    "\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(x_train_log10, y_train_log10)\n",
    "\n",
    "    def calc_r2_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(\n",
    "            train_y_predicted_log10)\n",
    "        self.r2_score = r2_score(self.train_y, train_y_predicted)\n",
    "\n",
    "    def calc_mae_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(\n",
    "            train_y_predicted_log10)\n",
    "        self.mae_score = mean_absolute_error(self.train_y, train_y_predicted)\n",
    "\n",
    "    def calc_mse_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(\n",
    "            train_y_predicted_log10)\n",
    "        self.mse_score = mean_squared_error(self.train_y, train_y_predicted)\n",
    "\n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_x_log10 = self.transformer_log10.transform(self.test_x)\n",
    "        test_y_predicted_log10 = self.lr.predict(test_x_log10)\n",
    "        test_y_predicted = self.transformer_log10.inverse_transform(\n",
    "            test_y_predicted_log10)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(\n",
    "            train_y_predicted_log10)\n",
    "        self.mape_score_InTrain = float(\n",
    "            mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def plot_graph(self, save=False, fileName=\"graph.pdf\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        x_model_line_log10 = self.transformer_log10.transform(\n",
    "            self.x_model_line)\n",
    "        y_model_line_log10 = self.lr.predict(x_model_line_log10)\n",
    "        self.y_model_line = self.transformer_log10.inverse_transform(\n",
    "            y_model_line_log10)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if(save):\n",
    "            plt.savefig(fileName)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num_log10 = self.transformer_log10.transform(num)\n",
    "        predicted_log10 = self.lr.predict(num_log10)\n",
    "        predicted = self.transformer_log10.inverse_transform(predicted_log10)\n",
    "        return(predicted)\n",
    "\n",
    "    def return_coef_(self):\n",
    "        return self.lr.coef_\n",
    "\n",
    "    def return_intercept_(self):\n",
    "        return self.lr.intercept_\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLog10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8832772-2bd9-4b4b-99e0-571474fc0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse proportion\n",
    "def ip_func(x):\n",
    "    return 1/x\n",
    "\n",
    "\n",
    "class ModelIP(ModelBase):\n",
    "\n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.transformer_ip = sp.FunctionTransformer(\n",
    "            func=ip_func, inverse_func=ip_func)\n",
    "        y_train_ip = self.transformer_ip.transform(self.train_y)\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.train_x, y_train_ip)\n",
    "\n",
    "    def calc_r2_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(\n",
    "            train_y_predicted_ip)\n",
    "        self.r2_score = r2_score(self.train_y, train_y_predicted)\n",
    "\n",
    "    def calc_mae_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(\n",
    "            train_y_predicted_ip)\n",
    "        self.mae_score = mean_absolute_error(self.train_y, train_y_predicted)\n",
    "\n",
    "    def calc_mse_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(\n",
    "            train_y_predicted_ip)\n",
    "        self.mse_score = mean_squared_error(self.train_y, train_y_predicted)\n",
    "\n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted_ip = self.lr.predict(self.test_x)\n",
    "        test_y_predicted = self.transformer_ip.inverse_transform(\n",
    "            test_y_predicted_ip)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(\n",
    "            train_y_predicted_ip)\n",
    "        self.mape_score_InTrain = float(\n",
    "            mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def plot_graph(self, save=False, fileName=\"graph.pdf\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        y_model_line_ip = self.lr.predict(self.x_model_line)\n",
    "        self.y_model_line = self.transformer_ip.inverse_transform(\n",
    "            y_model_line_ip)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if(save):\n",
    "            plt.savefig(fileName)\n",
    "\n",
    "    def predict(self, num):\n",
    "        predicted_ip = self.lr.predict(num)\n",
    "        predicted = self.transformer_ip.inverse_transform(predicted_ip)\n",
    "        return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992374c0-0bba-4558-967c-94364620a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBranch(ModelBase):\n",
    "\n",
    "    def calc_lr(self, AllData=False):\n",
    "        # 後述する t を算出するための処理\n",
    "        max_in_train_y = max(self.train_y)\n",
    "        max_in_train_y_first_index = self.train_y.tolist().index(max_in_train_y)\n",
    "        # 分岐点のインデックスを t とする\n",
    "        t = max_in_train_y_first_index\n",
    "        self.t = t\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.train_x, self.train_y)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.train_x, self.train_y)\n",
    "        else:\n",
    "            self.x_train_1 = self.train_x[:t]\n",
    "            self.x_train_2 = self.train_x[t:]\n",
    "            self.y_train_1 = self.train_y[:t]\n",
    "            self.y_train_2 = self.train_y[t:]\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.x_train_1, self.y_train_1)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.x_train_2, self.y_train_2)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            test_y_predicted = self.lr1.predict(self.test_x)\n",
    "            self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "        else:\n",
    "            x_test = self.test_x\n",
    "            y_test = self.test_y\n",
    "            y_test_predicted = self.lr2.predict(x_test)\n",
    "            self.mape_score = float(mape_score(y_test, y_test_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            train_y_predicted = self.lr1.predict(self.train_x)\n",
    "            self.mape_score_InTrain = float(\n",
    "                mape_score(self.train_y, train_y_predicted))\n",
    "        else:\n",
    "            train_y_predicted_1 = self.lr1.predict(self.x_train_1)\n",
    "            train_y_predicted_2 = self.lr2.predict(self.x_train_2)\n",
    "            mape_1 = float(mape_score(self.y_train_1, train_y_predicted_1))\n",
    "            mape_2 = float(mape_score(self.y_train_2, train_y_predicted_2))\n",
    "            self.mape_score_InTrain = (mape_1 + mape_2) / 2\n",
    "\n",
    "    def plot_graph(self, save=False, fileName=\"graph.pdf\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            y_model_line = self.lr.predict(x_model_line)\n",
    "            plt.plot(self.x_model_line, y_model_line, color=\"red\")\n",
    "        else:\n",
    "            # 回帰曲線を二つのモデルで分割するための処理\n",
    "            x_model_line = self.x_model_line\n",
    "            t_in_model_line = 0\n",
    "            for i in range(len(x_model_line)):\n",
    "                if (self.train_x[self.t] < x_model_line[i]):\n",
    "                    t_in_model_line = i\n",
    "                    break\n",
    "                else:\n",
    "                    t_in_model_line = i\n",
    "\n",
    "            x_model_line1 = self.x_model_line[:t_in_model_line]\n",
    "            x_model_line2 = self.x_model_line[t_in_model_line:]\n",
    "            y_model_line1 = self.lr1.predict(x_model_line1)\n",
    "            y_model_line2 = self.lr2.predict(x_model_line2)\n",
    "\n",
    "            plt.plot(x_model_line1, y_model_line1, color=\"red\")\n",
    "            plt.plot(x_model_line2, y_model_line2, color=\"red\")\n",
    "    #         plt.plot(self.test_x, self.test_y, color=\"yellow\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if(save):\n",
    "            plt.savefig(fileName)\n",
    "\n",
    "    def predict(self, num):\n",
    "        #         if num < self.raw_x[self.t]:\n",
    "        #             predicted = self.lr1.predict(num)\n",
    "        #         else:\n",
    "        #             predicted = self.lr2.predict(num)\n",
    "        predicted = self.lr2.predict(num)\n",
    "        return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelBranch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597aa22-2754-4b4e-9f6b-03954fa3322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_dict_summary_fixed(benchmark_name=\"cg\", fixed=\"class\"):\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_ = return_fixed_class(BenchMark=benchmark_name)\n",
    "    else:\n",
    "        fixed_ = return_fixed_process(BenchMark=benchmark_name)\n",
    "    summary_fixed_ = pd.concat(fixed_, axis=1)\n",
    "    columns = summary_fixed_.columns.to_numpy()\n",
    "    index = summary_fixed_.index.to_numpy()\n",
    "    if(fixed == \"class\"):\n",
    "        dict_summary_fixed_ = {\"processes\": columns}\n",
    "    else:\n",
    "        dict_summary_fixed_ = {\"class\": columns}\n",
    "    for index_name in index:\n",
    "        dict_summary_fixed_[\n",
    "            index_name] = summary_fixed_.T[index_name].to_numpy()\n",
    "    return dict_summary_fixed_\n",
    "\n",
    "# NaNが入った引数のリストをNaNのみを0にして返す関数\n",
    "\n",
    "\n",
    "def return_non_NaN_list(target_list):\n",
    "    for i in range(len(target_list)):\n",
    "        if (math.isnan(target_list[i])):\n",
    "            target_list[i] = 0\n",
    "    return target_list\n",
    "# NaNが入ったリストが引数として渡されるとTrueを返す関数\n",
    "\n",
    "\n",
    "def does_include_nan(target_list):\n",
    "    for i in range(len(target_list)):\n",
    "        if(math.isnan(target_list[i])):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d4716-0f61-4e50-91ef-a9c4d2019b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形・対数・反比例モデルでフィッティングを行い、MAPE値をまとめたCSVファイルを作成する関数\n",
    "def generateScoreTable(benchmark_name=\"cg\"):\n",
    "    list_ScoreTable = []\n",
    "    dict_summary_fixed_class = return_dict_summary_fixed(\n",
    "        benchmark_name=benchmark_name, fixed=\"class\")\n",
    "    raw_x = dict_summary_fixed_class[\"processes\"]\n",
    "    for content in dict_summary_fixed_class:\n",
    "        if(content == \"processes\"):\n",
    "            continue\n",
    "        raw_y = dict_summary_fixed_class[content]\n",
    "        if(does_include_nan(raw_y)):\n",
    "            continue\n",
    "        # 線形モデル\n",
    "        model_lin = ModelLin(raw_x, raw_y, benchmark_name, content)\n",
    "        model_lin.calc_lr()\n",
    "        model_lin.calc_r2_score()\n",
    "        model_lin.calc_mae_score()\n",
    "        model_lin.calc_mse_score()\n",
    "        model_lin.calc_rmse_score()\n",
    "        model_lin.calc_mape_score()\n",
    "        # logモデル\n",
    "        model_log10 = ModelLog10(raw_x, raw_y, benchmark_name, content)\n",
    "        model_log10.calc_lr()\n",
    "        model_log10.calc_r2_score()\n",
    "        model_log10.calc_mae_score()\n",
    "        model_log10.calc_mse_score()\n",
    "        model_log10.calc_rmse_score()\n",
    "        model_log10.calc_mape_score()\n",
    "        # 反比例モデル\n",
    "        model_ip = ModelIP(raw_x, raw_y, benchmark_name, content)\n",
    "        model_ip.calc_lr()\n",
    "        model_ip.calc_r2_score()\n",
    "        model_ip.calc_mae_score()\n",
    "        model_ip.calc_mse_score()\n",
    "        model_ip.calc_rmse_score()\n",
    "        model_ip.calc_mape_score()\n",
    "\n",
    "        list_ScoreTable.append(\n",
    "            [content, model_lin.mape_score, model_log10.mape_score, model_ip.mape_score])\n",
    "    df_ScoreTable = pd.DataFrame(list_ScoreTable)\n",
    "    df_ScoreTable.columns = [\"\", \"x mape\", \"logx mape\", \"1/x mape\"]\n",
    "    df_ScoreTable.set_index(\"\", inplace=True)\n",
    "    df_ScoreTable.to_csv(\"./tmp_GenerateScoreTable/\"+benchmark_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72469e4-2b6d-4876-a44c-9352f4930d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行の内容が同じものをまとめ、行タイトルに重複した行タイトルがまとめられた、データフレームが返される関数\n",
    "\n",
    "def return_summarized_Fixed_dataframe(BenchMark_name=\"cg\", fixed=\"class\"):\n",
    "\n",
    "    def evaluate_dataframes(df1, df2):\n",
    "        for i in range(len(df1.values.tolist()[0])):\n",
    "            if(df1.values.tolist()[0][i] != df2.values.tolist()[0][i]):\n",
    "                return False\n",
    "        return True\n",
    "    fixed_df = 0\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_df = return_fixed_class(BenchMark=BenchMark_name)\n",
    "    elif (fixed == \"process\"):\n",
    "        fixed_df = return_fixed_process(BenchMark=BenchMark_name)\n",
    "    summary_fixed_df = pd.concat(fixed_df, axis=1)\n",
    "    dropped_summary_fixed_df = summary_fixed_df.drop_duplicates()\n",
    "    dropped_summary_fixed_df_renamed = dropped_summary_fixed_df\n",
    "\n",
    "    for dropped_index_name in dropped_summary_fixed_df.index.values:\n",
    "        dropped_index_name_data = summary_fixed_df.loc[[dropped_index_name]]\n",
    "        replace_name = dropped_index_name\n",
    "        for all_index_name in summary_fixed_df.index.values:\n",
    "            all_index_name_data = summary_fixed_df.loc[[all_index_name]]\n",
    "            if(dropped_index_name == all_index_name):\n",
    "                pass\n",
    "            elif(evaluate_dataframes(dropped_index_name_data, all_index_name_data)):\n",
    "                replace_name += f\", {all_index_name}\"\n",
    "        dropped_summary_fixed_df_renamed = dropped_summary_fixed_df_renamed.rename(\n",
    "            index={dropped_index_name: replace_name})\n",
    "\n",
    "    return dropped_summary_fixed_df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bb77c-7a16-4a0f-a721-c9016d6f0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~_excludeBTSP, ~~~_onlyBTSP はそれぞれのベンチマークで取得したプロセス数\n",
    "processes_excludeBTSP = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "processes_onlyBTSP = [1, 4, 16, 64, 256]\n",
    "\n",
    "# 引数に横軸：プロセス数orベンチマーククラス, 縦軸：関数名となっているデータフレームを取る\n",
    "# 返値として\n",
    "# rowData:プロセス数のリスト もしくは プロセス数のリスト (引数に由来)\n",
    "# 各種関数名：実行回数のリスト\n",
    "# 以上のような関係の辞書を返す\n",
    "\n",
    "\n",
    "def return_dict_Data(DataFrame):\n",
    "    columns = DataFrame.columns.to_numpy()\n",
    "    index = DataFrame.index.to_numpy()\n",
    "    # 返値となる辞書return_dictに引数のデータフレームの列名(プロセス数orベンチマーククラス)を格納\n",
    "    return_dict = {\"rowData\": columns}\n",
    "    for index_name in index:\n",
    "        return_dict[index_name] = DataFrame.T[index_name].to_numpy()\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "### 引数はx軸となる値のリスト, y軸となる値のリスト, 関数名の文字列, 訓練データでMAPEを算出するかどうかの真偽\n",
    "# 返り値は次のようなリスト\n",
    "# [<関数名の文字列>, <線形モデルのMAPE値>, <対数モデルのMAPE値>, <反比例モデルのMAPE値>, <分岐モデルのMAPE値>]\n",
    "def return_Mape_row_list(x: list, y: list, function_name: str, test_ratio=0.3, train=False):\n",
    "\n",
    "    # 変数：model_lin\n",
    "    # 線形モデル\n",
    "    model_lin = ModelLin(x, y, test_ratio=test_ratio)\n",
    "    if(test_ratio == 0):\n",
    "        model_lin.train_x = model_lin.raw_x\n",
    "        model_lin.train_y = model_lin.raw_y\n",
    "    model_lin.calc_lr()\n",
    "    model_lin.calc_mape_score()\n",
    "    model_lin.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_log\n",
    "    # 対数モデル\n",
    "    model_log = ModelLog10(x, y, test_ratio=test_ratio)\n",
    "    if(test_ratio == 0):\n",
    "        model_log.train_x = model_log.raw_x\n",
    "        model_log.train_y = model_log.raw_y\n",
    "    model_log.calc_lr()\n",
    "    model_log.calc_mape_score()\n",
    "    model_log.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_ip\n",
    "    # 反比例モデル\n",
    "    model_ip = ModelIP(x, y, test_ratio=test_ratio)\n",
    "    if(test_ratio == 0):\n",
    "        model_ip.train_y = model_ip.raw_y\n",
    "        model_ip.train_x = model_ip.raw_x\n",
    "    model_ip.calc_lr()\n",
    "    model_ip.calc_mape_score()\n",
    "    model_ip.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_branch\n",
    "    # 特異点付き条件分岐モデル\n",
    "    model_branch = ModelBranch(x, y, test_ratio=test_ratio)\n",
    "    if(test_ratio == 0):\n",
    "        model_branch.train_x = model_branch.raw_x\n",
    "        model_branch.train_y = model_branch.raw_y\n",
    "    model_branch.calc_lr()\n",
    "    model_branch.calc_mape_score()\n",
    "    model_branch.calc_mape_score_InTrain()\n",
    "\n",
    "    if(train == True):\n",
    "        lin_score = model_lin.mape_score_InTrain\n",
    "        log_score = model_log.mape_score_InTrain\n",
    "        ip_score = model_ip.mape_score_InTrain\n",
    "        branch_score = model_branch.mape_score_InTrain\n",
    "    else:\n",
    "        lin_score = model_lin.mape_score\n",
    "        log_score = model_log.mape_score\n",
    "        ip_score = model_ip.mape_score\n",
    "        branch_score = model_branch.mape_score\n",
    "\n",
    "    # 変数：return_list\n",
    "    # 返り値となるリスト\n",
    "    return_list = [function_name, lin_score, log_score, ip_score, branch_score]\n",
    "    return(return_list)\n",
    "\n",
    "# 引数は「return_dict_DataFrame()」の返値, テストとして何割のデータを用いるかの割合, 訓練データでMAPEを算出するかの真偽\n",
    "# 返り値は行・列がモデル名・関数名で要素がMAPE値となっているDataFrame\n",
    "\n",
    "\n",
    "def return_MapeTable_per_benchmark(dict_data: dict, test_ratio, train=False):\n",
    "\n",
    "    # 変数：_names\n",
    "    # 引数の辞書のプロセス数もしくはベンチマーククラスの文字列のリスト\n",
    "    _names = dict_data['rowData']\n",
    "    # 変数：function_names\n",
    "    # 引数の辞書の関数名の文字列のリスト\n",
    "    function_names = list(dict_data.keys())\n",
    "    function_names.remove('rowData')\n",
    "\n",
    "    # リスト変数：before_DataFrame_list\n",
    "    # 最終的にDataFrameとする元となるリスト\n",
    "    before_DataFrame_list = []\n",
    "    collumn_names = [\"function name\", \"Linear model\",\n",
    "                     \"Log10 model\", \"Inverse model\", \"Branch model\"]\n",
    "    for function_name in function_names:\n",
    "        if(does_include_nan(dict_data[function_name])):\n",
    "            continue\n",
    "        before_DataFrame_list.append(return_Mape_row_list(\n",
    "            x=_names, y=dict_data[function_name], function_name=function_name, test_ratio=test_ratio, train=train))\n",
    "\n",
    "    # 変数：return_df\n",
    "    # 返り値となるリスト\n",
    "    return_df = pd.DataFrame(before_DataFrame_list)\n",
    "    return_df.columns = collumn_names\n",
    "    return_df = return_df.set_index(\"function name\")\n",
    "\n",
    "    return(return_df)\n",
    "\n",
    "\n",
    "# 構造体的に利用可能なクラス MapeData\n",
    "# 各ベンチマークの各モデルごとに作成される。\n",
    "# 要素として、割合, 最大値, 最小値 がある。\n",
    "class MapeData:\n",
    "    def __init__(self):\n",
    "        self.ratio = 0\n",
    "        self.max = np.nan\n",
    "        self.min = np.nan\n",
    "        self.appearance = 0\n",
    "\n",
    "    def printData(self):\n",
    "        print(f\"{self.ratio}({self.min}, {self.max})\")\n",
    "\n",
    "    def return_Data(self):\n",
    "        max_min = \"\"\n",
    "        if(self.min is np.nan):\n",
    "            max_min = \"(NoData)\"\n",
    "        else:\n",
    "            max_min = f\"({self.min}, {self.max})\"\n",
    "        return(f\"{self.ratio}%{max_min}\")\n",
    "\n",
    "# 引数に「return_MapeTable_per_benchmark()」の返り値, ベンチマーク名, (オプショナル)中間データの詳細をとる\n",
    "# 返り値として次のようなリストを返す\n",
    "# [<線形モデルのMAPEに関する奴>, <対数モデルのMAPEに関する奴>, <反比例モデルのMAPEに関する奴>, <ベンチマーク名>]\n",
    "\n",
    "\n",
    "def return_MapeTable_row(MapeDataframe_detail, benchmark_name: str):\n",
    "\n",
    "    # 引数として渡されたデータフレームの行列名をindex, columnsに格納\n",
    "    columns = MapeDataframe_detail.columns.to_numpy()\n",
    "    index = MapeDataframe_detail.index.to_numpy()\n",
    "\n",
    "    # この関数で返すリストの要素の準備\n",
    "    MapeLin = MapeData()\n",
    "    MapeLog = MapeData()\n",
    "    MapeIP = MapeData()\n",
    "    MapeBr = MapeData()\n",
    "    return_list = [MapeLin, MapeLog, MapeIP, MapeBr, benchmark_name]\n",
    "\n",
    "    # 返り値のリストの各要素の値を更新\n",
    "    for function_name in index:\n",
    "        MapeData_per_function = MapeDataframe_detail.loc[function_name].to_list(\n",
    "        )\n",
    "        min_mape = min(MapeData_per_function)\n",
    "        min_mape_index = MapeData_per_function.index(min_mape)\n",
    "        rounded_min_mape = int(min_mape * 10) / 10\n",
    "        return_list[min_mape_index].appearance += 1\n",
    "        if(return_list[min_mape_index].max is np.nan):\n",
    "            return_list[min_mape_index].max = rounded_min_mape\n",
    "            return_list[min_mape_index].min = rounded_min_mape\n",
    "        if(return_list[min_mape_index].min > min_mape):\n",
    "            return_list[min_mape_index].min = rounded_min_mape\n",
    "        elif(return_list[min_mape_index].max < min_mape):\n",
    "            return_list[min_mape_index].max = rounded_min_mape\n",
    "    sum_num = 0\n",
    "    # 集計データから割合を算出\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        sum_num += return_list[i].appearance\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        return_list[i].ratio = int(100 * return_list[i].appearance/sum_num)\n",
    "    # 割合の合計が100になるように調整\n",
    "    exclude_index0_ratios = 0\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        exclude_index0_ratios += return_list[i].ratio\n",
    "    return_list[0].ratio = 100 - exclude_index0_ratios\n",
    "\n",
    "    return(return_list)\n",
    "\n",
    "\n",
    "def save_MapeTable(MapeTable, suffix=\"\"):\n",
    "    tmp_table = MapeTable.copy()\n",
    "    columns = MapeTable.columns.to_numpy()\n",
    "    index = MapeTable.index.to_numpy()\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(len(index)):\n",
    "            tmp_table.iat[j, i] = tmp_table.iat[j, i].return_Data()\n",
    "    tmp_table.to_csv(f\"./tmp_GenerateResources/MapeTable_{str(suffix)}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365bb7c-0565-418a-a313-bf43acbb9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数はx軸となる値のリスト, y軸となる値のリスト, 関数名の文字列\n",
    "# 返り値は次のようなリスト\n",
    "# [<関数名の文字列>, <線形モデル>, <対数モデル>, <反比例モデル>, <分岐モデル>]\n",
    "def return_Model_row_list(x: list, y: list, function_name: str, test_ratio=0.3, train=False):\n",
    "\n",
    "    # 変数：model_lin\n",
    "    # 線形モデル\n",
    "    model_lin = ModelLin(x, y, test_ratio=test_ratio)\n",
    "    model_lin.calc_lr()\n",
    "    if(train):\n",
    "        model_lin.calc_mape_score_InTrain()\n",
    "    else:\n",
    "        model_lin.calc_mape_score()\n",
    "\n",
    "    # 変数：model_log\n",
    "    # 対数モデル\n",
    "    model_log = ModelLog10(x, y, test_ratio=test_ratio)\n",
    "    model_log.calc_lr()\n",
    "    if(train):\n",
    "        model_log.calc_mape_score_InTrain()\n",
    "    else:\n",
    "        model_log.calc_mape_score()\n",
    "\n",
    "    # 変数：model_ip\n",
    "    # 反比例モデル\n",
    "    model_ip = ModelIP(x, y, test_ratio=test_ratio)\n",
    "    model_ip.calc_lr()\n",
    "    if(train):\n",
    "        model_ip.calc_mape_score_InTrain()\n",
    "    else:\n",
    "        model_ip.calc_mape_score()\n",
    "\n",
    "    # 変数：model_branch\n",
    "    # 特異点付き条件分岐モデル\n",
    "    model_branch = ModelBranch(x, y, test_ratio=test_ratio)\n",
    "    model_branch.calc_lr()\n",
    "    if(train):\n",
    "        model_branch.calc_mape_score_InTrain()\n",
    "    else:\n",
    "        model_branch.calc_mape_score()\n",
    "\n",
    "    # 変数：return_list\n",
    "    # 返り値となるリスト\n",
    "    return_list = [function_name, model_lin, model_log, model_ip, model_branch]\n",
    "    return(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0e959-b329-46ae-8d57-179be9199fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、収集するベンチマークのリスト、実行したい学習の割合、固定したいベンチマーククラス\n",
    "# 返値は無し\n",
    "# 実行すると、\"./tmpGenerateResources\" に \"<ベンチマーク名>_FixedClassTrain_<テスト割合>.csv\" という形式でファイルが生成される\n",
    "\n",
    "def GenerateMapeTableFixClass(Benchmarks=[\"cg\"], TestRatios=[\"0.3\"], FixBenchmarksClass=\"C\"):\n",
    "    for test_ratio in TestRatios:\n",
    "        print(f\"test_ratio={test_ratio}\")\n",
    "        print(\n",
    "            f\"train_list, test_list = {split_by_ratio(processes_excludeBTSP, test_ratio)} on processes_excludeBTSP\")\n",
    "        print(\n",
    "            f\"train_list, test_list = {split_by_ratio(processes_onlyBTSP, test_ratio)} on processes_onlyBTSP\")\n",
    "        print(f\"\\n\")\n",
    "        fixed_class_list = [0] * len(Benchmarks)\n",
    "        for i in range(len(Benchmarks)):\n",
    "            if(Benchmarks[i] == \"bt\" or Benchmarks[i] == \"sp\"):\n",
    "                process = processes_onlyBTSP\n",
    "            else:\n",
    "                process = processes_excludeBTSP\n",
    "            fixed_class_list[i] = return_fixed_class(\n",
    "                BenchMark=Benchmarks[i], Processes=processes, FixedBenchMarkClass=FixBenchmarksClass)\n",
    "        fixed_class_DataFrame = [0] * len(fixed_class_list)\n",
    "        for i in range(len(fixed_class_list)):\n",
    "            fixed_class_DataFrame[i] = pd.concat(fixed_class_list[i], axis=1)\n",
    "        for i in range(len(fixed_class_DataFrame)):\n",
    "            dict_data = return_dict_Data(fixed_class_DataFrame[i])\n",
    "            MapeTable_per_benchmark = return_MapeTable_per_benchmark(\n",
    "                dict_data, test_ratio=test_ratio, train=True)\n",
    "            MapeTable_per_benchmark.to_csv(\n",
    "                f\"./tmp_GenerateResources/{Benchmarks[i]}_FixedClassTrain_{test_ratio}.csv\")\n",
    "# 使用例\n",
    "# GenerateMapeTableFixClass(Benchmarks=[\"cg\", \"lu\"], TestRatios=[0.3, 0.7], FixBenchmarksClass=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183213e-de54-4c5f-9328-a4597b0754e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、収集するベンチマークのリスト、実行したい学習の割合、固定したい実行プロセス数\n",
    "# 返値は無し\n",
    "# 実行すると、\"./tmpGenerateResources\" に \"<ベンチマーク名>_FixedProcessTrain_<テスト割合>.csv\" という形式でファイルが生成される\n",
    "\n",
    "def GenerateMapeTableFixProcess(Benchmarks=[\"cg\"], BenchmarkClasses=[\"A\", \"B\", \"C\", \"D\"], BenchmarkClasses_on_num=[1, 4, 16, 256], TestRatios=[\"0.3\"], FixProcess=64):\n",
    "    for test_ratio in TestRatios:\n",
    "        print(f\"test_ratio={test_ratio}\")\n",
    "        print(\n",
    "            f\"train_list, test_list = {split_by_ratio(BenchmarkClasses, test_ratio)} on BenchmarkClasses\")\n",
    "        print(f\"\\n\")\n",
    "        fixed_Process_list = [0] * len(benchmarks)\n",
    "        for i in range(len(fixed_Process_list)):\n",
    "            fixed_Process_list[i] = return_fixed_process(\n",
    "                BenchMark=Benchmarks[i], BenchMarkClasses=BenchmarkClasses, FixedProcess=FixProcess)\n",
    "        fixed_Process_DataFrame = [0] * len(fixed_Process_list)\n",
    "        for i in range(len(fixed_Process_DataFrame)):\n",
    "            fixed_Process_DataFrame[i] = pd.concat(\n",
    "                fixed_Process_list[i], axis=1)\n",
    "\n",
    "        for i in range(len(fixed_Process_DataFrame)):\n",
    "            dict_data = return_dict_Data(fixed_Process_DataFrame[i])\n",
    "            dict_data['rowData'] = BenchmarkClasses_on_num\n",
    "            try:\n",
    "                MapeTable_per_benchmark = return_MapeTable_per_benchmark(\n",
    "                    dict_data, test_ratio=test_ratio, train=True)\n",
    "            except:\n",
    "                print(f\"MAPEを算出するのに問題発生@{Benchmarks[i]}\")\n",
    "                continue\n",
    "            MapeTable_per_benchmark.to_csv(\n",
    "                f\"./tmp_GenerateResources/{benchmarks[i]}_FixedProcessTrain_{test_ratio}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4fe228-5426-4d93-a2a6-be36b38cb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertBencharkClass_inNPB(Alphabet: str):\n",
    "    if(Alphabet == \"A\"):\n",
    "        return (1)\n",
    "    elif(Alphabet == \"B\"):\n",
    "        return (4)\n",
    "    elif(Alphabet == \"C\"):\n",
    "        return (16)\n",
    "    elif(Alphabet == \"D\"):\n",
    "        return (256)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def ConvertBencharkClass_inNPB_fromNum(number):\n",
    "    number = int(number)\n",
    "    if(number == 1):\n",
    "        return(\"A\")\n",
    "    elif(number == 4):\n",
    "        return(\"B\")\n",
    "    elif(number == 16):\n",
    "        return(\"C\")\n",
    "    elif(number == 256):\n",
    "        return(\"D\")\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def ConvertBenchmarkClasses(InputList=[\"A\", \"B\", \"C\", \"D\"]):\n",
    "    ReturnList = []\n",
    "    for content in InputList:\n",
    "        ReturnList.append(ConvertBencharkClass_inNPB(content))\n",
    "    return(ReturnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd74fa-ed4e-4180-a761-70ac11995d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_OptimalProcessesList(BenchmarkName=\"cg\"):\n",
    "    if(BenchmarkName == \"bt\" or BenchmarkName == \"sp\"):\n",
    "        return(processes_onlyBTSP)\n",
    "    else:\n",
    "        return(processes_excludeBTSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76c393-5934-4c7c-ba91-3abe6cabc25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、ベンチマーク名、列名、固定する値、プロセスorクラスのどちらで固定するか\n",
    "# 返値は次のような形式の辞書\n",
    "# rowData:[プロセス数]or[ベンチマーククラス]\n",
    "# <関数名>:[実行回数]\n",
    "def returnDictForModelDataFrame(BenchmarkName=\"cg\", rowData=[\"A\", \"B\", \"C\", \"D\"], fix=\"64\", fixed=\"Process\"):\n",
    "    if(fixed == \"Process\"):\n",
    "        FixedProcessList = return_fixed_process(\n",
    "            BenchMark=BenchmarkName, BenchMarkClasses=rowData, FixedProcess=fix)\n",
    "        FixedProcessDataFrame = pd.concat(FixedProcessList, axis=1)\n",
    "        DictData = return_dict_Data(FixedProcessDataFrame)\n",
    "    elif(fixed == \"Class\"):\n",
    "        FixedClassList = return_fixed_class(\n",
    "            BenchMark=BenchmarkName, Processes=rowData, FixedBenchMarkClass=fix)\n",
    "        FixedClassDataFrame = pd.concat(FixedClassList, axis=1)\n",
    "        DictData = return_dict_Data(FixedClassDataFrame)\n",
    "    return(DictData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78503924-d571-4de9-8ddf-523390dbfb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、returnDictForModelDataFrame()の返値, rowData, 関数名のリスト, テストに割り当てる割合\n",
    "# 返値はリスト\n",
    "# [<関数名>, <学習済みデータ1>, ... , <学習済みデータn>]\n",
    "def return_ModelDataSourceList(DictData, x_list, Index, test_ratio=0.3):\n",
    "    ModelDataSourceList = []\n",
    "    for FunctionName in Index:\n",
    "        y_list = DictData[FunctionName]\n",
    "        if(does_include_nan(y_list)):\n",
    "            continue\n",
    "        ModelDataSourceList.append(return_Model_row_list(\n",
    "            x=x_list, y=y_list, function_name=FunctionName, test_ratio=test_ratio, train=True))\n",
    "    return(ModelDataSourceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c315f1-62d3-45de-a111-0512ab9619c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、関数名、rowData, テストに割り当てる割合, 固定するプロセス数orクラス, クラスで固定するかプロセス数で固定するか\n",
    "# 返値はDataFrame\n",
    "# 行名は関数名で、列名はモデルの種別\n",
    "# それぞれの要素は学習済みデータ\n",
    "def return_Models_per_Benchmark(BenchmarkName=\"cg\", rowData=[1, 4, 16, 256], TestRate=0.3, fix=\"C\", fixed=\"Class\"):\n",
    "    # 変数：MapeTable\n",
    "    # ベンチマークのMAPE表\n",
    "    # fixedが\"Class\"ならクラスで固定され、fixedが\"Process\"ならプロセス数で固定されたMAPE表を読み込んでいる。\n",
    "    MapeTable = pd.read_csv(\n",
    "        f\"./tmp_GenerateResources/{BenchmarkName}_Fixed{fixed}Train_{test_ratio}.csv\")\n",
    "    MapeTable = MapeTable.set_index(\"function name\")\n",
    "\n",
    "    # 変数：MapeTableColumns, MapTableIndex\n",
    "    # MapeTableの列名・行名\n",
    "    MapeTableColumns = MapeTable.columns.to_numpy()\n",
    "    MapeTableIndex = MapeTable.index.to_numpy()\n",
    "\n",
    "    # 変数：ModelDataFrame\n",
    "    # MapeTableにおける各関数の学習済みモデルが格納される\n",
    "    checked_rowData = rowData\n",
    "    if(fixed == \"Process\"):\n",
    "        checked_rowData = ConvertBenchmarkClasses(rowData)\n",
    "    DictData = returnDictForModelDataFrame(\n",
    "        BenchmarkName, rowData=rowData, fix=fix, fixed=fixed)\n",
    "    ModelDataFrameSourceList = return_ModelDataSourceList(\n",
    "        DictData=DictData, x_list=checked_rowData, Index=MapeTableIndex, test_ratio=TestRate)\n",
    "    ModelDataFrameSourceListCollumnsName = [\n",
    "        \"FunctionName\", \"ModelLin\", \"ModelLog\", \"ModelIp\", \"ModelBranch\"]\n",
    "    ModelDataFrame = pd.DataFrame(ModelDataFrameSourceList)\n",
    "    ModelDataFrame.columns = ModelDataFrameSourceListCollumnsName\n",
    "    ModelDataFrame = ModelDataFrame.set_index(\"FunctionName\")\n",
    "    return(ModelDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd96f3-c420-4183-a49e-ad8ea35c1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、読み込んだベンチマークごとのMAPE表, 各関数の全てのモデルにおける学習済みモデル\n",
    "# 返値は辞書\n",
    "# キーは<関数名>でバリューは<学習済みモデル>\n",
    "def return_BestModelsDict(MapeTable, ModelDataFrame):\n",
    "    BestModelsDict = {}\n",
    "    ModelNames = ModelDataFrame.columns.to_list()\n",
    "    ModelDataFrameIndexNameList = ModelDataFrame.index.to_numpy()\n",
    "    for FunctionName in ModelDataFrameIndexNameList:\n",
    "        MapeInFunction = MapeTable.loc[FunctionName].to_list()\n",
    "        SmallestModelIndex = MapeInFunction.index(min(MapeInFunction))\n",
    "        SmallestModelName = ModelNames[SmallestModelIndex]\n",
    "        BestModelsDict[FunctionName] = ModelDataFrame.at[FunctionName,\n",
    "                                                         SmallestModelName]\n",
    "    return BestModelsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81555b3-ac55-4eae-9ae8-f87cc153ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BenchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# 引数は\n",
    "# 返値は辞書\n",
    "# キーは<関数名>でバリューは<学習済みモデル>の辞書\n",
    "# 学習済みモデルのデータフレームの取得に失敗した場合はFalseを返す\n",
    "\n",
    "\n",
    "def generateBestModelDict(TestRatio=0.3, BenchmarkName=\"ft\", fixed=\"Class\", fix=\"C\", rowData=[1, 2, 4, 8, 16, 32, 64, 128, 256]):\n",
    "\n",
    "    # 変数：MapeTable\n",
    "    # ベンチマークのMAPE表\n",
    "    if(fixed == \"Class\"):\n",
    "        file_name = f\"{BenchmarkName}_FixedClassTrain_{TestRatio}.csv\"\n",
    "    else:\n",
    "        file_name = f\"{BenchmarkName}_FixedProcessTrain_{TestRatio}.csv\"\n",
    "    file_path = f\"./tmp_GenerateResources/{file_name}\"\n",
    "    MapeTable = pd.read_csv(file_path)\n",
    "    MapeTable = MapeTable.set_index(\"function name\")\n",
    "#     try:\n",
    "#         ModelDataFrame = return_Models_per_Benchmark(BenchmarkName=benchmark, rowData=processes, TestRate=test_ratio, fix=fix, fixed=fixed)\n",
    "#     except:\n",
    "#         print(f\"\\n全てのモデル形式で学習済みモデルを作成しているor集めている最中に問題が発生しました@{benchmark}\\n\")\n",
    "#         return False\n",
    "    ModelDataFrame = return_Models_per_Benchmark(\n",
    "        BenchmarkName=benchmark, rowData=rowData, TestRate=test_ratio, fix=fix, fixed=fixed)\n",
    "\n",
    "    BestModelsDict = return_BestModelsDict(\n",
    "        MapeTable=MapeTable, ModelDataFrame=ModelDataFrame)\n",
    "    return(BestModelsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6707a76-6ff9-4c0b-abb9-e4525c389b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、テストの割合のリスト, rowData\n",
    "# 返値はrowDataのtrainのリストを文字列化したもののリスト\n",
    "def return_StringTrainList(TestRatio=[0.3, 0.5], rowData=[1, 2, 4, 8]):\n",
    "    returnList = []\n",
    "    for test_ratio in TestRatio:\n",
    "        train_list, test_list = split_by_ratio(\n",
    "            base_list=rowData, test_ratio=test_ratio)\n",
    "        returnList.append(f\"{train_list}\")\n",
    "    return(returnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bc12b-248c-4134-94c5-e9ea014f597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、関数名, 予測値, ベンチマーク名, ベンチマーククラス, 実行プロセス数\n",
    "# 返値は予測値と実測値の誤差率(= 予測値/実測値 * 100)\n",
    "def return_ErrorRate(FunctionName=\"CG\", PredictNum=256, BenchmarkName=\"cg\", BenchmarkClass=\"D\", Process=256):\n",
    "    target_csv = pd.read_csv(\n",
    "        f\"./csv_files/pprof_{BenchmarkName}{BenchmarkClass}{Process}.csv\")\n",
    "    target_csv = target_csv.set_index(\"Name\")\n",
    "    try:\n",
    "        RealNum = target_csv.loc[FunctionName, \"#Call\"]\n",
    "    except:\n",
    "        print(f\"該当する関数はありませんでした@{Benchmakname}@{FunctionName}\")\n",
    "        RealNum = False\n",
    "    if(RealNum != False):\n",
    "        returnNum = abs(RealNum-PredictNum)/RealNum*100\n",
    "        return(returnNum)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb74c20-52f8-4020-9f24-5a244a623022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測値を返す関数\n",
    "# 引数は、学習済みモデル, 予測したいプロセス数もしくはベンチマーククラスを数値化したもの\n",
    "\n",
    "def return_Predicted(LearnedModel, num):\n",
    "    Input = np.reshape(num, (-1, 1))\n",
    "    PredictedData = LearnedModel.predict(Input)\n",
    "    PredictedList = PredictedData.tolist()\n",
    "    Predict = PredictedList[0][0]\n",
    "    return(Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ff67f-6b38-4cd7-a1e8-2664a96e9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return_FixedClassModelDF\n",
    "# 引数で指定されたベンチマークでベンチマーククラスを固定した際の各関数について最適な学習済みモデルを返す関数\n",
    "def return_FixedClassModelDF(benchmark=\"cg\", FixClass=\"C\"):\n",
    "    ProcessExcludeBTSP = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    ProcessIncludeBTSP = [1, 4, 16, 64, 256]\n",
    "    TestRates = [0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    if(benchmark == \"bt\" or benchmark == \"sp\"):\n",
    "        processes = ProcessIncludeBTSP\n",
    "        TestRates = [0, 0.2, 0.4, 0.6, 0.8]\n",
    "    else:\n",
    "        processes = ProcessExcludeBTSP\n",
    "\n",
    "    RawDataFrameSource = return_fixed_class(\n",
    "        BenchMark=benchmark, Processes=processes, FixedBenchMarkClass=FixClass)\n",
    "    RawDataFrame = pd.concat(RawDataFrameSource, axis=1)\n",
    "\n",
    "    x_list = RawDataFrame.columns.tolist()\n",
    "    FunctionNames = RawDataFrame.index.tolist()\n",
    "    ModelsInBenchmark = {}\n",
    "    for FunctionName in FunctionNames:\n",
    "        BestModelsPerFunction = [0] * len(TestRates)\n",
    "        y_list = RawDataFrame.loc[FunctionName].tolist()\n",
    "        for test_ratio in TestRates:\n",
    "            x_list_splited = split_by_ratio(x_list, test_ratio)[0]\n",
    "            y_list_splited = split_by_ratio(y_list, test_ratio)[0]\n",
    "            if(does_include_nan(y_list)):\n",
    "                continue\n",
    "            Models = return_Model_row_list(\n",
    "                x=x_list_splited, y=y_list_splited, function_name=FunctionName, test_ratio=0, train=True)[1:]\n",
    "            MapeScoreInTrains = [0] * len(Models)\n",
    "            for i in range(len(Models)):\n",
    "                MapeScoreInTrains[i] = Models[i].mape_score_InTrain\n",
    "            BestModelsPerFunction[TestRates.index(\n",
    "                test_ratio)] = Models[MapeScoreInTrains.index(min(MapeScoreInTrains))]\n",
    "        if(0 in BestModelsPerFunction):\n",
    "            continue\n",
    "        ModelsInBenchmark[FunctionName] = BestModelsPerFunction\n",
    "\n",
    "    ModelDF = pd.DataFrame.from_dict(ModelsInBenchmark, orient='index')\n",
    "    ModelDFcolumns = []\n",
    "    for test_ratio in TestRates:\n",
    "        ModelDFcolumns.append(f\"{split_by_ratio(x_list, test_ratio)[0]}\")\n",
    "    ModelDF.columns = ModelDFcolumns\n",
    "    return(ModelDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3d9d3-f7c8-4d76-b347-698a6bf4fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return_FixedProcessModelDF\n",
    "# 引数で指定されたベンチマークで実行プロセス数を固定した際の各関数について最適な学習済みモデルを返す関数\n",
    "def return_FixedProcessModelDF(benchmark=\"cg\", FixProcess=\"64\"):\n",
    "    BenchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "    TestRates = [0, 0.4, 0.5, 0.8]\n",
    "    BenchmarkClassesOnNum = ConvertBenchmarkClasses(BenchmarkClasses)\n",
    "\n",
    "    RawDataFrameSource = return_fixed_process(\n",
    "        BenchMark=benchmark, BenchMarkClasses=BenchmarkClasses, FixedProcess=FixProcess)\n",
    "    RawDataFrame = pd.concat(RawDataFrameSource, axis=1)\n",
    "    x_list_base = RawDataFrame.columns.tolist()\n",
    "    x_list = ConvertBenchmarkClasses(copy.deepcopy(x_list_base))\n",
    "    FunctionNames = RawDataFrame.index.tolist()\n",
    "    ModelsInBenchmark = {}\n",
    "    for FunctionName in FunctionNames:\n",
    "        BestModelsPerFunction = [0] * len(TestRates)\n",
    "        y_list = RawDataFrame.loc[FunctionName].tolist()\n",
    "        for test_ratio in TestRates:\n",
    "            x_list_splited = split_by_ratio(x_list, test_ratio)[0]\n",
    "            y_list_splited = split_by_ratio(y_list, test_ratio)[0]\n",
    "            if(does_include_nan(y_list)):\n",
    "                continue\n",
    "            Models = return_Model_row_list(\n",
    "                x=x_list_splited, y=y_list_splited, function_name=FunctionName, test_ratio=0, train=True)[1:]\n",
    "            MapeScoreInTrains = [0] * len(Models)\n",
    "            for i in range(len(Models)):\n",
    "                MapeScoreInTrains[i] = Models[i].mape_score_InTrain\n",
    "            BestModelsPerFunction[TestRates.index(\n",
    "                test_ratio)] = Models[MapeScoreInTrains.index(min(MapeScoreInTrains))]\n",
    "        if(0 in BestModelsPerFunction):\n",
    "            continue\n",
    "        ModelsInBenchmark[FunctionName] = BestModelsPerFunction\n",
    "\n",
    "    ModelDF = pd.DataFrame.from_dict(ModelsInBenchmark, orient='index')\n",
    "    ModelDFcolumns = []\n",
    "    for test_ratio in TestRates:\n",
    "        ModelDFcolumns.append(f\"{split_by_ratio(x_list_base, test_ratio)[0]}\")\n",
    "    ModelDF.columns = ModelDFcolumns\n",
    "    return(ModelDF)\n",
    "\n",
    "\n",
    "ModelDF = return_FixedProcessModelDF(benchmark=\"cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fb1aa-6a7b-40b7-b2ef-285b719e1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_MapeTableFixedClass(benchmark=\"cg\"):\n",
    "    DirPath = \"./tmp_GenerateResources/\"\n",
    "    try:\n",
    "        ModelDFfixedClass = return_FixedClassModelDF(benchmark=benchmark)\n",
    "    except:\n",
    "        print(f\"return_FixedClassModelDF({benchmark})の実行に失敗しました\")\n",
    "        return -1\n",
    "    ModelDFfixedClass_LowestMape = ModelDFfixedClass\n",
    "    index = ModelDFfixedClass_LowestMape.index.tolist()\n",
    "    columns = ModelDFfixedClass_LowestMape.columns.tolist()\n",
    "    for column in columns:\n",
    "        for row in index:\n",
    "            ModelDFfixedClass_LowestMape.at[row,\n",
    "                                            column] = ModelDFfixedClass.at[row, column].mape_score_InTrain\n",
    "    average = ModelDFfixedClass_LowestMape.mean()\n",
    "    average.name = \"Average\"\n",
    "    ModelDFfixedClass_LowestMape.append(average)\n",
    "    return ModelDFfixedClass_LowestMape\n",
    "\n",
    "\n",
    "def return_MapeTableFixedProcess(benchmark=\"cg\"):\n",
    "    DirPath = \"./tmp_GenerateResources/\"\n",
    "    try:\n",
    "        ModelDFfixedProcess = return_FixedProcessModelDF(benchmark=benchmark)\n",
    "    except:\n",
    "        print(f\"return_FixedProcessModelDF({benchmark})の実行に失敗しました\")\n",
    "        return -1\n",
    "    ModelDFfixedProcess_LowestMape = ModelDFfixedProcess\n",
    "    index = ModelDFfixedProcess_LowestMape.index.tolist()\n",
    "    columns = ModelDFfixedProcess_LowestMape.columns.tolist()\n",
    "    for column in columns:\n",
    "        for row in index:\n",
    "            ModelDFfixedProcess_LowestMape.at[row,\n",
    "                                              column] = ModelDFfixedProcess.at[row, column].mape_score_InTrain\n",
    "    average = ModelDFfixedProcess_LowestMape.mean()\n",
    "    average.name = \"Average\"\n",
    "    ModelDFfixedProcess_LowestMape.append(average)\n",
    "    return ModelDFfixedProcess_LowestMape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49838771-7a7a-4eb4-bae9-0ec5ac43a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_MapeTableFixed<Class or Process>\n",
    "# 引数で渡されたベンチマークについて、<クラス or プロセス>を固定したMAPE表となったデータフレームを返す\n",
    "\n",
    "def return_MapeTableFixedClass(benchmark=\"cg\", FixClass=\"C\"):\n",
    "    DirPath = \"./tmp_GenerateResources/\"\n",
    "    try:\n",
    "        ModelDFfixedClass = return_FixedClassModelDF(\n",
    "            benchmark=benchmark, FixClass=FixClass)\n",
    "    except:\n",
    "        print(f\"return_FixedClassModelDF({benchmark})の実行に失敗しました\")\n",
    "        return -1\n",
    "    ModelDFfixedClass_LowestMape = ModelDFfixedClass\n",
    "    index = ModelDFfixedClass_LowestMape.index.tolist()\n",
    "    columns = ModelDFfixedClass_LowestMape.columns.tolist()\n",
    "    for column in columns:\n",
    "        for row in index:\n",
    "            ModelDFfixedClass_LowestMape.at[row,\n",
    "                                            column] = ModelDFfixedClass.at[row, column].mape_score_InTrain\n",
    "    average = ModelDFfixedClass_LowestMape.mean()\n",
    "    average.name = \"Average\"\n",
    "    ModelDFfixedClass_LowestMape = ModelDFfixedClass_LowestMape.append(average)\n",
    "    return ModelDFfixedClass_LowestMape\n",
    "\n",
    "\n",
    "def return_MapeTableFixedProcess(benchmark=\"cg\", FixProcess=\"64\"):\n",
    "    DirPath = \"./tmp_GenerateResources/\"\n",
    "    try:\n",
    "        ModelDFfixedProcess = return_FixedProcessModelDF(\n",
    "            benchmark=benchmark, FixProcess=FixProcess)\n",
    "    except:\n",
    "        print(f\"return_FixedProcessModelDF({benchmark})の実行に失敗しました\")\n",
    "        return -1\n",
    "    ModelDFfixedProcess_LowestMape = ModelDFfixedProcess\n",
    "    index = ModelDFfixedProcess_LowestMape.index.tolist()\n",
    "    columns = ModelDFfixedProcess_LowestMape.columns.tolist()\n",
    "    for column in columns:\n",
    "        for row in index:\n",
    "            ModelDFfixedProcess_LowestMape.at[row,\n",
    "                                              column] = ModelDFfixedProcess.at[row, column].mape_score_InTrain\n",
    "    average = ModelDFfixedProcess_LowestMape.mean()\n",
    "    average.name = \"Average\"\n",
    "    ModelDFfixedProcess_LowestMape = ModelDFfixedProcess_LowestMape.append(\n",
    "        average)\n",
    "    return ModelDFfixedProcess_LowestMape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd81a4a-8514-4518-b0e5-ee86df28ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mape表をCSVとして保存する関数\n",
    "# 引数はbenchmark:ベンチマーク名, FixClass:固定するベンチマーククラス, FixProcess:固定するプロセス数, DirPath:保存するディレクトリ\n",
    "def SaveMapeTables(FixClass=\"B\", FixProcess=256, DirPath=\"./tmp_GenerateResources/\"):\n",
    "    for benchmark in benchmarks:\n",
    "        MapeTableFixedClass = return_MapeTableFixedClass(\n",
    "            benchmark, FixClass=\"B\")\n",
    "        MapeTableFixedProcess = return_MapeTableFixedProcess(\n",
    "            benchmark, FixProcess=256)\n",
    "        FixedClassFileName = f\"MapeTableFixedClass_{benchmark}{FixClass}.csv\"\n",
    "        FixedProcessFileName = f\"MapeTableFixedProcess_{benchmark}{FixProcess}.csv\"\n",
    "    #     print(f\"FixedClassFileName={FixedClassFileName}, MapeTableFixedProcess={FixedProcessFileName}\")\n",
    "        if(type(MapeTableFixedClass) is pd.core.frame.DataFrame):\n",
    "            #             print(f\"FixedClassFileName={FixedClassFileName}\")\n",
    "            MapeTableFixedClass.to_csv(f\"{DirPath}{FixedClassFileName}\")\n",
    "        if(type(MapeTableFixedProcess) is pd.core.frame.DataFrame):\n",
    "            #             print(f\"FixedProcessFileName={FixedProcessFileName}\")\n",
    "            MapeTableFixedProcess.to_csv(f\"{DirPath}{FixedProcessFileName}\")\n",
    "\n",
    "\n",
    "# SaveMapeTables(FixClass=\"C\", FixProcess=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54918c8d-b97f-421d-831f-64f63eb452e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラス：TimeData\n",
    "# 下記の関数(= return_TimeDataList())のために作成された\n",
    "# メソッドreturn_AllData()で値のすべてを辞書形式で受けてることができる\n",
    "class TimeData:\n",
    "    def __init__(self, benchmark=\"cg\", process=\"32\", BenchmarkClass=\"C\", time=-1):\n",
    "        self.benchmark = benchmark\n",
    "        self.process = process\n",
    "        self.BenchmarkClass = BenchmarkClass\n",
    "        self.time = time\n",
    "\n",
    "    def return_AllData(self):\n",
    "        return {\"benchmark\": self.benchmark, \"process\": self.process, \"BenchmarkClass\": self.BenchmarkClass, \"time\": self.time}\n",
    "\n",
    "# 返値に独自クラスTimeDataのリストを返す\n",
    "# 引数に実行プロセス数を取る\n",
    "# 返値のリストの要素は引数のリストのプロセス数で実行されたベンチマークの\n",
    "# ベンチマーク名・実行プロセス数・ベンチマーククラス・実行時間が記録されたTimeDataクラスのインスタンス\n",
    "\n",
    "\n",
    "def return_TimeDataList(process=256):\n",
    "    return_list = []\n",
    "    with open(f\"./toGetProfile/toGetTime/TimeWith{process}.txt\") as f:\n",
    "        line_count = 0\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            if(line_count % 3 == 1):\n",
    "                benchmark = line[1:3].lower()\n",
    "                # print(f\"benchmark={benchmark}, len(benchmark)={len(benchmark)}\")\n",
    "                Data = TimeData(benchmark=benchmark, process=process)\n",
    "            if(line_count % 3 == 2):\n",
    "                BenchmarkClass = line[-2]\n",
    "                # print(f\"BenchmarkClass={BenchmarkClass}, len(BenchmarkClass)={len(BenchmarkClass)}\")\n",
    "                Data.BenchmarkClass = BenchmarkClass\n",
    "            if(line_count % 3 == 0):\n",
    "                Time = line[-25:]\n",
    "                Time = Time.strip()\n",
    "                # print(f\"Time={Time}\")\n",
    "                Data.time = Time\n",
    "                return_list.append(Data)\n",
    "    return(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5acd8-3947-4147-a268-a798c9a4a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 関数:FillCSV()\n",
    "# 引数は,benchmark:ベンチマーク名, process:実行プロセス数, BenchmarkClass:ベンチマーククラス, time:実行時間\n",
    "# 引数として渡された値を適切なCSVに保存する\n",
    "def FillCSV(benchmark=\"cg\", process=256, BenchmarkClass=\"A\", time=\"0.04\"):\n",
    "    time = float(time)\n",
    "    CSVFilename = f\"./csv_files/ExecTime@{benchmark}.csv\"\n",
    "    DataFrame = pd.read_csv(CSVFilename, index_col=0)\n",
    "    DataFrame.at[BenchmarkClass, process] = time\n",
    "    DataFrame.to_csv(CSVFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496ab09-8e11-44cb-b6b4-00d0c3082727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 誤差率の表を作成する関数\n",
    "# 引数は,benchmark:関数名, predict_class:予測したいクラス, predict_process:予測したいプロセス数, FixProcess:固定する実行プロセス数, FixClass:固定するベンチマーククラス\n",
    "# 返り値はなし\n",
    "def GenerateErrorRateTable(benchmark=\"cg\", predict_class=\"D\", predict_process=256, FixProcess=64, FixClass=\"B\"):\n",
    "\n",
    "    DirName = \"./table_LatexForm/\"\n",
    "    FileSuffix = f\".table\"\n",
    "    FilePrefix = f\"ErrorRateTable@{benchmark}Fixed\"\n",
    "\n",
    "    try:\n",
    "        ErrorRateFixedProcessDF = return_ErrorRateFixedProcessDF(\n",
    "            benchmark=benchmark, FixProcess=FixProcess, predict_class=predict_class)\n",
    "    except:\n",
    "        print(\"実行プロセスを固定した際の誤差率の表を取得するのに失敗したので、CSVとして保存できませんでした。\")\n",
    "    ErrorRateFixedProcessDF.to_latex(\n",
    "        f\"{DirName}{FilePrefix}{FixProcess}{FileSuffix}\")\n",
    "    try:\n",
    "        ErrorRateFixedClassDF = return_ErrorRateFixedClassDF(\n",
    "            benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "    except:\n",
    "        print(\"ベンチマーククラスを固定した際の誤差率の表を取得するのに失敗したので、CSVとして保存できませんでした。\")\n",
    "    ErrorRateFixedClassDF.to_latex(\n",
    "        f\"{DirName}{FilePrefix}{FixClass}{FileSuffix}\")\n",
    "\n",
    "# 誤差率のデータフレームを返す関数\n",
    "### return_ErrorRateFixedProcessDF(), return_ErrorRateFixedClassDF()\n",
    "\n",
    "\n",
    "def return_ErrorRateFixedProcessDF(benchmark=\"cg\", FixProcess=64, predict_class=\"D\"):\n",
    "    FixProcessDF = return_FixedProcessModelDF(\n",
    "        benchmark=benchmark, FixProcess=256)\n",
    "    # print(FixProcessDF)\n",
    "    for index in FixProcessDF.index.tolist():\n",
    "        for column in FixProcessDF.columns.tolist():\n",
    "            PredictNum = return_Predicted(\n",
    "                FixProcessDF.at[index, column], ConvertBencharkClass_inNPB(predict_class))\n",
    "            FixProcessDF.at[index, column] = int(return_ErrorRate(\n",
    "                FunctionName=index, PredictNum=PredictNum, BenchmarkName=benchmark, BenchmarkClass=predict_class, Process=FixProcess)*1000)/1000\n",
    "    return (FixProcessDF)\n",
    "\n",
    "\n",
    "def return_ErrorRateFixedClassDF(benchmark=\"cg\", FixClass=\"B\", predict_process=256):\n",
    "    FixClassDF = return_FixedClassModelDF(benchmark, FixClass=FixClass)\n",
    "    # print(FixClassDF)\n",
    "    for index in FixClassDF.index.tolist():\n",
    "        for column in FixClassDF.columns.tolist():\n",
    "            PredictNum = return_Predicted(\n",
    "                FixClassDF.at[index, column], predict_process)\n",
    "            FixClassDF.at[index, column] = int(return_ErrorRate(\n",
    "                FunctionName=index, PredictNum=PredictNum, BenchmarkName=benchmark, BenchmarkClass=FixClass, Process=predict_process)*1000)/1000\n",
    "    return(FixClassDF)\n",
    "\n",
    "# for benchmark in benchmarks:\n",
    "#     GenerateErrorRateTable(benchmark=benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e8f58-0ea7-4923-8544-cd2b82287d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "processes_onlyBTSP = [1, 4, 16, 64, 256]\n",
    "processes_excludeBTSP = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "# 関数：reutrn_MapeTableRowDataframe_FixedClass\n",
    "# 引数：benchmark=<ベンチマーク名>, FixedClass=<固定するクラス>, test_ratio=<テストに用いる割合>\n",
    "# 返値：引数の指定通りの設定で、各モデルの採用割合, 最大値, 最小値が保持された独自クラスを要素としたデータフレーム\n",
    "\n",
    "\n",
    "def reutrn_MapeTableRowDataframe_FixedClass(benchmark=\"cg\", FixedClass=\"B\", test_ratio=0.3, enableTrain=True):\n",
    "\n",
    "    if(benchmark == \"bt\" or benchmark == \"sp\"):\n",
    "        process = processes_onlyBTSP\n",
    "    else:\n",
    "        process = processes_excludeBTSP\n",
    "\n",
    "    FixedClassList = return_fixed_class(\n",
    "        BenchMark=benchmark, Processes=process, FixedBenchMarkClass=FixedClass)\n",
    "    FixedClassDataFrame = pd.concat(FixedClassList, axis=1)\n",
    "\n",
    "    DictData = return_dict_Data(FixedClassDataFrame)\n",
    "    MapeTablePerBenchmark = return_MapeTable_per_benchmark(\n",
    "        DictData, test_ratio=test_ratio, train=enableTrain)\n",
    "    MapeTableRow = return_MapeTable_row(MapeTablePerBenchmark, benchmark)\n",
    "    MapeTableSource = MapeTableRow\n",
    "    MapeTableSourceColumnName = [\n",
    "        \"線形モデル\", \"対数モデル\", \"反比例モデル\", \"分岐モデル\", \"ベンチマーク名\"]\n",
    "    MapeTable = pd.DataFrame(MapeTableSource)\n",
    "    MapeTable = MapeTable.T\n",
    "    MapeTable.columns = MapeTableSourceColumnName\n",
    "    MapeTable = MapeTable.set_index('ベンチマーク名')\n",
    "    return (MapeTable)\n",
    "\n",
    "\n",
    "# 関数：return_MapeTableDataframe_FixedClass()\n",
    "# 引数：FixedClass=<固定するベンチマーククラス>, test_ratio=<テストに用いる割合>\n",
    "# 返値：引数の指定通りの設定で全てのベンチマークに関する、各モデルの採用割合、最大値、最小値が保持された独自クラスを要素としたデータフレーム\n",
    "def return_MapeTableDataframe_FixedClass(FixedClass=\"B\", test_ratio=0.3):\n",
    "    MapeTableRowDataframes_list = []\n",
    "    for benchmark in benchmarks:\n",
    "        MapeTableRowDataframe = reutrn_MapeTableRowDataframe_FixedClass(\n",
    "            benchmark=benchmark, FixedClass=FixedClass, test_ratio=test_ratio, enableTrain=True)\n",
    "        MapeTableRowDataframes_list.append(MapeTableRowDataframe)\n",
    "    MapeTableDataframe = pd.concat(MapeTableRowDataframes_list)\n",
    "    return(MapeTableDataframe)\n",
    "\n",
    "# 関数：return_MapeTableOnlyStrDataframe()\n",
    "# 引数：return_MapeTableDataframe_Fixed<Class or Process>()の返値\n",
    "# 返値：文字列を要素としたMape表のデータフレーム\n",
    "\n",
    "\n",
    "def return_MapeTableOnlyStrDataframe(input_MapeTableDF):\n",
    "    index = input_MapeTableDF.index\n",
    "    columns = input_MapeTableDF.columns\n",
    "    return_DF = pd.DataFrame().reindex_like(input_MapeTableDF).astype('str')\n",
    "    for i in index:\n",
    "        for j in columns:\n",
    "            contentStr = input_MapeTableDF.at[i, j].return_Data()\n",
    "            return_DF.at[i, j] = contentStr\n",
    "    return(return_DF)\n",
    "\n",
    "# 利用例\n",
    "# input_MapeTableDF = return_MapeTableDataframe_FixedClass()\n",
    "# print(return_MapeTableOnlyStrDataframe(input_MapeTableDF=input_MapeTableDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52e0aa-c7f6-4a6e-bb3b-1aa712c7b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数：reutrn_MapeTableRowDataframe_FixedProcess\n",
    "# 引数：benchmark=<ベンチマーク名>, FixedProcess=<固定する実行プロセス>, test_ratio=<テストに用いる割合>\n",
    "# 返値：引数の指定通りの設定で、各モデルの採用割合, 最大値, 最小値が保持された独自クラスを要素としたデータフレーム\n",
    "def reutrn_MapeTableRowDataframe_FixedProcess(benchmark=\"cg\", FixedProcess=256, test_ratio=0.3, enableTrain=True):\n",
    "\n",
    "    # classes[2:]としているのは、変換関数(ConvertBenchmarkClasses)がベンチマーククラスS, Wの数値化に対応していないため\n",
    "    classes_inFunc = classes[2:]\n",
    "    classes_onNum = ConvertBenchmarkClasses(classes_inFunc)\n",
    "\n",
    "    FixedProcessList = return_fixed_process(\n",
    "        BenchMark=benchmark, BenchMarkClasses=classes_inFunc, FixedProcess=FixedProcess)\n",
    "    FixedProcessDataFrame = pd.concat(FixedProcessList, axis=1)\n",
    "\n",
    "    DictData = return_dict_Data(FixedProcessDataFrame)\n",
    "    DictData['rowData'] = classes_onNum\n",
    "    MapeTablePerBenchmark = return_MapeTable_per_benchmark(\n",
    "        DictData, test_ratio=test_ratio, train=enableTrain)\n",
    "    MapeTableRow = return_MapeTable_row(MapeTablePerBenchmark, benchmark)\n",
    "    MapeTableSource = MapeTableRow\n",
    "    MapeTableSourceColumnName = [\n",
    "        \"線形モデル\", \"対数モデル\", \"反比例モデル\", \"分岐モデル\", \"ベンチマーク名\"]\n",
    "    MapeTable = pd.DataFrame(MapeTableSource)\n",
    "    MapeTable = MapeTable.T\n",
    "    MapeTable.columns = MapeTableSourceColumnName\n",
    "    MapeTable = MapeTable.set_index('ベンチマーク名')\n",
    "    return (MapeTable)\n",
    "\n",
    "# 関数：return_MapeTableDataframe_FixedProcess()\n",
    "# 引数：FixedProcess=<固定する実行プロセス数>, test_ratio=<テストに用いる割合>\n",
    "# 返値：引数の指定通りの設定で全てのベンチマークに関する、各モデルの採用割合、最大値、最小値が保持された独自クラスを要素としたデータフレーム\n",
    "\n",
    "\n",
    "def return_MapeTableDataframe_FixedProcess(FixedProcess=256, test_ratio=0.3):\n",
    "    MapeTableRowDataframes_list = []\n",
    "    for benchmark in benchmarks:\n",
    "        MapeTableRowDataframe = reutrn_MapeTableRowDataframe_FixedProcess(\n",
    "            benchmark=benchmark, FixedProcess=FixedProcess, test_ratio=test_ratio, enableTrain=True)\n",
    "        MapeTableRowDataframes_list.append(MapeTableRowDataframe)\n",
    "    MapeTableDataframe = pd.concat(MapeTableRowDataframes_list)\n",
    "    return(MapeTableDataframe)\n",
    "\n",
    "# 利用例\n",
    "# input_MapeTableDF = return_MapeTableDataframe_FixedProcess()\n",
    "# print(return_MapeTableOnlyStrDataframe(input_MapeTableDF=input_MapeTableDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d08344-a1f7-43b4-b5ee-7464a3bd34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数：return_ErrorRateFixed<Class or Process>DFwithAverage()\n",
    "# 引数：benchmark:ベンチマーク名, FixProcess:固定するプロセス数,  predict_class:予測するクラス\n",
    "# 引数：benchmark:ベンチマーク名, FixClass:固定するクラス数,  predict_process:予測するプロセス数\n",
    "# 返り値はエラー率の平均が付与されたエラー率の表\n",
    "def return_ErrorRateFixedProcessDFwithAverate(benchmark=\"cg\", FixProcess=64, predict_class=\"D\"):\n",
    "    ErrorRateFixedProcessDF = return_ErrorRateFixedProcessDF(\n",
    "        benchmark=benchmark, FixProcess=FixProcess, predict_class=predict_class)\n",
    "    average = ErrorRateFixedProcessDF.mean()\n",
    "    average.name = \"Average\"\n",
    "    return(ErrorRateFixedProcessDF.append(average))\n",
    "\n",
    "\n",
    "def return_ErrorRateFixedClassDFwithAverage(benchmark=\"cg\", FixClass=\"B\", predict_process=256):\n",
    "    ErrorRateFixedClsssDF = return_ErrorRateFixedClassDF(\n",
    "        benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "    average = ErrorRateFixedClsssDF.mean()\n",
    "    average.name = \"Average\"\n",
    "    return(ErrorRateFixedClsssDF.append(average))\n",
    "\n",
    "# 関数：convert_StrListToIntList()\n",
    "# 引数：要素はすべて整数なリストのプリント出力\n",
    "# 返り値：数値のリスト\n",
    "\n",
    "\n",
    "def convert_StrListToIntList(InputList: list):\n",
    "    InputList = InputList[1:-1]\n",
    "    ReturnList = [int(x.strip()) for x in InputList.split(',')]\n",
    "    return(ReturnList)\n",
    "\n",
    "# 関数：convert_StrListToStrList()\n",
    "# 引数：要素はすべて一文字のアルファベットなリストのプリント出力\n",
    "# 返り値：アルファベットのリスト\n",
    "\n",
    "\n",
    "def convert_StrListToStrList(InputList: list):\n",
    "    InputList = InputList[1:-1]\n",
    "    InputList = InputList.replace('\\'', '')\n",
    "    ReturnList = [x.strip() for x in InputList.split(',')]\n",
    "    return(ReturnList)\n",
    "\n",
    "# 関数：return_ExecTime()\n",
    "# 引数：ベンチマーク名, ベンチマーククラス, 実行プロセス数\n",
    "# 返り値：引数に該当するベンチマークの実行時間\n",
    "\n",
    "\n",
    "def return_ExecTime(benchmark=\"cg\", BenchmarkClass=\"B\", Process=256):\n",
    "    FileDir = \"./csv_files/\"\n",
    "    FileName = f\"ExecTime@{benchmark}.csv\"\n",
    "    ExecTimeDF = pd.read_csv(f\"{FileDir}{FileName}\", index_col=0)\n",
    "    TargetNum = ExecTimeDF.at[BenchmarkClass, f\"{Process}\"]\n",
    "    if(np.isnan(TargetNum)):\n",
    "        TargetNum = -1\n",
    "    return(TargetNum)\n",
    "\n",
    "# 関数:return_FixClassCost()\n",
    "# 引数:benchmark=ベンチマーク名, ProcessList=実行プロセスのリスト, BenchmarkClass=ベンチマーククラス\n",
    "# 返り値：引数の条件に当てはまるコスト\n",
    "\n",
    "\n",
    "def return_FixClassCost(benchmark=\"cg\", ProcessList=[1, 2, 4], BenchmarkClass=\"B\"):\n",
    "    cost = 0\n",
    "    for process in ProcessList:\n",
    "        ExecTime = return_ExecTime(\n",
    "            benchmark=benchmark, BenchmarkClass=BenchmarkClass, Process=process)\n",
    "        if(ExecTime < 0):\n",
    "            return(-1)\n",
    "        cost += ExecTime * process\n",
    "    return(cost)\n",
    "\n",
    "# 関数:return_FixProcessCost()\n",
    "# 引数:benchmark=ベンチマーク名, BenchmarkClassList=ベンチマーククラスのリスト, Process=実行プロセス\n",
    "# 返り値：引数の条件に当てはまるコスト\n",
    "\n",
    "\n",
    "def return_FixProcessCost(benchmark=\"cg\", BenchmarkClassList=[\"A\", \"B\", \"C\"], Process=256):\n",
    "    cost = 0\n",
    "    for BenchmarkClass in BenchmarkClassList:\n",
    "        ExecTime = return_ExecTime(\n",
    "            benchmark=benchmark, BenchmarkClass=BenchmarkClass, Process=Process)\n",
    "        if(ExecTime < 0):\n",
    "            return(-1)\n",
    "        cost += ExecTime * Process\n",
    "    return(cost)\n",
    "\n",
    "\n",
    "def return_ErrorRateFixedClassAverageAndCosts(benchmark=\"cg\", FixClass=\"B\", predict_process=256):\n",
    "    ErrorRateFixedClassDFwithAverage = return_ErrorRateFixedClassDFwithAverage(\n",
    "        benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "    ErrorRateFixedClassAverageSeries = ErrorRateFixedClassDFwithAverage.loc[\"Average\"]\n",
    "    ErrorRateFixedClassAverageDF = pd.DataFrame(\n",
    "        ErrorRateFixedClassAverageSeries)\n",
    "    index = ErrorRateFixedClassAverageDF.index.tolist()\n",
    "    columns = ErrorRateFixedClassAverageDF.columns.tolist()\n",
    "    listedIndex = []\n",
    "    for i in index:\n",
    "        listedIndex.append(convert_StrListToIntList(i))\n",
    "    Costs = []\n",
    "    for i in listedIndex:\n",
    "        Costs.append(return_FixClassCost(benchmark=benchmark,\n",
    "                     ProcessList=i, BenchmarkClass=FixClass))\n",
    "    ErrorRateFixedClassAverageDF[\"PredictCosts\"] = Costs\n",
    "    return(ErrorRateFixedClassAverageDF)\n",
    "\n",
    "\n",
    "def return_ErrorRateFixedProcessAverageAndCosts(benchmark=\"cg\", FixProcess=256, predict_class=\"D\"):\n",
    "    ErrorRateFixedProcessDFwithAverage = return_ErrorRateFixedProcessDFwithAverate(\n",
    "        benchmark=benchmark, FixProcess=FixProcess, predict_class=predict_class)\n",
    "    ErrorRateFixedProcessAverageSeries = ErrorRateFixedProcessDFwithAverage.loc[\"Average\"]\n",
    "    ErrorRateFixedProcessAverageDF = pd.DataFrame(\n",
    "        ErrorRateFixedProcessAverageSeries)\n",
    "    index = ErrorRateFixedProcessAverageDF.index.tolist()\n",
    "    columns = ErrorRateFixedProcessAverageDF.columns.tolist()\n",
    "    listedIndex = []\n",
    "    for i in index:\n",
    "        listedIndex.append(convert_StrListToStrList(i))\n",
    "    Costs = []\n",
    "    for i in listedIndex:\n",
    "        Costs.append(return_FixProcessCost(benchmark=benchmark,\n",
    "                     BenchmarkClassList=i, Process=FixProcess))\n",
    "    ErrorRateFixedProcessAverageDF[\"PredictCosts\"] = Costs\n",
    "    return(ErrorRateFixedProcessAverageDF)\n",
    "\n",
    "# benchmark=\"cg\"\n",
    "# FixProcess=64\n",
    "# predict_class=\"D\"\n",
    "# FixClass=\"B\"\n",
    "# predict_process=256\n",
    "# BenchmarkClass=\"B\"\n",
    "# Process=64\n",
    "\n",
    "\n",
    "def return_ErrorRateFixedClass_AveragePredictCostRealCost(benchmark=\"cg\", FixClass=\"B\", predict_process=256):\n",
    "    ErrorRateFixedClass = return_ErrorRateFixedClassAverageAndCosts(\n",
    "        benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "    index = ErrorRateFixedClass.index.tolist()\n",
    "    columns = ErrorRateFixedClass.columns.tolist()\n",
    "    RealClass = FixClass\n",
    "    RealProcess = predict_process\n",
    "    RealTime = return_ExecTime(\n",
    "        benchmark=benchmark, BenchmarkClass=RealClass, Process=RealProcess)\n",
    "    RealCost = RealProcess * RealTime\n",
    "    RealCostList = [RealCost] * len(index)\n",
    "    ErrorRateFixedClass[\"RealCost\"] = RealCostList\n",
    "    return(ErrorRateFixedClass)\n",
    "\n",
    "\n",
    "def return_ErrorRateFixedProcess_AveragePredictCostRealCost(benchmark=\"cg\", FixProcess=256, predict_class=\"D\"):\n",
    "    ErrorRateFixedProcess = return_ErrorRateFixedProcessAverageAndCosts(\n",
    "        benchmark=benchmark, FixProcess=FixProcess, predict_class=predict_class)\n",
    "    index = ErrorRateFixedProcess.index.tolist()\n",
    "    columns = ErrorRateFixedProcess.columns.tolist()\n",
    "    RealClass = predict_class\n",
    "    RealProcess = FixProcess\n",
    "    RealTime = return_ExecTime(\n",
    "        benchmark=benchmark, BenchmarkClass=RealClass, Process=RealProcess)\n",
    "    RealCost = RealProcess * RealTime\n",
    "    RealCostList = [RealCost] * len(index)\n",
    "    ErrorRateFixedProcess[\"RealCost\"] = RealCostList\n",
    "    return(ErrorRateFixedProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae63593-c0f1-427f-9806-3839e3e890e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateMapeTable(FixedClass=\"B\", FixedProcess=\"64\", test_ratio=0.3):\n",
    "\n",
    "    DirName = \"./table_LatexForm/\"\n",
    "    FileSuffix = f\"Train0{int(test_ratio*10)}.table\"\n",
    "    FilePrefix = \"MapeTableFixed\"\n",
    "\n",
    "    input_MapeTableDF = return_MapeTableDataframe_FixedProcess(\n",
    "        FixedProcess=FixedProcess, test_ratio=test_ratio)\n",
    "    return_MapeTableDF = return_MapeTableOnlyStrDataframe(\n",
    "        input_MapeTableDF=input_MapeTableDF)\n",
    "    return_MapeTableDF.to_latex(\n",
    "        f\"{DirName}{FilePrefix}{FixedProcess}{FileSuffix}\")\n",
    "    input_MapeTableDF = return_MapeTableDataframe_FixedClass(\n",
    "        FixedClass=FixedClass, test_ratio=test_ratio)\n",
    "    return_MapeTableDF = return_MapeTableOnlyStrDataframe(\n",
    "        input_MapeTableDF=input_MapeTableDF)\n",
    "    return_MapeTableDF.to_latex(\n",
    "        f\"{DirName}{FilePrefix}{FixedClass}{FileSuffix}\")\n",
    "\n",
    "# MAPE表を生成する例\n",
    "# GenerateMapeTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc66d7b-96c6-4c63-aed2-ff88977aa5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数：ConvertIndexNameToNumOfProfile()\n",
    "# 引数：行名が使用したプロファイル, 列名が平均誤差率・予測コスト・実測コストとなっているデータフレーム\n",
    "# 返値：\n",
    "def ConvertIndexNameToNumOfProfile(inputDF, Fixed=\"Process\"):\n",
    "    index = inputDF.index.tolist()\n",
    "    # ConvertedIndexには入力データフレームの行名から使用されたプロファイルの数を格納している。\n",
    "    ConvertedIndex = [0] * len(index)\n",
    "    for i in range(len(index)):\n",
    "        if(Fixed == \"Process\"):\n",
    "            ConvertedIndex[i] = len(convert_StrListToStrList(index[i]))\n",
    "        elif(Fixed == \"Class\"):\n",
    "            ConvertedIndex[i] = len(convert_StrListToIntList(index[i]))\n",
    "    returnDF = inputDF.copy(deep=True)\n",
    "    returnDF.index = ConvertedIndex\n",
    "    return(returnDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afd64e-cbeb-4977-9aa5-a4bc9ff6bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenGraphAveragePerProfileNum(benchmarks=[\"cg\"], Fixed=\"Process\", Fix=64, Predict=\"D\", EnableTitle=False, EnableScatter=False, SaveGraph=False):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for benchmark in benchmarks:\n",
    "        if(Fixed == \"Process\"):\n",
    "            FixedDF = return_ErrorRateFixedProcess_AveragePredictCostRealCost(\n",
    "                benchmark=benchmark, FixProcess=Fix, predict_class=Predict)\n",
    "            GraphTitle = f\"ベンチマーク{benchmarks}で実行プロセス数を固定\"\n",
    "        else:\n",
    "            FixedDF = return_ErrorRateFixedClass_AveragePredictCostRealCost(\n",
    "                benchmark=benchmark, FixClass=Fix, predict_process=Predict)\n",
    "            GraphTitle = f\"ベンチマーク{benchmarks}でベンチマーククラスを固定\"\n",
    "        ConvertedIndexFixedDF = ConvertIndexNameToNumOfProfile(\n",
    "            FixedDF, Fixed=Fixed)\n",
    "        x = ConvertedIndexFixedDF.index.tolist()\n",
    "        y = ConvertedIndexFixedDF[\"Average\"].tolist()\n",
    "        plt.plot(x, y, label=f\"{benchmark.upper()}\", marker='o')\n",
    "        if(EnableScatter):\n",
    "            plt.scatter(x, y)\n",
    "        plt.legend()\n",
    "    plt.xlabel(\"使用したプロファイル数\")\n",
    "    plt.ylabel(\"平均絶対誤差率 [%]\")\n",
    "    plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    if(EnableTitle):\n",
    "        plt.title(GraphTitle)\n",
    "    if(Fixed == \"Process\"):\n",
    "        plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.subplots_adjust(right=0.9)\n",
    "    plt.yscale('log')\n",
    "    if(SaveGraph):\n",
    "        filename = f\"Fix{Fixed}AverageError.pdf\"\n",
    "        plt.savefig(f\"./tmp_GenerateResources/{filename}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# 使用例\n",
    "# GenGraphAveragePerProfileNum(benchmarks=benchmarks, Fixed=\"Process\", Fix=64, Predict=\"D\")\n",
    "# GenGraphAveragePerProfileNum(benchmarks=benchmarks, Fixed=\"Class\", Fix=\"B\", Predict=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d926991-7cb2-42c9-bda9-ef7d0833bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for benchmark in benchmarks:\n",
    "#     print(return_ErrorRateFixedClass_AveragePredictCostRealCost(benchmark=benchmark, FixClass=\"B\", predict_process=256))\n",
    "def GenGraphTotalTimePerProfileNum(benchmark=\"cg\", Fixed=\"Process\", Fix=64, Predict=\"D\", EnableTitle=False, SaveGraph=False):\n",
    "    if(Fixed == \"Process\"):\n",
    "        FixedDF = return_ErrorRateFixedProcess_AveragePredictCostRealCost(\n",
    "            benchmark=benchmark, FixProcess=Fix, predict_class=Predict)\n",
    "        GraphTitle = f\"ベンチマーク{benchmark}で実行プロセス数を固定\"\n",
    "    else:\n",
    "        FixedDF = return_ErrorRateFixedClass_AveragePredictCostRealCost(\n",
    "            benchmark=benchmark, FixClass=Fix, predict_process=Predict)\n",
    "        GraphTitle = f\"ベンチマーク{benchmark}でベンチマーククラスを固定\"\n",
    "    ConvertedIndexFixedDF = ConvertIndexNameToNumOfProfile(\n",
    "        inputDF=FixedDF, Fixed=\"Process\")\n",
    "    x = ConvertedIndexFixedDF.index.tolist()\n",
    "    y = ConvertedIndexFixedDF[\"PredictCosts\"]\n",
    "    RealCost = ConvertedIndexFixedDF[\"RealCost\"]\n",
    "    plt.figure()\n",
    "    plt.plot(x, y, marker=\"o\", color=\"blue\", label=\"予測にかかるコスト\")\n",
    "    plt.plot(x, RealCost, color=\"red\", label=\"実測にかかるコスト\")\n",
    "    plt.xlabel(\"使用したプロファイル数\")\n",
    "    plt.ylabel(\"コスト [秒]\")\n",
    "    plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "#     plt.legend(bbox_to_anchor=(0, -0.15), loc='upper left', borderaxespad=0)\n",
    "    plt.legend(loc='best', borderaxespad=0)\n",
    "    if(EnableTitle):\n",
    "        plt.title(GraphTitle)\n",
    "    if(SaveGraph):\n",
    "        filename = f\"Fix{Fixed}AllTime@{benchmark.upper()}.pdf\"\n",
    "        plt.savefig(f\"./tmp_GenerateResources/{filename}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# 使用例\n",
    "# GenGraphTotalTimePerProfileNum(EnableTitle=True)\n",
    "# GenGraphTotalTimePerProfileNum(benchmark=benchmark, Fixed=\"Class\", Fix=\"C\", Predict=256, EnableTitle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defff8bf-14b9-4aa9-a4e8-7cf4a4a1a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ErrorRateAverageAndCosts(benchmark=\"cg\", Fix=\"B\", Predict=256, Fixed=\"Class\"):\n",
    "    if(Fixed == \"Class\"):\n",
    "        FixDF = return_ErrorRateFixedClassAverageAndCosts(\n",
    "            benchmark=benchmark, FixClass=Fix, predict_process=Predict)\n",
    "    else:\n",
    "        FixDF = return_ErrorRateFixedProcessAverageAndCosts(\n",
    "            benchmark=benchmark, FixProcess=Fix, predict_class=Predict)\n",
    "    index = FixDF.index.tolist()\n",
    "    NewIndex = []\n",
    "    for i in index:\n",
    "        if(Fixed == \"Class\"):\n",
    "            NewIndex.append(len(convert_StrListToIntList(i)))\n",
    "        else:\n",
    "            NewIndex.append(len(convert_StrListToStrList(i)))\n",
    "    FixDF = FixDF.reset_index()\n",
    "    FixDF['index'] = NewIndex\n",
    "    FixDF = FixDF.rename(\n",
    "        columns={'index': '使用したプロファイル数', 'Average': '平均誤差率', 'PredictCosts': \"コスト\"})\n",
    "    FixDF = FixDF.set_index('使用したプロファイル数')\n",
    "    return(FixDF)\n",
    "\n",
    "# # 使用例\n",
    "# sampleDF = return_ErrorRateAverageAndCosts(benchmark=\"cg\", Fix=\"B\", Predict=256, Fixed=\"Class\")\n",
    "# print(sampleDF)\n",
    "# sampleDF  = return_ErrorRateAverageAndCosts(benchmark=\"cg\", Fix=64, Predict=\"D\", Fixed=\"Process\")\n",
    "# print(sampleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7efc7-e16d-44b1-a86f-a43474302c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数：BestModelsInDF()\n",
    "# 引数：return_FixedClassModelDF()の返値\n",
    "# 返値：引数で渡されたDFの要素のモデル名を要素としたDF\n",
    "def BestModelsInDF(inputDF):\n",
    "    returnDF = pd.DataFrame()\n",
    "    returnDF = returnDF.reindex_like(inputDF).astype('str')\n",
    "    index = returnDF.index.tolist()\n",
    "    columns = returnDF.columns.tolist()\n",
    "    for i in index:\n",
    "        for j in columns:\n",
    "            returnDF.at[i, j] = inputDF.at[i, j].ModelName()\n",
    "    return(returnDF)\n",
    "# ### 使用例\n",
    "# benchmark = \"mg\"\n",
    "# FixClass = \"B\"\n",
    "# sampleDF = return_FixedClassModelDF(benchmark = benchmark, FixClass = FixClass)\n",
    "# print(BestModelsInDF(inputDF=sampleDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3661802-8efc-471c-bba2-93e0b5fd86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実データを取得する関数\n",
    "\n",
    "benchmark = \"cg\"\n",
    "benchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "FixedProcess = 64\n",
    "process_onlyBTSP = [1, 4, 16, 64, 256]\n",
    "process_excludeBTSP = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "functionName = \".TAU_application\"\n",
    "\n",
    "\n",
    "def returnRawDF(Benchmark=\"cg\", functionName=\".TAU_application\", fix=\"Process\", benchmarkClass=[\"A\", \"B\", \"C\", \"D\"], FixedProcess=64, Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"B\"):\n",
    "    if(fix == \"Process\"):\n",
    "        fixed = return_fixed_process(\n",
    "            BenchMark=Benchmark, BenchMarkClasses=benchmarkClass, FixedProcess=FixedProcess)\n",
    "    else:\n",
    "        fixed = return_fixed_class(\n",
    "            BenchMark=Benchmark, Processes=Processes, FixedBenchMarkClass=FixedBenchmarkClass)\n",
    "    summaryRawData = pd.concat(fixed, axis=1)\n",
    "    return summaryRawData.loc[[functionName]]\n",
    "\n",
    "# returnRawDF(fix=\"Process\")\n",
    "# returnRawDF(fix=\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b791f79-51f6-4b28-8bc2-fd6596cc074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマークごとの生データを取得する関数\n",
    "# 引数は returnRawDF() と基本的に同等\n",
    "# 異なる部分は functionName が存在しないこと\n",
    "\n",
    "def returnRawDFperBenchmark(Benchmark=\"cg\", fix=\"Process\", benchmarkClass=[\"A\", \"B\", \"C\", \"D\"], FixedProcess=64, Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"B\"):\n",
    "    if(fix == \"Process\"):\n",
    "        fixed = return_fixed_process(\n",
    "            BenchMark=Benchmark, BenchMarkClasses=benchmarkClass, FixedProcess=FixedProcess)\n",
    "    else:\n",
    "        fixed = return_fixed_class(\n",
    "            BenchMark=Benchmark, Processes=Processes, FixedBenchMarkClass=FixedBenchmarkClass)\n",
    "    summaryRawData = pd.concat(fixed, axis=1)\n",
    "    return summaryRawData\n",
    "\n",
    "# # 使用例\n",
    "# returnRawDFperBenchmark(fix=\"Process\")\n",
    "# returnRawDFperBenchmark(fix=\"Classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb8140-ee8c-4a43-a9c5-90631adf0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数：inputDF  はreturnRawDF()の返り値\n",
    "# 引数：repeated はDATA列を繰り返す回数\n",
    "# 引数：includeLastData は学習に最後の値を使うかどうかを指定\n",
    "\n",
    "def generateInputFileForExtraP(inputDF=returnRawDF(fix=\"Class\"), repeated=3, includeLastData=False, fileName=\"tmp_functionName.txt\"):\n",
    "\n",
    "    # https://github.com/extra-p/extrap/blob/master/docs/examples/input.txt\n",
    "    # が入力例のテキストファイルとなっており、これに対応するファイルを作成する。\n",
    "    functionNames = inputDF.index.tolist()\n",
    "    for functionName in functionNames:\n",
    "        tmpfile = open(fileName, 'w+')\n",
    "        columns = inputDF.columns.tolist()\n",
    "        tmpfile.write(\"PARAMETER p\\n\")\n",
    "        tmpfile.write(\"POINTS\")\n",
    "\n",
    "        if(includeLastData == False):\n",
    "            columns = columns[:-1]\n",
    "\n",
    "        for column in columns:\n",
    "            tmpfile.write(f\" ({column})\")\n",
    "        tmpfile.write(\"\\n\\n\")\n",
    "\n",
    "        tmpfile.write(f\"REGION {functionName}\\n\")\n",
    "        tmpfile.write(\"METRIC functionCall\\n\")\n",
    "\n",
    "        for column in columns:\n",
    "\n",
    "            tmpfile.write(\"DATA\")\n",
    "            content = float(inputDF.at[functionName, column])\n",
    "\n",
    "            for i in range(repeated):\n",
    "                tmpfile.write(f\" {content}\")\n",
    "            tmpfile.write(\"\\n\")\n",
    "\n",
    "        tmpfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff975bf-950e-4bde-b676-5cb1d6a5acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数：input_x  は横軸に相当する値のリスト\n",
    "# 引数：input_y  は縦軸に相当する値のリスト\n",
    "# 引数：repeated はDATA列を繰り返す回数\n",
    "# 引数：includeFirstData は学習に最初の値を使うかどうかを指定\n",
    "# 引数：includeLastData は学習に最初の値を使うかどうかを指定\n",
    "\n",
    "def generateInputFileForExtraPfromLists(input_x=[1, 2, 4, 8, 16, 32, 64, 128, 256], input_y=[1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], repeated=3, includeFirstData=False, includeLastData=False, fileName=\"tmp_functionName.txt\"):\n",
    "\n",
    "    # https://github.com/extra-p/extrap/blob/master/docs/examples/input.txt\n",
    "    # が入力例のテキストファイルとなっており、これに対応するファイルを作成する。\n",
    "    tmpfile = open(fileName, 'w+')\n",
    "    tmpfile.write(\"PARAMETER p\\n\")\n",
    "    tmpfile.write(\"POINTS\")\n",
    "\n",
    "    if(includeLastData == False):\n",
    "        input_x = input_x[:-1]\n",
    "        input_y = input_y[:-1]\n",
    "    if(includeFirstData == False):\n",
    "        input_x = input_x[1:]\n",
    "        input_y = input_y[1:]\n",
    "\n",
    "    for x in input_x:\n",
    "        tmpfile.write(f\" ({x})\")\n",
    "    tmpfile.write(\"\\n\\n\")\n",
    "\n",
    "    tmpfile.write(f\"REGION {functionName}\\n\")\n",
    "    tmpfile.write(\"METRIC functionCall\\n\")\n",
    "\n",
    "    for y in input_y:\n",
    "\n",
    "        tmpfile.write(\"DATA\")\n",
    "        content = float(y)\n",
    "\n",
    "        for i in range(repeated):\n",
    "            tmpfile.write(f\" {content}\")\n",
    "        tmpfile.write(\"\\n\")\n",
    "\n",
    "    tmpfile.close()\n",
    "\n",
    "##### 使用例 #####\n",
    "##準備##\n",
    "# inputDF = returnRawDF(fix=\"Class\")\n",
    "# input_x = inputDF.columns.tolist()\n",
    "# functionNames = inputDF.index.tolist()\n",
    "# functionName = functionNames[0]\n",
    "# input_y = []\n",
    "# for x in input_x:\n",
    "#     input_y.append(inputDF.at[functionName, x])\n",
    "##使用##\n",
    "# generateInputFileForExtraPfromLists(input_x=input_x, input_y=input_y, repeated=3, includeFirstData=False, includeLastData=False, fileName=\"tmp_functionName.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6454d94-d2df-4ef8-be0e-eb9651fdb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの共通部分となるクラス\n",
    "# すべての引数はただのリスト。クラスの初期化時に\"\"np.reshape()\"\"を実行する\n",
    "class ModelBase2:\n",
    "    def __init__(self, train_x, train_y, target_x=[], target_y=[], benchmark_name=\"benchmark_name\", function_name=\"function_name\"):\n",
    "        self.benchmark_name = benchmark_name\n",
    "        self.function_name = function_name\n",
    "\n",
    "        self.train_x = np.reshape(train_x, (-1, 1))\n",
    "        self.train_y = np.reshape(train_y, (-1, 1))\n",
    "        self.target_x = np.reshape(target_x, (-1, 1))\n",
    "        self.target_x = np.reshape(target_x, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5acdcb-06e1-447f-8b2b-1a5019b62bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形モデルでロバスト回帰を行う\n",
    "# 作成したModelBase2を継承\n",
    "\n",
    "class ModelLin_rob(ModelBase2):\n",
    "    def calc_hr(self):\n",
    "        self.hr = HuberRegressor()\n",
    "        self.hr.fit(self.train_x, self.train_y)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.target_x)\n",
    "        self.mape_score = float(mape_score(self.target_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted = self.lr.predict(self.train_x)\n",
    "        self.mape_score_InTrain = float(\n",
    "            mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def predict(self, num):\n",
    "        predicted = self.hr.predict(num)\n",
    "        return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLin_rob\")\n",
    "\n",
    "# 反比例モデルでロバスト回帰を行う\n",
    "# ModelBase2を継承\n",
    "\n",
    "\n",
    "def ip_func(x):\n",
    "    return 1/x\n",
    "\n",
    "\n",
    "class ModelIp_rob(ModelBase2):\n",
    "\n",
    "    def calc_hr(self):\n",
    "        self.transformer_ip = sp.FunctionTransformer(\n",
    "            func=ip_func, inverse_func=ip_func)\n",
    "        y_train_ip = self.transformer_ip.transform(self.train_y)\n",
    "        self.hr = HuberRegressor()\n",
    "        self.hr.fit(self.train_x, y_train_ip)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted_ip = self.hr.predict(self.test_x)\n",
    "        test_y_predicted = self.transformer_ip.inverse_transform(\n",
    "            test_y_predicted_ip)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted_ip = self.hr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(\n",
    "            train_y_predicted_ip)\n",
    "        self.mape_score_InTrain = float(\n",
    "            mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def predict(self, num):\n",
    "        predicted_ip = self.hr.predict(num)\n",
    "        predicted = self.transformer_ip.inverse_transform(predicted_ip)\n",
    "        return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelIP_rob\")\n",
    "\n",
    "# 対数モデルでロバスト回帰を行う\n",
    "# ModelBase2を継承\n",
    "\n",
    "\n",
    "def inverter_log10_func(x):\n",
    "    return 10**x\n",
    "\n",
    "\n",
    "class ModelLog10_rob(ModelBase2):\n",
    "\n",
    "    def calc_hr(self):\n",
    "        self.transformer_log10 = sp.FunctionTransformer(\n",
    "            func=np.log10, inverse_func=inverter_log10_func)\n",
    "        x_train_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        y_train_log10 = self.transformer_log10.transform(self.train_y)\n",
    "\n",
    "        self.hr = HuberRegressor()\n",
    "        self.hr.fit(x_train_log10, y_train_log10)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_x_log10 = self.transformer_log10.transform(self.test_x)\n",
    "        test_y_predicted_log10 = self.hr.predict(test_x_log10)\n",
    "        test_y_predicted = self.transformer_log10.inverse_transform(\n",
    "            test_y_predicted_log10)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.hr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(\n",
    "            train_y_predicted_log10)\n",
    "        self.mape_score_InTrain = float(\n",
    "            mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def predict(self, num):\n",
    "        num_log10 = self.transformer_log10.transform(num)\n",
    "        predicted_log10 = self.hr.predict(num_log10)\n",
    "        predicted = self.transformer_log10.inverse_transform(predicted_log10)\n",
    "        return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLog10_rob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ead3f-e427-482d-bd7b-bbd4ca493c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反比例モデルmk2\n",
    "# ModelBaseを継承した反比例モデルは yの逆数をとっていた。\n",
    "# しかし、それでは意図したモデルとならないことが判明した。\n",
    "# そのため、xの逆数をとる、反比例モデルがコレ。\n",
    "\n",
    "class ModelIp_mk2(ModelBase2):\n",
    "\n",
    "    def calc_lr(self):\n",
    "        self.transformer_ip = sp.FunctionTransformer(\n",
    "            func=ip_func, inverse_func=ip_func)\n",
    "        x_train_ip = self.transformer_ip.transform(self.train_x)\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(x_train_ip, self.train_y)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num = np.reshape(num, (-1, 1))\n",
    "        numConverted = self.transformer_ip.transform(num)\n",
    "        predicted = self.lr.predict(numConverted)\n",
    "        return(predicted)\n",
    "\n",
    "    def return_coef_(self):\n",
    "        return self.lr.coef_\n",
    "\n",
    "    def return_intercept_(self):\n",
    "        return self.lr.intercept_\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelIp\")\n",
    "\n",
    "# # 使用例\n",
    "# modelIpMk2 = ModelIp_mk2(train_x=train_x, train_y=train_y, target_x=target_x, target_y=target_y)\n",
    "# modelIpMk2.calc_lr()\n",
    "# plot_y = modelIpMk2.predict(plot_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee7c1d-b568-4148-9e82-0beac315844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対数モデルmk2\n",
    "# ModelBaseを継承した対数モデルはどこかに不具合がある。\n",
    "# ModelBase2を継承して、改修した対数モデルがこのモデル。\n",
    "\n",
    "class ModelLog10_mk2(ModelBase2):\n",
    "\n",
    "    def calc_lr(self):\n",
    "        self.transformer_log10 = sp.FunctionTransformer(\n",
    "            func=np.log10, inverse_func=inverter_log10_func)\n",
    "        x_train_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(x_train_log10, self.train_y)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num = np.reshape(num, (-1, 1))\n",
    "        numConverted = self.transformer_log10.transform(num)\n",
    "        predicted = self.lr.predict(numConverted)\n",
    "        return(predicted)\n",
    "\n",
    "    def return_coef_(self):\n",
    "        return self.lr.coef_\n",
    "\n",
    "    def return_intercept_(self):\n",
    "        return self.lr.intercept_\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLog\")\n",
    "\n",
    "# # 使用例\n",
    "# modelLog10Mk2 = ModelIp_mk2(train_x=train_x, train_y=train_y, target_x=target_x, target_y=target_y)\n",
    "# modelLog10Mk2.calc_lr()\n",
    "# plot_y = modelLog10Mk2.predict(plot_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6e1bc-5e4e-48b7-92d8-22156c78f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形モデルmk2\n",
    "# ModelBase2を継承して、改修したモデル。\n",
    "\n",
    "class ModelLin_mk2(ModelBase2):\n",
    "\n",
    "    def calc_lr(self):\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.train_x, self.train_y)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num = np.reshape(num, (-1, 1))\n",
    "        predicted = self.lr.predict(num)\n",
    "        return(predicted)\n",
    "\n",
    "    def return_coef_(self):\n",
    "        return self.lr.coef_\n",
    "\n",
    "    def return_intercept_(self):\n",
    "        return self.lr.intercept_\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLin\")\n",
    "\n",
    "# 分岐モデルmk2\n",
    "# ModelBase2を継承して、改修したモデル\n",
    "\n",
    "\n",
    "class ModelBranch_mk2(ModelBase2):\n",
    "    def calc_lr(self):\n",
    "        self.t = np.ndarray.argmax(self.train_y)\n",
    "        self.t_num = self.train_x[self.t]\n",
    "        if (self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.train_x, self.train_y)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.train_x, self.train_y)\n",
    "        else:\n",
    "            self.train_x_1 = self.train_x[:self.t]\n",
    "            self.train_x_2 = self.train_x[self.t:]\n",
    "            self.train_y_1 = self.train_y[:self.t]\n",
    "            self.train_y_2 = self.train_y[self.t:]\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.train_x_1, self.train_y_1)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.train_x_2, self.train_y_2)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num = np.reshape(num, (-1, 1))\n",
    "        num_t = np.ndarray.argmax(num)\n",
    "        num_t_max = num[num_t]\n",
    "        k = np.abs(np.asarray(num) - self.t_num).argmin()\n",
    "        if(len(num) == 1 and num_t_max >= self.t_num):\n",
    "            predicted = self.lr2.predict(num)\n",
    "            return(predicted)\n",
    "        elif (num_t_max < self.train_x[self.t] or k == 0):\n",
    "            predicted = self.lr1.predict(num)\n",
    "            return(predicted)\n",
    "        else:\n",
    "            num_1 = num[:k]\n",
    "            num_2 = num[k:]\n",
    "            predicted_1 = self.lr1.predict(num_1)\n",
    "            predicted_2 = self.lr2.predict(num_2)\n",
    "            predicted = np.concatenate([predicted_1, predicted_2])\n",
    "            return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelBranch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc79b3-c284-4a9f-95c3-c0c1fbbb6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectFunctionNamesPerBenchmark(benchmarkName=\"cg\", benchmarkClasses=[\"A\", \"B\", \"C\"], processes=[1, 2, 4, 8], baseDir=\"./csv_files/\"):\n",
    "    dataframesList = []\n",
    "    for benchmarkClass in benchmarkClasses:\n",
    "        for process in processes:\n",
    "            # ファイル名を決める\n",
    "            fileName = (\"pprof_\"+benchmarkName +\n",
    "                        benchmarkClass+str(process)+\".csv\")\n",
    "            # ファイルのパスを決める\n",
    "            filePath = baseDir + fileName\n",
    "            # ファイルが存在して、空データではないという条件で\n",
    "            if (os.path.exists(filePath) and os.stat(filePath).st_size != 0):\n",
    "                DF = pd.read_csv(filePath)\n",
    "                len_indice = DF.shape[0]\n",
    "                len_columns = DF.shape[1]\n",
    "\n",
    "                DF = DF.rename(\n",
    "                    columns={'Name': 'functionName', '#Call': 'call'})\n",
    "                DF['benchmarkClass'] = [\n",
    "                    ConvertBencharkClass_inNPB(benchmarkClass)] * len_indice\n",
    "                DF['benchmarkName'] = [benchmarkName] * len_indice\n",
    "                DF['process'] = [str(process)] * len_indice\n",
    "                dataframesList.append(DF)\n",
    "    returnDF = pd.concat(dataframesList)\n",
    "    return returnDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363841f-da7c-4e12-9492-611e5d153926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験結果を集計するためのデータフレームのカラムの名称のリストを返す関数\n",
    "def return_numOfColumns(dataType=False):\n",
    "    returnList = []\n",
    "    returnDict = {}\n",
    "    # ベンチマーク名\n",
    "    returnList.append(\"benchmarkName\")\n",
    "    returnDict[\"benchmarkName\"] = str\n",
    "    # 関数名\n",
    "    returnList.append(\"functionName\")\n",
    "    returnDict[\"functionName\"] = str\n",
    "    # 使用データ(数値化されたリスト,説明変数)\n",
    "    returnList.append(\"usedData_x\")\n",
    "    returnDict[\"usedData_x\"] = object\n",
    "    # 使用データ(数値化されたリスト,目的変数)\n",
    "    returnList.append(\"usedData_y\")\n",
    "    returnDict[\"usedData_y\"] = object\n",
    "    # 使用データ数\n",
    "    returnList.append(\"numOfData\")\n",
    "    returnDict[\"numOfData\"] = \"int16\"\n",
    "    # 固定したもの(\"Process\" or \"Class\")\n",
    "    returnList.append(\"ProcessOrClass\")\n",
    "    returnDict[\"ProcessOrClass\"] = str\n",
    "    # 固定したもの(プロセス数(数値)or問題サイズ(数値))\n",
    "    returnList.append(\"fixed\")\n",
    "    returnDict[\"fixed\"] = \"float32\"\n",
    "    # 予測対象プロセス数\n",
    "    returnList.append(\"targetNumOfProcess\")\n",
    "    returnDict[\"targetNumOfProcess\"] = \"int16\"\n",
    "    # 予測対象問題サイズ（数値）\n",
    "    returnList.append(\"targetNumOfProblemSize\")\n",
    "    returnDict[\"targetNumOfProblemSize\"] = \"float32\"\n",
    "    # 予測対象問題サイズ\n",
    "    returnList.append(\"targetProblemSize\")\n",
    "    returnDict[\"targetProblemSize\"] = str\n",
    "    # 予測対象関数コール回数\n",
    "    returnList.append(\"targetNumOfFunctionCall\")\n",
    "    returnDict[\"targetNumOfFunctionCall\"] = \"float32\"\n",
    "    # 予測対象の条件で予測された関数コール回数\n",
    "    returnList.append(\"predictedTargetNumOfFunctionCall\")\n",
    "    returnDict[\"predictedTargetNumOfFunctionCall\"] = \"float32\"\n",
    "    # 線形モデルのオブジェクト\n",
    "    returnList.append(\"objectLinModel\")\n",
    "    returnDict[\"objectLinModel\"] = object\n",
    "    # 線形モデルのMAPE\n",
    "    returnList.append(\"MAPEOfLinModel\")\n",
    "    returnDict[\"MAPEOfLinModel\"] = \"float32\"\n",
    "    # 反比例モデルのオブジェクト\n",
    "    returnList.append(\"objectIpModel\")\n",
    "    returnDict[\"objectIpModel\"] = object\n",
    "    # 反比例モデルのMAPE\n",
    "    returnList.append(\"MAPEOfIpModel\")\n",
    "    returnDict[\"MAPEOfIpModel\"] = \"float32\"\n",
    "    # 対数モデルのオブジェクト\n",
    "    returnList.append(\"objectLogModel\")\n",
    "    returnDict[\"objectLogModel\"] = object\n",
    "    # 対数モデルのMAPE\n",
    "    returnList.append(\"MAPEOfLogModel\")\n",
    "    returnDict[\"MAPEOfLogModel\"] = \"float32\"\n",
    "    # 線形飽和モデルのオブジェクト\n",
    "    returnList.append(\"objectBranchModel\")\n",
    "    returnDict[\"objectBranchModel\"] = object\n",
    "    # 線形飽和モデルのMAPE\n",
    "    returnList.append(\"MAPEOfBranchModel\")\n",
    "    returnDict[\"MAPEOfBranchModel\"] = \"float32\"\n",
    "    # 最も相対誤差の小さいモデル名\n",
    "    returnList.append(\"objectBestModelName\")\n",
    "    returnDict[\"objectBestModelName\"] = object\n",
    "    # 最小の相対誤差\n",
    "    returnList.append(\"relativeErrorOfBestModel\")\n",
    "    returnDict[\"relativeErrorOfBestModel\"] = \"float32\"\n",
    "    if(dataType == True):\n",
    "        return(returnDict)\n",
    "    else:\n",
    "        return(returnList)\n",
    "\n",
    "# 使用例\n",
    "# columnNames = return_numOfColumns()\n",
    "# df_sample = pd.DataFrame(columns=columnNames)\n",
    "# df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19115238-af8e-4a5c-bf96-8c3634197ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマーク名・関数名・プロセス数・問題サイズを指定することで、その条件での関数コール回数を取得する関数\n",
    "\n",
    "def returnSpecificData(benchmarkName=\"cg\", functionName=\".TAU_application\", process=256, benchmarkClass=\"D\"):\n",
    "    targetRawDF = returnRawDF(Benchmark=benchmarkName, functionName=functionName, benchmarkClass=[\n",
    "                              benchmarkClass], FixedProcess=process, Processes=[process], FixedBenchmarkClass=benchmarkClass)\n",
    "    return targetRawDF.iat[0, 0]\n",
    "# returnSpecificData(benchmarkName=\"mg\", functionName=\"BUBBLE\", process=256, benchmarkClass=\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985084a5-4e6d-49a7-95a9-bf132c203101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_numOfColumns()でのカラム名としてのモデル名、モデルのメソッドModelName()が返すモデル名を相互的なキー・バリューとした辞書を返す関数\n",
    "def returnDictModelNames():\n",
    "    returnDict = {}\n",
    "    # カラム名をキー・モデルが返すモデル名をバリュー\n",
    "    returnDict[\"objectLinModel\"] = \"ModelLin\"\n",
    "    returnDict[\"objectIpModel\"] = \"ModelIp\"\n",
    "    returnDict[\"objectLogModel\"] = \"ModelLog\"\n",
    "    returnDict[\"objectBranchModel\"] = \"ModelBranch\"\n",
    "    # モデルが返すモデル名をキー・カラム名をバリュー\n",
    "    returnDict[\"ModelLin\"] = \"objectLinModel\"\n",
    "    returnDict[\"ModelIp\"] = \"objectIpModel\"\n",
    "    returnDict[\"ModelLog\"] = \"objectLogModel\"\n",
    "    returnDict[\"ModelBranch\"] = \"objectBranchModel\"\n",
    "\n",
    "    return(returnDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c609a-66bd-47c5-b6da-47b4342042e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数のreal_data, predict_dataから相対誤差を返す関数\n",
    "# 単位は「％」\n",
    "\n",
    "def returnRelativeErrorRate(real_data: float, predict_data: float):\n",
    "    return(abs(real_data - predict_data)/real_data*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b86007-193d-4557-8cd3-b4e214d6a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を集計するためのDFに挿入するSeriesを作成する関数\n",
    "def returnSeriesOfData(benchmarkName=\"benhmarkName\", functionName=\"functionName\", raw_x=[1, 2, 3], raw_y=[1, 2, 3], fix=\"Class\", fixed=\"B\", targetProcess=256, targetProblemSize=\"B\"):\n",
    "\n",
    "    dataSeries = pd.Series(index=return_numOfColumns(), dtype=object)\n",
    "    dataSeries[\"benchmarkName\"] = benchmarkName\n",
    "    dataSeries[\"functionName\"] = functionName\n",
    "    dataSeries[\"usedData_x\"] = raw_x\n",
    "    dataSeries[\"usedData_y\"] = raw_y\n",
    "    dataSeries[\"numOfData\"] = len(raw_y)\n",
    "    dataSeries[\"ProcessOrClass\"] = fix\n",
    "    dataSeries[\"fixed\"] = ConvertBencharkClass_inNPB(fixed)\n",
    "    targetNumOfProcess = targetProcess\n",
    "    dataSeries[\"targetNumOfProcess\"] = targetProcess\n",
    "    targetProblemsize = fixed\n",
    "    targetNumOfProblemSize = ConvertBencharkClass_inNPB(targetProblemsize)\n",
    "    dataSeries[\"targetNumOfProblemSize\"] = targetNumOfProblemSize\n",
    "    dataSeries[\"targetProblemSize\"] = targetProblemsize\n",
    "\n",
    "    dataSeries[\"targetNumOfFunctionCall\"] = returnSpecificData(\n",
    "        benchmarkName=benchmarkName, functionName=functionName, process=targetNumOfProcess, benchmarkClass=targetProblemsize)\n",
    "\n",
    "    # MAPE の算出には mape_score()を用いる\n",
    "    # mape_score()の返り値の単位は％\n",
    "\n",
    "    raw_x\n",
    "    raw_y\n",
    "    # 線形モデル\n",
    "    modelLin = ModelLin_mk2(train_x=raw_x, train_y=raw_y)\n",
    "    modelLin.calc_lr()\n",
    "    predicted_y = modelLin.predict(raw_x)\n",
    "    dataSeries[\"objectLinModel\"] = modelLin\n",
    "    dataSeries[\"MAPEOfLinModel\"] = mape_score(predicted_y, raw_y)\n",
    "    # 反比例モデル\n",
    "    modelIp = ModelIp_mk2(train_x=raw_x, train_y=raw_y)\n",
    "    modelIp.calc_lr()\n",
    "    predicted_y = modelIp.predict(raw_x)\n",
    "    dataSeries[\"objectIpModel\"] = modelIp\n",
    "    dataSeries[\"MAPEOfIpModel\"] = mape_score(predicted_y, raw_y)\n",
    "    # 対数モデル\n",
    "    modelLog = ModelLog10_mk2(train_x=raw_x, train_y=raw_y)\n",
    "    modelLog.calc_lr()\n",
    "    predicted_y = modelLog.predict(raw_x)\n",
    "    dataSeries[\"objectLogModel\"] = modelLog\n",
    "    dataSeries[\"MAPEOfLogModel\"] = mape_score(predicted_y, raw_y)\n",
    "    # 分岐モデル\n",
    "    modelBranch = ModelBranch_mk2(train_x=raw_x, train_y=raw_y)\n",
    "    modelBranch.calc_lr()\n",
    "    predicted_y = modelBranch.predict(raw_x)\n",
    "    dataSeries[\"objectBranchModel\"] = modelBranch\n",
    "    dataSeries[\"MAPEOfBranchModel\"] = mape_score(predicted_y, raw_y)\n",
    "    # 最適なモデルのモデルのモデル名・MAPE値の算出\n",
    "    listToCalcBestModel = {}\n",
    "    listToCalcBestModel[dataSeries[\"objectLinModel\"].ModelName(\n",
    "    )] = dataSeries[\"MAPEOfLinModel\"]\n",
    "    listToCalcBestModel[dataSeries[\"objectIpModel\"].ModelName(\n",
    "    )] = dataSeries[\"MAPEOfIpModel\"]\n",
    "    listToCalcBestModel[dataSeries[\"objectLogModel\"].ModelName(\n",
    "    )] = dataSeries[\"MAPEOfLogModel\"]\n",
    "    listToCalcBestModel[dataSeries[\"objectBranchModel\"].ModelName(\n",
    "    )] = dataSeries[\"MAPEOfBranchModel\"]\n",
    "    minMAPE = min(listToCalcBestModel.values())\n",
    "    dataSeries[\"MAPEOfBestModel\"] = minMAPE\n",
    "    dataSeries[\"objectBestModelName\"] = [\n",
    "        k for k, v in listToCalcBestModel.items() if v == minMAPE][0]\n",
    "    # relativeErrorOfBestModelへのデータ格納処理\n",
    "    # これには、学習したモデルから対象となる関数コール回数を予測し、予測値と実測値の相対誤差を入れる\n",
    "    dictOfModelNames = returnDictModelNames()\n",
    "    bestModelName = dataSeries[\"objectBestModelName\"]\n",
    "    bestModelColumnName = dictOfModelNames[bestModelName]\n",
    "    bestModelObject = dataSeries[bestModelColumnName]\n",
    "    if(fix == \"Class\"):\n",
    "        target_x = targetProcess\n",
    "    else:\n",
    "        target_x = ConvertBencharkClass_inNPB(targetProblemSize)\n",
    "    predictedTargetData = bestModelObject.predict(target_x).tolist()[0]\n",
    "    dataSeries[\"predictedTargetNumOfFunctionCall\"] = predictedTargetData\n",
    "    dataSeries[\"relativeErrorOfBestModel\"] = returnRelativeErrorRate(\n",
    "        real_data=dataSeries[\"targetNumOfFunctionCall\"], predict_data=predictedTargetData)\n",
    "\n",
    "    return(dataSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b412c-a8ce-4f17-99d3-598f675a1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数noNaNDFのデータに基づいて各関数での予測精度などを保持したDFを返す関数\n",
    "# 引数詳細\n",
    "# benchmark:ベンチマーク名\n",
    "# noNaNDF:生の実験データから NaN が含まれる関数の行を削除したDF\n",
    "# targetNumOfProcess:対象となるプロセス数を格納した変数\n",
    "# targetProblemSize:対象となる問題サイズを格納した変数\n",
    "# fix:\"Process\" or \"Class\"\n",
    "def return_calculatedDF(benchmark: str, noNaNDF, targetNumOfProcess=256, targetProblemSize=\"B\", fix=\"Class\"):\n",
    "\n",
    "    # 取得した実験データから NaN が含まれない関数名のリスト\n",
    "    functionNames = noNaNDF.index.tolist()\n",
    "    # プロセス数のリスト\n",
    "    processes = noNaNDF.columns.tolist()\n",
    "    # 集計するためのDF\n",
    "    calculatedDF = pd.DataFrame(columns=return_numOfColumns())\n",
    "    calculatedDF = calculatedDF.astype(return_numOfColumns(dataType=True))\n",
    "\n",
    "    for functionName in functionNames:\n",
    "        #         print(f\"functionName={functionName}\")\n",
    "        for i in reversed(range(1, len(processes))):\n",
    "\n",
    "            indexSeparator = i+1\n",
    "            raw_y = noNaNDF.loc[functionName].tolist()[:indexSeparator]\n",
    "            raw_x = processes[:indexSeparator]\n",
    "#             print(f\"raw_x={raw_x}, raw_y={raw_y}\")\n",
    "            dataSeries = returnSeriesOfData(benchmarkName=benchmark, functionName=functionName, raw_x=raw_x, raw_y=raw_y,\n",
    "                                            fix=fix, fixed=targetProblemSize, targetProcess=targetNumOfProcess, targetProblemSize=targetProblemSize)\n",
    "            calculatedDF = calculatedDF.append(dataSeries, ignore_index=True)\n",
    "    return(calculatedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93df6f3-ee98-4957-a6bd-e8eaa7e9bc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 集計した各ベンチマークごとにX/Y軸のデータを保持した辞書を返す\n",
    "def returnDictForPlotPerNumOfUsedData(Benchmark=[\"cg\"], fix=\"Class\", benchmarkClass=[\"A\", \"B\", \"C\", \"D\"], FixedProcess=64, Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"C\"):\n",
    "    # 実際に返す辞書\n",
    "    returnDict = {}\n",
    "    # 計算されたDFを保持するための辞書\n",
    "    dictOfCalculatedDF = {}\n",
    "    # プロットするためのデータを集計する\n",
    "    for benchmark in benchmarks:\n",
    "        DF = returnRawDFperBenchmark(Benchmark=benchmark, fix=fix, benchmarkClass=benchmarkClass,\n",
    "                                     FixedProcess=FixedProcess, Processes=Processes, FixedBenchmarkClass=FixedBenchmarkClass)\n",
    "        noNaNDF = DF.dropna(how='any')\n",
    "        dictOfCalculatedDF[benchmark] = return_calculatedDF(\n",
    "            benchmark=benchmark, noNaNDF=noNaNDF, targetNumOfProcess=256, targetProblemSize=FixedBenchmarkClass, fix=fix)\n",
    "#         print(dictOfCalculatedDF[benchmark][[\"benchmarkName\", \"functionName\", \"objectBestModelName\", \"relativeErrorOfBestModel\"]])\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    # ベンチマークごとに集計する\n",
    "    for benchmark in list(dictOfCalculatedDF.keys()):\n",
    "        # numOfData列の要素一覧を作成し、ソートされたリストを、listOfNumDataに格納する\n",
    "        valueCalculatedDF = dictOfCalculatedDF[benchmark]\n",
    "        listOfNumOfData = valueCalculatedDF['numOfData'].tolist()\n",
    "        listOfNumOfData = sorted(list(set(listOfNumOfData)))\n",
    "\n",
    "        # 使用したプロファイル数をキー・最適モデルでの相対誤差の平均をバリューとした辞書を作成する\n",
    "        dictAverageRelativeErrorOfBestModel = {}\n",
    "        x = []\n",
    "        y = []\n",
    "        for numOfData in listOfNumOfData:\n",
    "            # 使用したプロファイル数で抽出\n",
    "            extractedPerNumOfProfileDF = valueCalculatedDF[valueCalculatedDF['numOfData'] == numOfData]\n",
    "            meanDF = extractedPerNumOfProfileDF.mean()\n",
    "            data = meanDF.at['relativeErrorOfBestModel']\n",
    "            dictAverageRelativeErrorOfBestModel[numOfData] = data\n",
    "\n",
    "        # プロットするにあたってX/Y軸に分ける\n",
    "        x = list(dictAverageRelativeErrorOfBestModel.keys())\n",
    "        y = [dictAverageRelativeErrorOfBestModel[key] for key in x]\n",
    "        returnDict[benchmark] = {\"x\": x, \"y\": y}\n",
    "\n",
    "    return(returnDict)\n",
    "\n",
    "\n",
    "# returnDictForPlotPerNumOfUsedData(Benchmark=benchmarks, fix=\"Class\", benchmarkClass=[\n",
    "#                                   \"A\", \"B\", \"C\", \"D\"], FixedProcess=64, Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"C\")\n",
    "\n",
    "# 相対コストを返す関数(返り値の単位は％)\n",
    "# variablesToLearn:予測に用いる説明変数のリスト ex1:[1,2,4,8], ex2:[\"A\", \"B\", \"C\"]\n",
    "# variablesToPreditct:予測したい説明変数のリスト ex1:[256], ex2:[\"D\"]\n",
    "# fixedClassOrProcess:\"Class\"or\"Process\"\n",
    "# fixed:\"C\"\n",
    "def returnRelativeCost(benchmark=\"cg\", variablesToLearn=[1, 2, 4, 8, 16, 32, 64, 128], variablesToPredict=[256], fixedClassOrProcess=\"Class\", fixed=\"C\"):\n",
    "    if fixedClassOrProcess == \"Class\":\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    # 目標となる環境でのコスト\n",
    "    targetCost = 0\n",
    "    for variableToPredict in variablesToPredict:\n",
    "        targetCost += variableToPredict * \\\n",
    "            return_ExecTime(benchmark=benchmark,\n",
    "                            BenchmarkClass=fixed, Process=variableToPredict)\n",
    "    # 予測するのに必要なコスト\n",
    "    predictCost = 0\n",
    "    for variableToLearn in variablesToLearn:\n",
    "        predictCost += variableToLearn * \\\n",
    "            return_ExecTime(benchmark=benchmark,\n",
    "                            BenchmarkClass=fixed, Process=variableToLearn)\n",
    "    return 100 * predictCost / targetCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05b2bf-cc40-4ad5-840f-1e5550631efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "874995e4-1ac1-4f95-8f68-b963a5be6665",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa26d6-dede-4299-9ac0-28ef1cfa6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f664724-52dd-4376-b3ae-61a56cbf6fae",
   "metadata": {},
   "source": [
    "# 修正したモデルから卒論時に集計したデータを作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e052533-fc89-42cd-8dfa-ac9ff832784f",
   "metadata": {},
   "source": [
    "1. 表\n",
    "\n",
    "| ベンチマーク名 | 平均誤差率(%) | コスト比(%) |\n",
    "|---------|----------|---------|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626e6be-a2a8-43f1-8fe9-408826d6da28",
   "metadata": {},
   "source": [
    "2. 表\n",
    "\n",
    "| ベンチマーク名 | 採用割合(最大MAPE(%), 最小MAPE(%)) |\n",
    "|---------|----------------------------|\n",
    "|         | モデル(1), モデル(2), ...        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b800830-c8be-48a4-a4c0-83cbb33fb986",
   "metadata": {},
   "source": [
    "* 平均誤差率：大規模実行時の関数コール回数との比較\n",
    "* MAPE：トレーニングデータとの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720bf22-940b-447c-863f-3421709112e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.xlabel(\"使用したプロファイル数\")\n",
    "plt.ylabel(\"平均誤差率(%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cdf78f-8448-46aa-a005-fd89e15fd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_option(\"display.max_columns\")\n",
    "# pd.get_option(\"display.max_rows\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0858768-1ffc-4fe7-8588-8cee5054e7a5",
   "metadata": {},
   "source": [
    "benchmark = \"cg\"\n",
    "fix = \"Class\"\n",
    "if benchmark == \"bt\" or benchmark == \"sp\":\n",
    "    processes = processes_onlyBTSP\n",
    "else:\n",
    "    processes = processes_excludeBTSP\n",
    "benchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "targetNumOfProcess = 256\n",
    "fixedBenchmarkClass = \"B\"\n",
    "fixedProcess = 64\n",
    "\n",
    "# 引数の条件に合った生の実験データを取得する\n",
    "DF = returnRawDFperBenchmark(Benchmark=benchmark, fix=fix, benchmarkClass=benchmarkClasses,\n",
    "                             FixedProcess=fixedProcess, Processes=processes, FixedBenchmarkClass=fixedBenchmarkClass)\n",
    "# 取得した生の実験データから NaN が含まれる関数の実験データを削除\n",
    "noNaNDF = DF.dropna(how='any')\n",
    "# noNaNDF\n",
    "\n",
    "returnedCalculatedDF = return_calculatedDF(benchmark=benchmark, noNaNDF=noNaNDF,\n",
    "                                           targetNumOfProcess=targetNumOfProcess, targetProblemSize=fixedBenchmarkClass, fix=fix)\n",
    "# returnedCalculatedDF\n",
    "\n",
    "# numOfData列の要素一覧を作成し、ソートされたリストを、listOfNumDataに格納する\n",
    "listOfNumOfData = returnedCalculatedDF['numOfData'].tolist()\n",
    "listOfNumOfData = sorted(list(set(listOfNumOfData)))\n",
    "# listOfNumOfData\n",
    "\n",
    "\n",
    "# 使用したプロファイル数をキー・最適モデルでの相対誤差の平均をバリューとした辞書を作成する\n",
    "dictAverageRelativeErrorOfBestModel = {}\n",
    "x = []\n",
    "y = []\n",
    "for numOfData in listOfNumOfData:\n",
    "    # 使用したプロファイル数で抽出\n",
    "    extractedPerNumOfProfileDF = returnedCalculatedDF[returnedCalculatedDF['numOfData'] == numOfData]\n",
    "    meanDF = extractedPerNumOfProfileDF.mean()\n",
    "    data = meanDF.at['relativeErrorOfBestModel']\n",
    "    dictAverageRelativeErrorOfBestModel[numOfData] = data\n",
    "\n",
    "x = list(dictAverageRelativeErrorOfBestModel.keys())\n",
    "x\n",
    "y = [dictAverageRelativeErrorOfBestModel[key] for key in x]\n",
    "y\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, marker='o')\n",
    "plt.xlabel(\"使用したプロファイル数[％]\")\n",
    "plt.ylabel(\"平均絶対誤差率\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd6550-619a-4a8b-a80a-e2d2c515ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマーク名・関数名・プロセス数・問題サイズを指定することで、その条件での関数コール回数を取得する関数\n",
    "\n",
    "def returnSpecificData(benchmarkName=\"cg\", functionName=\".TAU_application\", process=256, benchmarkClass=\"D\"):\n",
    "    targetRawDF = returnRawDF(Benchmark=benchmarkName, functionName=functionName, benchmarkClass=[\n",
    "                              benchmarkClass], FixedProcess=process, Processes=[process], FixedBenchmarkClass=benchmarkClass)\n",
    "    return targetRawDF.iat[0, 0]\n",
    "# returnSpecificData(benchmarkName=\"mg\", functionName=\"BUBBLE\", process=256, benchmarkClass=\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22c3d8-1ace-401a-b336-a1478ec4ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bt, sp以外のベンチマーク名のリスト\n",
    "benchmarks = [\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"]\n",
    "# pandasのDFをprintした時の幅を広げる\n",
    "pd.set_option('display.width', 100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "271f370a-508c-4b6c-bbf5-35ceb47bbad6",
   "metadata": {},
   "source": [
    "dictTmp = returnDictForPlotPerNumOfUsedData(Benchmark=benchmarks, fix=\"Class\", benchmarkClass=[\n",
    "    \"A\", \"B\", \"C\", \"D\"], FixedProcess=64, Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"C\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d415f89-b279-4a90-9e08-232ab2306d21",
   "metadata": {},
   "source": [
    "pd.options.display.float_format = '{:.4g}'.format\n",
    "\n",
    "tmpDF = pd.DataFrame()\n",
    "for benchmark in benchmarks:\n",
    "    listToLearn = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    listToPredict = [256]\n",
    "    benchmark_x = dictTmp[benchmark][\"x\"]\n",
    "    benchmark_y = dictTmp[benchmark][\"y\"]\n",
    "    index = benchmark_x.index(len(listToLearn))\n",
    "    MAPE = benchmark_y[index]\n",
    "    relativeCost = returnRelativeCost(benchmark=benchmark, variablesToLearn=listToLearn,\n",
    "                                      variablesToPredict=listToPredict, fixedClassOrProcess=\"Class\", fixed=\"C\")\n",
    "    dictRowData = {\"ベンチマーク名\": benchmark.upper(\n",
    "    ), \"平均絶対誤差率[％]\": MAPE, \"相対コスト[％]\": relativeCost}\n",
    "    iDF = pd.DataFrame.from_dict(dictRowData, orient='index').T\n",
    "    tmpDF = tmpDF.append(iDF)\n",
    "tmpDFMean = tmpDF.mean()\n",
    "type(tmpDFMean)\n",
    "print(tmpDF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7842896e-7d01-4a45-8038-401801768eed",
   "metadata": {},
   "source": [
    "# dictTmp\n",
    "\n",
    "plt.figure(figsize=(5.72, 4), dpi=200)\n",
    "for benchmark in list(dictTmp.keys()):\n",
    "    x = dictTmp[benchmark][\"x\"]\n",
    "    y = dictTmp[benchmark][\"y\"]\n",
    "    plt.plot(x, y, marker='o', label=benchmark.upper())\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"使用したプロファイル数\")\n",
    "    plt.ylabel(\"平均絶対誤差率[％]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a8f32-9879-4c91-a5da-7f9190657475",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.72, 4), dpi=200)\n",
    "\n",
    "# Extra-PでfixProcessデータを入力して出力したモデルの図時\n",
    "plot_x = np.linspace(0.8, 256, 500)\n",
    "# -3590464.6990329633 + 3759195.349891038 * p^(1/4)\n",
    "plot_y = []\n",
    "for x in plot_x:\n",
    "    plot_y.append(2286768.3333333326 + 301997.61904761934 * math.log2(x)**(1))\n",
    "plt.plot(plot_x, plot_y, label=\"ExtraP\")\n",
    "\n",
    "x = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "y = [1984770.0, 2263540.0, 2821070.0, 3936140.0,\n",
    "     3936140.0, 3936140.0, 3936140.0, 3936140.0]\n",
    "x = np.array(x).reshape(-1, 1)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "plt.scatter(x, y, marker=\"o\", label=\"予測に用いた関数コール回数\")\n",
    "plot_x = np.array(plot_x).reshape(-1, 1)\n",
    "x_target = [256]\n",
    "y_target = [3936140]\n",
    "plt.scatter(x_target, y_target, marker=\"o\", label=\"予測したい関数コール回数の実測値\")\n",
    "\n",
    "benchmarkName = \"CG\"\n",
    "functionName = \"ICNVRT\"\n",
    "\n",
    "# 線形モデル\n",
    "# 対数モデル\n",
    "\n",
    "# 反比例モデル\n",
    "modelIpMk2 = ModelIp_mk2(train_x=x, train_y=y, target_x=x_target, target_y=y_target,\n",
    "                         benchmark_name=benchmarkName, function_name=functionName)\n",
    "modelIpMk2.calc_lr()\n",
    "plot_y_IpMk2 = modelIpMk2.predict(plot_x)\n",
    "plt.plot(plot_x, plot_y_IpMk2, label=\"反比例モデル\")\n",
    "# 線形飽和モデル\n",
    "modelBranchMk2 = ModelBranch_mk2(train_x=x, train_y=y, target_x=x_target,\n",
    "                                 target_y=y_target, benchmark_name=benchmarkName, function_name=functionName)\n",
    "modelBranchMk2.calc_lr()\n",
    "plot_y_BranchMk2 = modelBranchMk2.predict(plot_x)\n",
    "plt.plot(plot_x, plot_y_BranchMk2, label=\"線形飽和モデル\")\n",
    "# # 線形モデル\n",
    "# model_lin = ModelLin(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_lin.calc_lr()\n",
    "# plot_y_lin = model_lin.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_lin, label=\"線形モデル\")\n",
    "# # 対数モデル\n",
    "# model_log10 = ModelLog10(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_log10.calc_lr()\n",
    "# plot_y_log10 = model_log10.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_log10, label=\"対数モデル\")\n",
    "# # 反比例モデル\n",
    "# model_ip = ModelIP(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_ip.calc_lr()\n",
    "# plot_y_ip = model_ip.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_ip, label=\"反比例モデル\")\n",
    "# # 線形飽和モデル\n",
    "# model_branch = ModelBranch(x, y, \"CG\", \"ICNVRT\", test_ratio=0)\n",
    "# model_branch.calc_lr()\n",
    "# plot_y_branch = model_branch.predict(plot_x)\n",
    "# plt.plot(plot_x, plot_y_branch, label=\"線形飽和モデル\")\n",
    "# 凡例の表示\n",
    "plt.legend()\n",
    "# 軸ラベルの設定\n",
    "plt.ylabel(\"関数コール回数\")\n",
    "plt.xlabel(\"実行コア数\")\n",
    "\n",
    "plt.scatter(x, y, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526a46e-f406-465c-bb76-f1f08d29231d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 実際にプロットする\n",
    "\n",
    "\n",
    "# print(f\"fix={fix}, benchmarkClasses={benchmarkClasses}, fixedProcess={fixedProcess}, Processes={processes}, FixedBenchmarkClass={fixedBenchmarkClass}\")\n",
    "# print(f\"targetNumOfProcess={targetNumOfProcess}, targetProblemSize={fixedBenchmarkClass}, fix={fix}\")\n",
    "\n",
    "# DF = returnRawDFperBenchmark(Benchmark=\"mg\", fix=\"Process\", benchmarkClass=[\"A\", \"B\", \"C\", \"D\"], Processes=[\n",
    "#                              1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchmarkClass=\"B\", FixedProcess=64)\n",
    "# DF.dropna(how='any')\n",
    "# DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3c68a-3b38-4d3a-9e04-385de4834c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b27692-caa1-4d34-8a7d-aed062c4a4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmarkNamesExcludeBTSP = [\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"]\n",
    "# classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "classes = [\"B\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "targetIndex = -1\n",
    "csvDirPath = \"./csv_files/\"\n",
    "\n",
    "dfByDatumExcludeBTSP = returnDFSummarizedData(\n",
    "    benchmarkNames=benchmarkNamesExcludeBTSP, classes=classes, processes=processes, targetIndex=targetIndex, csvDirPath=csvDirPath)\n",
    "# dfByDatumExcludeBTSP\n",
    "\n",
    "dictForLatexTable = {}\n",
    "numOfData = 0\n",
    "for benchmarkName in benchmarkNamesExcludeBTSP:\n",
    "    dictForLatexTable[benchmarkName] = dfByDatumExcludeBTSP[dfByDatumExcludeBTSP[\"benchmarkName\"] == benchmarkName]\n",
    "    numOfData += len(\n",
    "        dfByDatumExcludeBTSP[dfByDatumExcludeBTSP[\"benchmarkName\"] == benchmarkName])\n",
    "\n",
    "numOfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c01f81-5db9-41fd-bb8b-f9bdb4d6371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "listForDF = []\n",
    "\n",
    "for benchmarkName in benchmarkNamesExcludeBTSP:\n",
    "    listForDF.append(returnSeriesOfDatumPerBenchmark(\n",
    "        inputDF=dictForLatexTable[benchmarkName]))\n",
    "DF = pd.DataFrame(listForDF)\n",
    "print(DF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c730e-472d-470f-b0fc-d832c7bda2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_returnSeriesOfDatumPerBenchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ab520-0e46-41db-aa17-edbcc07b9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultIs = dictForLatexTable[\"is\"]\n",
    "# resultIs\n",
    "resultIsAtModelBranch = resultIs[resultIs[\"objectBestModelName\"]\n",
    "                                 == \"ModelBranch\"]\n",
    "datumX = resultIsAtModelBranch[\"usedDataX\"].tolist()\n",
    "datumY = resultIsAtModelBranch[\"usedDataY\"].tolist()\n",
    "\n",
    "datumX\n",
    "datumY\n",
    "\n",
    "# returnSeriesOfData(benchmarkName=\"is\", functionName=\"double_randlc(double_*_double_*)\", rawX=dataX, rawY=dataY, fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6a9eb-910d-43f4-9699-0522cd548619",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultIs = dictForLatexTable[\"ft\"]\n",
    "# resultIs\n",
    "resultIsAtModelBranch = resultIs[resultIs[\"objectBestModelName\"]\n",
    "                                 == \"ModelBranch\"]\n",
    "resultIsAtModelBranchOfNotLowMAPE = resultIsAtModelBranch[\n",
    "    resultIsAtModelBranch[\"MAPEOfBestModel\"] > 1]\n",
    "resultIsAtModelBranchOfNotLowMAPE\n",
    "datumX = resultIsAtModelBranchOfNotLowMAPE[\"usedDataX\"].tolist()\n",
    "datumY = resultIsAtModelBranchOfNotLowMAPE[\"usedDataY\"].tolist()\n",
    "\n",
    "datumX\n",
    "datumY\n",
    "\n",
    "for dataIndex in range(len(datumX)):\n",
    "    plt.figure()\n",
    "    plt.scatter(datumX[dataIndex], datumY[dataIndex])\n",
    "\n",
    "# returnSeriesOfData(benchmarkName=\"is\", functionName=\"double_randlc(double_*_double_*)\", rawX=dataX, rawY=dataY, fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b51fd592-ee43-4aa3-87bc-dd601ddf6fba",
   "metadata": {},
   "source": [
    "# 実験用に小規模なリスト\n",
    "benchmarkNames = [\"cg\"]\n",
    "classes = [\"B\"]\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "975fece9-e11a-4a4e-9555-22fe148104f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 生データの取得\n",
    "cgDF = returnCollectedExistingData(benchmarkNames=[\"cg\"], classes=[\"A\", \"B\", \"C\", \"D\"], processes=[\n",
    "                                   1, 2, 4, 8, 16, 32, 64, 128, 256], csvDirPath=\"./csv_files/\")\n",
    "cgDF\n",
    "# ベンチマーククラスがAの情報を取得\n",
    "cgDFfixedA = cgDF[cgDF[\"benchmarkClass\"] == \"A\"]\n",
    "cgDFfixedA\n",
    "# 関数名のリストを取得\n",
    "functionNames = sorted(list(set(cgDFfixedA[\"functionName\"])))\n",
    "print(functionNames)\n",
    "\n",
    "# 関数名を関数名のリストから抽出\n",
    "functionNameCG = cgDFfixedA[cgDFfixedA[\"functionName\"] == \"CG\"]\n",
    "functionNameCG\n",
    "\n",
    "# 説明変数と目的変数とをリスト化したものを抽出\n",
    "# プロセス数\n",
    "raw_x = functionNameCG['process'].tolist()\n",
    "# 関数コール回数\n",
    "raw_y = functionNameCG['functionCallNum'].tolist()\n",
    "\n",
    "print(f\"raw_x={raw_x}\")\n",
    "print(f\"raw_y={raw_y}\")\n",
    "\n",
    "bencmarkName = \"CG\"\n",
    "functionName = \"CG\"\n",
    "fixProcessOrClass = \"Class\"\n",
    "fixed = \"A\"\n",
    "targetProcess = 256\n",
    "targetBenchmarkClass = fixed\n",
    "targetFunctionCallNum = raw_y[-1]\n",
    "returnSeriesOfData(benchmarkName=\"benhmarkName\", functionName=\"functionName\", rawX=[1, 2, 3], rawY=[\n",
    "                   1, 2, 3], fixProcessOrClass=\"Class\", fixed=\"B\", targetProcess=256, targetBenchmarkClass=\"B\", targetFunctionCallNum=-1, csvDirPath=\"./csv_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9943c-f464-486a-9994-b85f38f89212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb92c6-809b-4e04-92a2-83d10ae9a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブック中で変数のみを記述することでデータフレームをきれいに表示させる設定の有効化\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca733b-96f5-4528-9310-bbcd4810c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040c29e-6015-408d-bd8b-2819138761db",
   "metadata": {},
   "source": [
    "# 問題サイズD, コア数256での関数コール回数を予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3f0f9-f54b-4477-b82f-a97c4a4f83c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO：BT, SP以外のベンチマーク名を入れる\n",
    "benchmarkNames = ['cg', 'ep', 'ft', 'is', 'lu', 'mg']\n",
    "classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "dictForSummarizedResult = {}\n",
    "columnName = [\"benchmarkName\", \"functionName\", \"score\", \"relativeErrorRate\"]\n",
    "dfForSummarizedResult = pd.DataFrame(columns=columnName)\n",
    "for benchmarkName in benchmarkNames:\n",
    "    # ベンチマークごとにscoreを保持するためのリスト\n",
    "    listForSummarizedResultPerBenchmarkName = []\n",
    "    # 学習用生データ\n",
    "    DF = returnCollectedExistingData(benchmarkNames=[\n",
    "                                     benchmarkName], classes=classes, processes=processes, csvDirPath=\"./csv_files/\")\n",
    "    # 重複のない関数名のリスト\n",
    "    functionNames = list(set(DF[\"functionName\"]))\n",
    "    usefulFunctionNames = []\n",
    "    # このループで関数ごとのデータが問題サイズパターン数xコア数パターン数 分だけ存在する関数名のリストを作成する\n",
    "    for functionName in functionNames:\n",
    "        # 関数ごとに生データを集計\n",
    "        dfPerFunction = DF[DF[\"functionName\"] == functionName]\n",
    "        if len(classes) * len(processes) == len(dfPerFunction):\n",
    "            usefulFunctionNames.append(functionName)\n",
    "    if len(usefulFunctionNames) == 0:\n",
    "        continue\n",
    "    # 関数ごとのデータを抽出\n",
    "    for functionName in usefulFunctionNames:\n",
    "        # 問題サイズを数値化したカラムを追加\n",
    "        listBenchmarkClass = DF[\"benchmarkClass\"].tolist()\n",
    "        DFWithNumInBenchmarkClass = DF.assign(\n",
    "            benchmarkClassInNum=convertBenchmarkClasses_problemSizeInNPB(listBenchmarkClass))\n",
    "        # 学習用データ\n",
    "        dfPerFunctionForTrain = DFWithNumInBenchmarkClass[(\n",
    "            DFWithNumInBenchmarkClass[\"functionName\"] == functionName)]\n",
    "        dfPerFunctionForTest = DFWithNumInBenchmarkClass[(DFWithNumInBenchmarkClass[\"functionName\"] == functionName) & (\n",
    "            DFWithNumInBenchmarkClass[\"benchmarkClass\"] == \"D\") & (DFWithNumInBenchmarkClass[\"process\"] == 256)]\n",
    "\n",
    "        # x:説明変数, t:目的変数\n",
    "        trainX = dfPerFunctionForTrain[[\"process\", \"benchmarkClassInNum\"]]\n",
    "        trainT = dfPerFunctionForTrain[[\"functionCallNum\"]]\n",
    "        testX = dfPerFunctionForTest[[\"process\", \"benchmarkClassInNum\"]]\n",
    "        testT = dfPerFunctionForTest[[\"functionCallNum\"]]\n",
    "        # 重回帰分析する\n",
    "        reg_model = LinearRegression()\n",
    "        reg_model.fit(trainX, trainT)\n",
    "        # 関数ごとの結果をベンチマークごとの結果に入れる\n",
    "        scorePerFunction = reg_model.score(trainX, trainT)\n",
    "        listForSummarizedResultPerBenchmarkName.append(scorePerFunction)\n",
    "        # 予測を実施して、相対誤差を算出\n",
    "        predictedTByTestX = reg_model.predict(testX)\n",
    "        predictedData = predictedTByTestX[0][0]\n",
    "        realData = testT[\"functionCallNum\"].tolist()[0]\n",
    "        relativeErrorPerFunction = abs(predictedData - realData)/realData * 100\n",
    "        ##\n",
    "        dfPerFunction = pd.DataFrame(index=columnName, data=[\n",
    "                                     benchmarkName, functionName, scorePerFunction, relativeErrorPerFunction]).T\n",
    "        dfForSummarizedResult = dfForSummarizedResult.append(dfPerFunction)\n",
    "\n",
    "# ( A ~ D ) * (1 ~ 256) のすべての条件を\n",
    "# 満たしていたら、リストに追加\n",
    "# 満たしていなければ、なにもしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4edaf-ab5a-4625-b598-3bd87156b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForSummarizedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9bb2a-e4e3-4e04-89d7-8ba486ebaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = dfForSummarizedResult\n",
    "\n",
    "\n",
    "benchmarkNamesInDF = list(set(dfForSummarizedResult[\"benchmarkName\"].tolist()))\n",
    "\n",
    "listForLatexTable = []\n",
    "for benchmarkName in benchmarkNamesInDF:\n",
    "    print(benchmarkName)\n",
    "    inputDFPerBenchmark = inputDF[inputDF[\"benchmarkName\"] == benchmarkName]\n",
    "    meanData = inputDFPerBenchmark.mean()\n",
    "    print(type(meanData))\n",
    "    meanData[\"benchmarkName\"] = f\"{benchmarkName.upper()}({len(inputDFPerBenchmark)})\"\n",
    "    listForLatexTable.append(meanData)\n",
    "DF = pd.DataFrame(listForLatexTable)\n",
    "\n",
    "DF = DF.sort_index(axis='columns')\n",
    "DF\n",
    "# relativeErrorの単位は[%]ではない。scoreの値はscore()で取得できたもの\n",
    "DF.columns = [\"ベンチマーク名(関数の個数)\", \"MAPE(予測対象関数コール回数に対する)\", \"決定係数\"]\n",
    "print(DF.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc60324-27c4-49a1-ac3d-8dee1c1ca74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forInputDF = returnDFSummarizedData(\n",
    "    benchmarkNames=[\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"],\n",
    "    classes=[\"C\"],\n",
    "    processes=[2, 4, 8, 16, 32, 64, 128, 256],\n",
    "    targetIndex=-1,\n",
    "    csvDirPath=\"./csv_files/\",\n",
    ")\n",
    "\n",
    "benchmarkNames = list(set(forInputDF[\"benchmarkName\"].tolist()))\n",
    "columnsNames = [\"ベンチマーク名(関数の個数)\", \"MAPE(予測対象関数コール回数に対する)\"]\n",
    "listForRelativeErrorTable = []\n",
    "for benchmarkName in benchmarkNames:\n",
    "    forInputDFPerBenchmark = forInputDF[forInputDF[\"benchmarkName\"]\n",
    "                                        == benchmarkName]\n",
    "    column1 = f\"{benchmarkName.upper()}({len(forInputDFPerBenchmark)})\"\n",
    "    seriesOfMean = forInputDFPerBenchmark.mean()\n",
    "    seriesOfMeanRelativeErrorRate = seriesOfMean[\"RelativeErrorRate\"]\n",
    "    column2 = int(seriesOfMeanRelativeErrorRate * 100) / 100\n",
    "    listForRelativeErrorTable.append([{column1}, {column2}])\n",
    "print(pd.DataFrame(listForRelativeErrorTable,\n",
    "      columns=columnsNames).to_latex(index=False))\n",
    "\n",
    "# forInputDFPerBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e42071-d0f9-4faf-b80c-cb69bff95025",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(listForRelativeErrorTable, columns=columnsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d9e07-8d55-4113-9351-807fcc0f9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925ddfc-9e6b-493f-9456-786bc969813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkNames = ['cg', 'ep', 'ft', 'is', 'lu', 'mg']\n",
    "\n",
    "classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "targetClass = classes[-1]\n",
    "targetProcess = processes[-1]\n",
    "\n",
    "# 学習用生データ\n",
    "DF = returnCollectedExistingData(benchmarkNames=benchmarkNames,\n",
    "                                 classes=classes, processes=processes, csvDirPath=\"./csv_files/\")\n",
    "DFByValidFunction = returnDFwithFunctionsExecUnderAllConditions(\n",
    "    inputDF=DF, classes=classes, processes=processes)\n",
    "# 問題サイズを数値化したカラムを追加\n",
    "listBenchmarkClass = DFByValidFunction[\"benchmarkClass\"].tolist()\n",
    "# 生データにカラムなどを加えた整形済みのDF\n",
    "shapedDF = DFByValidFunction.assign(\n",
    "    benchmarkClassInNum=convertBenchmarkClasses_problemSizeInNPB(listBenchmarkClass))\n",
    "\n",
    "# 説明変数のカラム名のリスト\n",
    "expVarColNames = [\"process\", \"benchmarkClassInNum\"]\n",
    "# 目的変数のカラム名のリスト\n",
    "resVarColNames = [\"functionCallNum\"]\n",
    "\n",
    "# データ内にあるベンチマーク名のリスト\n",
    "benchmarkNames = set(shapedDF[\"benchmarkName\"].tolist())\n",
    "\n",
    "# 集計前のデータを作成\n",
    "# DFで[functionName | benchmarkName | expVarDatumDict | resVarDatumDict | modelsName | dictAggregateResult]がカラム名\n",
    "\n",
    "dictToMakeSummary = {}\n",
    "for benchmarkName in benchmarkNames:\n",
    "    DFperBenchmark = shapedDF[shapedDF[\"benchmarkName\"] == benchmarkName]\n",
    "    # すべての条件で実行された関数名のリスト\n",
    "    validFunctionNames = list(\n",
    "        set(DFperBenchmark[\"functionName\"].tolist()))\n",
    "    listToMakeDF = []\n",
    "    print(f\"benchmakName={benchmarkName}, 関数の個数:{len(validFunctionNames)}\")\n",
    "    for validFunctionName in validFunctionNames:\n",
    "        # 3モデルを一気に作成するmodels()を利用\n",
    "        inputDFperFunction = DFperBenchmark[(DFperBenchmark[\"functionName\"] == validFunctionName) & (\n",
    "            DFperBenchmark[\"benchmarkName\"] == benchmarkName)].reset_index()\n",
    "        targetDFperFunction = inputDFperFunction[(inputDFperFunction[\"benchmarkClass\"] == targetClass) & (\n",
    "            inputDFperFunction[\"process\"] == targetProcess)]\n",
    "        # 説明変数のカラム名リストを作成\n",
    "        expVarColNames = [\"process\", \"benchmarkClassInNum\"]\n",
    "        # 目的変数のカラム名リストを作成\n",
    "        resVarColNames = [\"functionCallNum\"]\n",
    "        # モデルを一括で作成\n",
    "        if len(targetDFperFunction) == 0:\n",
    "            print(\n",
    "                f\"benchmarkName={benchmarkName}, functionName={validFunctionName}\")\n",
    "            continue\n",
    "        returnedDF = returnDFtoMakeSummary(inputDF=inputDFperFunction, benchmarkName=benchmarkName, validFunctionName=validFunctionName,\n",
    "                                           targetClass=targetClass, targetProcess=targetProcess, expVarColNames=expVarColNames, resVarColNames=resVarColNames)\n",
    "        listToMakeDF.append(returnedDF)\n",
    "    if len(listToMakeDF) == 0:\n",
    "        continue\n",
    "    inputDFtoMakeDFperBenchmark = pd.concat(\n",
    "        listToMakeDF).reset_index(drop=True)\n",
    "    returnedDict = convertDictToMakeSummary(\n",
    "        inputDF=inputDFtoMakeDFperBenchmark, modelAdoptionRate=True, averageRelativeError=True)\n",
    "    dictToMakeSummary[benchmarkName] = returnedDict\n",
    "# TODO:作成したDFを入力として集計関数を実行\n",
    "\n",
    "modelAdoptionRate = returnedDict[\"modelAdoptionRate\"]\n",
    "averageRelativeError = returnedDict[\"averageRelativeError\"]\n",
    "returnedDict\n",
    "\n",
    "modelAdoptionRate\n",
    "\n",
    "averageRelativeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975a2bc-01ae-4e01-8ebc-8a0cb660e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnBenchmarkName = []\n",
    "# 採用割合\n",
    "columnAdoptionRateLog = []\n",
    "columnAdoptionRateIp = []\n",
    "columnAdoptionRateLin = []\n",
    "# 平均相対誤差率\n",
    "columnAverageRelativeError = []\n",
    "\n",
    "for benchmarkName in dictToMakeSummary.keys():\n",
    "    # 関数の個数\n",
    "    numOfFunctions = 0\n",
    "\n",
    "    # 採用割合\n",
    "    modelAdoptionRate = dictToMakeSummary[benchmarkName][\"modelAdoptionRate\"]\n",
    "    modelNames = list(modelAdoptionRate.keys())\n",
    "    for modelName in modelNames:\n",
    "        #         print(modelName)\n",
    "        numOfFunctions += modelAdoptionRate[modelName][\"count\"]\n",
    "    dictToMakeSummary[benchmarkName][\"関数の個数\"] = numOfFunctions\n",
    "\n",
    "    # 採用割合を算出するモデルが３つであることを仮定している\n",
    "    # modelLog\n",
    "    adoptionRateLog = int(\n",
    "        modelAdoptionRate[\"modelLog\"][\"count\"]/numOfFunctions * 100)\n",
    "    if modelAdoptionRate[\"modelLog\"][\"count\"] == 0:\n",
    "        logMinMAPE = \"-\"\n",
    "        logMaxMAPE = \"-\"\n",
    "    else:\n",
    "        logMinMAPE = int(float(modelAdoptionRate[\"modelLog\"][\"min\"])*10)/10\n",
    "        logMaxMAPE = int(float(modelAdoptionRate[\"modelLog\"][\"max\"])*10)/10\n",
    "    # modelIp\n",
    "    adoptionRateIp = int(\n",
    "        modelAdoptionRate[\"modelIp\"][\"count\"]/numOfFunctions * 100)\n",
    "    if modelAdoptionRate[\"modelIp\"][\"count\"] == 0:\n",
    "        ipMinMAPE = \"-\"\n",
    "        ipMaxMAPE = \"-\"\n",
    "    else:\n",
    "        ipMinMAPE = int(float(modelAdoptionRate[\"modelIp\"][\"min\"])*10)/10\n",
    "        ipMaxMAPE = int(float(modelAdoptionRate[\"modelIp\"][\"max\"])*10)/10\n",
    "    # modelLin\n",
    "    adoptionRateLin = 100 - adoptionRateIp - adoptionRateLog\n",
    "    if modelAdoptionRate[\"modelLin\"][\"count\"] == 0:\n",
    "        linMinMAPE = \"-\"\n",
    "        linMaxMAPE = \"-\"\n",
    "    else:\n",
    "        linMinMAPE = int(float(modelAdoptionRate[\"modelLin\"][\"min\"])*10)/10\n",
    "        linMaxMAPE = int(float(modelAdoptionRate[\"modelLin\"][\"max\"])*10)/10\n",
    "\n",
    "    # Latex化するためにカラムとして入れる\n",
    "    columnBenchmarkName.append(f\"{benchmarkName.upper()}({numOfFunctions})\")\n",
    "    columnAdoptionRateLog.append(\n",
    "        f\"{adoptionRateLog}({logMinMAPE},{logMaxMAPE})\")\n",
    "    columnAdoptionRateIp.append(f\"{adoptionRateIp}({ipMinMAPE},{ipMaxMAPE})\")\n",
    "    columnAdoptionRateLin.append(\n",
    "        f\"{adoptionRateLin}({linMinMAPE},{linMaxMAPE})\")\n",
    "\n",
    "    # 相対誤差率\n",
    "    averageRelativeError = dictToMakeSummary[benchmarkName][\"averageRelativeError\"]\n",
    "    columnAverageRelativeError.append(int(averageRelativeError*100)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172913c-150e-4a39-afa4-ff9925884c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictToMakeSummary\n",
    "\n",
    "columnBenchmarkName\n",
    "\n",
    "columnAdoptionRateLog\n",
    "\n",
    "columnAdoptionRateIp\n",
    "\n",
    "columnAdoptionRateLin\n",
    "\n",
    "columnAverageRelativeError\n",
    "\n",
    "# 採用割合\n",
    "採用割合 = pd.DataFrame({\"ベンチマーク名\": columnBenchmarkName,\n",
    "                    \"反比例モデル\": columnAdoptionRateIp, \"対数モデル\": columnAdoptionRateLog, \"線形モデル\": columnAdoptionRateLin})\n",
    "# 相対誤差率\n",
    "相対誤差率 = pd.DataFrame({\"ベンチマーク名\": columnBenchmarkName,\n",
    "                     \"相対誤差率[%]\": columnAverageRelativeError})\n",
    "\n",
    "print(採用割合.to_latex(index=False))\n",
    "\n",
    "print(相対誤差率.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda7787-067e-4bdd-b0b6-79d7baeb2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb\n",
    "\n",
    "class ModelBranchForMultipleRegression(ModelBaseForMultipleRegression):\n",
    "    # 線形モデル（重回帰分析）\n",
    "\n",
    "    def transformDataForModel(self, inputDF):\n",
    "        # inputDFで与えられたデータをモデルに適した形に変形する\n",
    "        return(inputDF)\n",
    "\n",
    "    def setUpDataBeforeCalcLr(self):\n",
    "        # 説明変数・目的変数を変換する関数\n",
    "        # 分岐を実際に実行するかを判断するためのフラグ\n",
    "        enableBranch = False\n",
    "        # enableBranch == False ---> 分岐しない\n",
    "        # enableBranch == True  ---> 分岐する\n",
    "        \n",
    "        # TODO:分岐点の探索\n",
    "        \n",
    "        # enableBranchの値に応じて分岐するかどうか\n",
    "        \n",
    "        # モデル構築用データ\n",
    "        self.dataXForPredict = self.transformDataForModel(\n",
    "            self.rawExplanaoryVariable)\n",
    "        self.dataTForPredict = self.transformDataForModel(\n",
    "            self.rawResponseVariable)\n",
    "        # テスト用データ\n",
    "        self.dataXForTest = self.transformDataForModel(\n",
    "            self.rawExplanaoryVariableForTest)\n",
    "        self.dataTForTest = self.transformDataForModel(\n",
    "            self.rawResponseVariableForTest)\n",
    "\n",
    "    def calcLr(self):\n",
    "        # 実際にモデルを構築する\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.dataXForPredict, self.dataTForPredict)\n",
    "\n",
    "    def predict(self, inputDF):\n",
    "        # inputDFのデータから構築されたモデルを使って予測を行う\n",
    "\n",
    "        # inputDFから説明変数データのみを取得\n",
    "        inputDFOnlyExplanatoryVariableColumn = inputDF[self.explanatoryVariableColumnNames]\n",
    "        # 予測を実行\n",
    "        result = self.lr.predict(inputDFOnlyExplanatoryVariableColumn)\n",
    "\n",
    "        return(result)\n",
    "\n",
    "\n",
    "def test_ModelBranchForMultipleRegression():\n",
    "    # 単なる線形モデル\n",
    "    # 説明変数\n",
    "    plotX = np.linspace(0, 20, 10)\n",
    "    plotY = np.linspace(20, 40, 10)\n",
    "    plotZ = np.linspace(40, 60, 10)\n",
    "    # 目的変数\n",
    "    plotT = plotX + 2 * plotY + 3 * plotZ + 4\n",
    "\n",
    "    # DFを作成する\n",
    "    # カラム名のリスト\n",
    "    columnNames = [\"plotX\", \"plotY\", \"plotZ\", \"plotT\"]\n",
    "    datumForDF = [plotX, plotY, plotZ, plotT]\n",
    "    inputDFForTest = pd.DataFrame(index=columnNames, data=datumForDF).T\n",
    "    inputDFForTest[\"functionName\"] = \"functionName\"\n",
    "\n",
    "    # 目的変数・説明変数のカラム名のリスト\n",
    "    # 目的変数のカラム名のリスト\n",
    "    columnNamesForExp = columnNames[:-1]\n",
    "    # 説明変数のカラム名のリスト\n",
    "    columnNamesForRes = columnNames[-1:]\n",
    "\n",
    "    # 予測をする\n",
    "    # モデルオブジェクトの作成\n",
    "    objectModel = ModelBranchForMultipleRegression(inputDF=inputDFForTest, explanatoryVariableColumnNames=columnNamesForExp,\n",
    "                                                   responseVariableColumnNames=columnNamesForRes, conditionDictForTest={})\n",
    "    # モデルの生成の準備\n",
    "    objectModel.setUpDataBeforeCalcLr()\n",
    "    # モデルの生成\n",
    "    objectModel.calcLr()\n",
    "    # モデルによる予測\n",
    "    # 入力データDFを作成\n",
    "    inputDFForPredict = pd.DataFrame(inputDFForTest.tail(1))\n",
    "    predictedNum = objectModel.predict(inputDFForPredict)\n",
    "\n",
    "    # 相対誤差率でテスト対象のデータが想定通りに動作しているかを判断する\n",
    "    # 相対誤差率を計算するために実データを取得する\n",
    "    realNum = plotT[-1]\n",
    "    relativeErrorRate = returnRelativeErrorRate(\n",
    "        realNum=realNum, predictedNum=predictedNum)\n",
    "\n",
    "    assert relativeErrorRate < 1\n",
    "\n",
    "    # 線形飽和モデル\n",
    "    branchIndex = 5\n",
    "    # 説明変数\n",
    "    branchX = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    branchY = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "    branchZ = [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "    # 目的変数\n",
    "    branchT_X = returnListForBranchModel(\n",
    "        inputList=branchX, branchIndex=branchIndex, a=1, b=2)\n",
    "    branchT_Y = returnListForBranchModel(\n",
    "        inputList=branchY, branchIndex=branchIndex, a=3, b=4)\n",
    "    branchT_Z = returnListForBranchModel(\n",
    "        inputList=branchZ, branchIndex=branchIndex, a=5, b=6)\n",
    "    branchT = []\n",
    "    for numX, numY, numZ in zip(branchT_X, branchT_Y, branchT_Z):\n",
    "        branchT.append(numX+numY+numZ)\n",
    "    # DFを作成する\n",
    "    # カラム名のリスト\n",
    "    inputDFForTest = pd.DataFrame(\n",
    "        {\"branchX\": branchX, \"branchY\": branchY, \"branchZ\": branchZ, \"branchT\": branchT})\n",
    "    # 目的変数のカラム名のリスト\n",
    "    expVarName = [\"branchX\", \"branchY\", \"branchZ\"]\n",
    "    # 説明変数のカラム名のリスト\n",
    "    resVarName = [\"branchT\"]\n",
    "    # 関数名\n",
    "    inputDFForTest[\"functionName\"] = \"functionName\"\n",
    "    # 予測のためのモデルオブジェクトの作成\n",
    "    objectModel = ModelBranchForMultipleRegression(\n",
    "        inputDF=inputDFForTest, explanatoryVariableColumnNames=expVarName, responseVariableColumnNames=resVarName, conditionDictForTest={})\n",
    "    # モデルの生成の準備\n",
    "    objectModel.setUpDataBeforeCalcLr()\n",
    "    objectModel.calcLr()\n",
    "    # モデルによる予測\n",
    "    inputDFForPredict = pd.DataFrame(inputDFForTest.tail(1))\n",
    "    predictedNum = objectModel.predict(inputDFForPredict)\n",
    "    # 相対誤差率でテスト対象のデータに焚いてモデルが想定通りに動作しているかを判断する\n",
    "    realNum = branchT[-1]\n",
    "    relativeErrorRate = returnRelativeErrorRate(\n",
    "        realNum=realNum, predictedNum=predictedNum)\n",
    "    print(f\"relatieErrorRate={relativeErrorRate}\")\n",
    "    assert relativeErrorRate < 1\n",
    "\n",
    "\n",
    "test_ModelBranchForMultipleRegression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
