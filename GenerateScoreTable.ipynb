{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def return_dict_summary_fixed(benchmark_name=\"cg\", fixed=\"class\"):\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_ = return_fixed_class(BenchMark=benchmark_name)\n",
    "    else:\n",
    "        fixed_ = return_fixed_process(BenchMark=benchmark_name)\n",
    "    summary_fixed_ = pd.concat(fixed_ , axis=1)\n",
    "    columns = summary_fixed_.columns.to_numpy()\n",
    "    index = summary_fixed_.index.to_numpy()\n",
    "    if(fixed == \"class\"):\n",
    "        dict_summary_fixed_ = {\"processes\":columns}\n",
    "    else:\n",
    "        dict_summary_fixed_ = {\"class\":columns}\n",
    "    for index_name in index:\n",
    "        dict_summary_fixed_[index_name] = summary_fixed_.T[index_name].to_numpy()\n",
    "    return dict_summary_fixed_\n",
    "\n",
    "def return_non_NaN_list(target_list):\n",
    "    for i in range(len(target_list)):\n",
    "        if (math.isnan(target_list[i])):\n",
    "            target_list[i] = 0\n",
    "    return target_list\n",
    "\n",
    "def does_include_nan(target_list):\n",
    "    for i in range(len(target_list)):\n",
    "        if(math.isnan(target_list[i])):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateScoreTable(benchmark_name=\"cg\"):\n",
    "    list_ScoreTable = []\n",
    "    dict_summary_fixed_class = return_dict_summary_fixed(benchmark_name=benchmark_name, fixed=\"class\")\n",
    "    raw_x = dict_summary_fixed_class[\"processes\"]\n",
    "#     print(f\"raw_x : {raw_x}, benchmark : {benchmark_name}\")\n",
    "    for content in dict_summary_fixed_class:\n",
    "        if(content == \"processes\"):\n",
    "            continue\n",
    "        raw_y = dict_summary_fixed_class[content]\n",
    "        if(does_include_nan(raw_y)):\n",
    "            continue\n",
    "#         print(f\"raw_y(={content}) : {raw_y}\")\n",
    "        # 線形モデル\n",
    "        model_lin = ModelLin(raw_x, raw_y, benchmark_name, content)\n",
    "        model_lin.calc_lr()\n",
    "        model_lin.calc_r2_score()\n",
    "#         model_lin.plot_graph()\n",
    "        model_lin.calc_mae_score()\n",
    "        model_lin.calc_mse_score()\n",
    "        model_lin.calc_rmse_score()\n",
    "        model_lin.calc_mape_score()\n",
    "        # logモデル\n",
    "        model_log10 = ModelLog10(raw_x, raw_y, benchmark_name, content)\n",
    "        model_log10.calc_lr()\n",
    "        model_log10.calc_r2_score()\n",
    "#         model_log10.plot_graph()\n",
    "        model_log10.calc_mae_score()\n",
    "        model_log10.calc_mse_score()\n",
    "        model_log10.calc_rmse_score()\n",
    "        model_log10.calc_mape_score()\n",
    "        # 反比例モデル\n",
    "        model_ip = ModelIP(raw_x, raw_y, benchmark_name, content)\n",
    "        model_ip.calc_lr()\n",
    "        model_ip.calc_r2_score()\n",
    "#         model_ip.plot_graph()\n",
    "        model_ip.calc_mae_score()\n",
    "        model_ip.calc_mse_score()\n",
    "        model_ip.calc_rmse_score()\n",
    "        model_ip.calc_mape_score()\n",
    "        \n",
    "        list_ScoreTable.append([content, model_lin.mape_score, model_log10.mape_score, model_ip.mape_score])\n",
    "    df_ScoreTable = pd.DataFrame(list_ScoreTable)\n",
    "    df_ScoreTable.columns = [\"\", \"x mape\", \"logx mape\", \"1/x mape\"]\n",
    "    df_ScoreTable.set_index(\"\",inplace=True)\n",
    "    df_ScoreTable.to_csv(\"./tmp_GenerateScoreTable/\"+benchmark_name+\".csv\")\n",
    "\n",
    "generateScoreTable(\"bt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for benchmark in benchmarks:\n",
    "    # 以下の「benchmark == \"XXX\"」はNaNがデータとして存在するものを飛ばすために存在\n",
    "    if(benchmark == \"bt\"):\n",
    "        continue\n",
    "    elif(benchmark == \"ft\"):\n",
    "        continue\n",
    "#     elif(benchmark == \"is\"):\n",
    "#         continue\n",
    "#     elif(benchmark == \"mg\"):\n",
    "#         continue\n",
    "#     elif(benchmark == \"sp\"):\n",
    "#         continue\n",
    "#     elif(benchmark == \"ft\"):\n",
    "#         continue\n",
    "    generateScoreTable(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
