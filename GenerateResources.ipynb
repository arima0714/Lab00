{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 発表資料などの資料作成に必要な素材を生成するためのノート\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summarized_ = return_summarized_Fixed_dataframe()\n",
    "# print(summarized_)\n",
    "# indices = summarized_.index.values\n",
    "# columns = summarized_.columns.values\n",
    "# print(indices)\n",
    "# print(columns)\n",
    "\n",
    "# plt.figure()\n",
    "# for row in indices:\n",
    "#     print(summarized_.loc[row].to_numpy())\n",
    "#     plt.plot(columns, summarized_.loc[row].to_numpy())\n",
    "\n",
    "# return_summarized_Fixed_dataframe(BenchMark_name=\"cg\").to_csv(\"./tmp_GenerateResources/SummarizedFixedDataframe_cg.csv\")\n",
    "\n",
    "# path = \"./tmp_GenerateResources/\"\n",
    "\n",
    "# def generateScoreTable(benchmark_name=\"cg\"):\n",
    "#     list_ScoreTable = []\n",
    "#     dict_summary_fixed_class = return_dict_summary_fixed(benchmark_name=benchmark_name, fixed=\"class\")\n",
    "#     raw_x = dict_summary_fixed_class[\"processes\"]\n",
    "# #     print(f\"raw_x : {raw_x}, benchmark : {benchmark_name}\")\n",
    "#     for content in dict_summary_fixed_class:\n",
    "#         if(content == \"processes\"):\n",
    "#             continue\n",
    "#         raw_y = dict_summary_fixed_class[content]\n",
    "#         if(does_include_nan(raw_y)):\n",
    "#             continue\n",
    "# #         print(f\"raw_y(={content}) : {raw_y}\")\n",
    "#         # 線形モデル\n",
    "#         model_lin = ModelLin(raw_x, raw_y, benchmark_name, content)\n",
    "#         model_lin.calc_lr()\n",
    "#         model_lin.calc_r2_score()\n",
    "#         model_lin.calc_mae_score()\n",
    "#         model_lin.calc_mse_score()\n",
    "#         model_lin.calc_rmse_score()\n",
    "#         model_lin.calc_mape_score()\n",
    "#         model_lin.plot_graph()\n",
    "#         plt.title(f\"ベンチマーク名：{model_lin.benchmark_name}, 関数名：{model_lin.function_name}, MAPE : {model_lin.mape_score}\", y=-0.2)\n",
    "#         plt.show()\n",
    "#         # logモデル\n",
    "#         model_log10 = ModelLog10(raw_x, raw_y, benchmark_name, content)\n",
    "#         model_log10.calc_lr()\n",
    "#         model_log10.calc_r2_score()\n",
    "#         model_log10.calc_mae_score()\n",
    "#         model_log10.calc_mse_score()\n",
    "#         model_log10.calc_rmse_score()\n",
    "#         model_log10.calc_mape_score()\n",
    "#         model_log10.plot_graph()\n",
    "#         if(model_log10.benchmark_name == \"cg\" and model_log10.function_name == \"MPI_Irecv()\"):\n",
    "#             plt.savefig(path+model_log10.benchmark_name+'_'+model_log10.function_name+'.pdf')\n",
    "#         if(model_log10.benchmark_name == \"cg\" and model_log10.function_name == \"ICNVRT\"):\n",
    "#             plt.savefig(path+model_log10.benchmark_name+'_'+model_log10.function_name+'.pdf')\n",
    "#         plt.title(f\"ベンチマーク名：{model_log10.benchmark_name}, 関数名：{model_log10.function_name}, MAPE : {model_log10.mape_score}\", y=-0.2)\n",
    "#         plt.show()\n",
    "#         # 反比例モデル\n",
    "#         model_ip = ModelIP(raw_x, raw_y, benchmark_name, content)\n",
    "#         model_ip.calc_lr()\n",
    "#         model_ip.calc_r2_score()\n",
    "#         model_ip.calc_mae_score()\n",
    "#         model_ip.calc_mse_score()\n",
    "#         model_ip.calc_rmse_score()\n",
    "#         model_ip.calc_mape_score()\n",
    "#         model_ip.plot_graph()\n",
    "#         if(model_log10.benchmark_name == \"lu\" and model_log10.function_name == \"EXACT\"):\n",
    "#             plt.savefig(path+model_log10.benchmark_name+'_'+model_log10.function_name+'.pdf')\n",
    "#         plt.title(f\"ベンチマーク名：{model_ip.benchmark_name}, 関数名：{model_ip.function_name}, MAPE : {model_ip.mape_score}\", y=-0.2)\n",
    "#         plt.show()\n",
    "#         list_ScoreTable.append([content, model_lin.mape_score, model_log10.mape_score, model_ip.mape_score])\n",
    "#     df_ScoreTable = pd.DataFrame(list_ScoreTable)\n",
    "#     df_ScoreTable.columns = [\"\", \"x mape\", \"logx mape\", \"1/x mape\"]\n",
    "#     df_ScoreTable.set_index(\"\",inplace=True)\n",
    "#     df_ScoreTable\n",
    "\n",
    "# for benchmark in benchmarks:\n",
    "#     generateScoreTable(benchmark)\n",
    "\n",
    "# # 資料作成に使用する最もフィットするモデルはどれかを示すための表・グラフを作るためのプログラム\n",
    "\n",
    "\n",
    "# def return_list_of_ratio_row(input_list):\n",
    "#     sum_of_input_list = 0\n",
    "#     return_list = []\n",
    "#     for list_child in input_list:\n",
    "#         sum_of_input_list += len(list_child)\n",
    "#     for list_child in input_list:\n",
    "#         num = int(len(list_child)/sum_of_input_list*10000)/100\n",
    "#         return_list.append(num)\n",
    "#     # 総和を100にする処理\n",
    "#     # 最大になることの多い線形モデルはほかのモデルの割合の総和を100から引いたものにしている\n",
    "#     i_0 = 100\n",
    "#     for i in range(1, len(input_list)):\n",
    "#         i_0 -= return_list[i]\n",
    "#     return_list[0] = i_0\n",
    "#     return(return_list)\n",
    "\n",
    "# def return_list_of_range_row(input_list):\n",
    "#     return_list = []\n",
    "#     for list_child in input_list:\n",
    "#         if(len(list_child) == 0):\n",
    "#             data_str = \"(NoData)\"\n",
    "#         else:\n",
    "#             min_data = int(min(list_child)*100)/100\n",
    "#             max_data = int(max(list_child)*100)/100\n",
    "#             data_str = f\"({min_data}-{max_data})\"\n",
    "#         return_list.append(data_str)\n",
    "#     return(return_list)\n",
    "\n",
    "\n",
    "# def return_row_list(input_list):\n",
    "#     return_list = []\n",
    "#     # 割合の入ったリストと最小・最大値の入ったリスト\n",
    "#     list_of_ratio_row = return_list_of_ratio_row(input_list)\n",
    "#     list_of_range_row = return_list_of_range_row(input_list)\n",
    "    \n",
    "#     # 上の二つのリストの要素同士を結合させる\n",
    "#     for i in range(len(list_of_ratio_row)):\n",
    "#         return_list.append(f\"{list_of_ratio_row[i]}%{list_of_range_row[i]}\")\n",
    "    \n",
    "#     return return_list\n",
    "\n",
    "# csv_directory_path = './tmp_GenerateResources/'\n",
    "\n",
    "# result_of_all = [[], [], []]\n",
    "# list_for_csv = []\n",
    "\n",
    "# list_for_csv_ratio = []\n",
    "# list_for_csv_range = []\n",
    "\n",
    "# for benchmark in benchmarks:\n",
    "#     result_of_benchmark = [[], [], []]\n",
    "#     file_name = benchmark+'.csv'\n",
    "#     file_path = csv_directory_path+file_name\n",
    "#     # ファイルが存在しない場合は処理を飛ばす\n",
    "#     if(os.path.isfile(csv_directory_path+file_name) == False):\n",
    "#         continue\n",
    "#     # 完全に値をとれたもののみで集計するようにしている\n",
    "#     if(benchmark != \"cg\" and benchmark != \"ep\" and benchmark != \"lu\"):\n",
    "#         continue\n",
    "#     # 現在処理中のベンチマーク名を出力\n",
    "#     print(benchmark)\n",
    "#     df = pd.read_csv(file_path, index_col=0)\n",
    "#     columns = df.columns.values\n",
    "#     indices = df.index.values\n",
    "#     for index in indices:\n",
    "#         row = df.loc[index].tolist()\n",
    "#         result_of_benchmark[row.index(min(row))].append(min(row))\n",
    "#         result_of_all[row.index(min(row))].append(min(row))\n",
    "#     list_for_csv.append([benchmark]+return_row_list(result_of_benchmark))\n",
    "# list_for_csv.append([\"all\"]+return_row_list(result_of_all))\n",
    "\n",
    "# print(list_for_csv)\n",
    "\n",
    "# csv_head = [\"\", \"線形モデル\", \"対数モデル\", \"反比例モデル\"]\n",
    "# df_for_csv = pd.DataFrame(list_for_csv)\n",
    "# df_for_csv.columns = csv_head\n",
    "# df_for_csv.set_index(\"\")\n",
    "# df_for_csv\n",
    "\n",
    "# # ipynb形式のライブラリのインポート\n",
    "# %run ./lib.ipynb\n",
    "\n",
    "# for benchmark in benchmarks:\n",
    "#     SummarizedDF= return_summarized_Fixed_dataframe(BenchMark_name =benchmark, fixed=\"class\")\n",
    "#     SummarizedDFIndex = SummarizedDF.index.tolist()\n",
    "#     SummarizedDFColumns = SummarizedDF.columns.tolist()\n",
    "#     x_list = SummarizedDFColumns\n",
    "#     targetFunctions=[\"RHS\", \"CFFTZ\", \"ICNVRT\", \"BUBBLE\"]\n",
    "#     for FunctionNames in SummarizedDFIndex:\n",
    "#         y_list = SummarizedDF.loc[FunctionNames].tolist()\n",
    "#         if(FunctionNames in targetFunctions and (does_include_nan(y_list))==False):\n",
    "#             print(f\"{FunctionNames}@{benchmark}\")\n",
    "#             plt.figure()\n",
    "#             plt.title(FunctionNames)\n",
    "#             plt.plot(x_list,y_list)\n",
    "#             plt.xlabel('プロセス数')\n",
    "#             plt.ylabel('コール回数')\n",
    "#             plt.show()\n",
    "#             if(FunctionNames==\"RHS\"):\n",
    "#                 modelLin = ModelLin(raw_x=x_list, raw_y=y_list, benchmark_name=benchmark, function_name=FunctionNames)\n",
    "#                 modelLin.train_x = modelLin.raw_x\n",
    "#                 modelLin.train_y = modelLin.raw_y\n",
    "#                 modelLin.calc_lr()\n",
    "#                 modelLin.plot_graph(save=True, fileName=f\"./tmp_GenerateResources/{FunctionNames}@{benchmark}.pdf\")\n",
    "#             elif(FunctionNames==\"BUBBLE\"):\n",
    "#                 modelLog10 = ModelLog10(raw_x=x_list, raw_y=y_list, benchmark_name=benchmark, function_name=FunctionNames)\n",
    "#                 modelLog10.train_x = modelLog10.raw_x\n",
    "#                 modelLog10.train_y = modelLog10.raw_y\n",
    "#                 modelLog10.calc_lr()\n",
    "#                 modelLog10.plot_graph(save=True, fileName=f\"./tmp_GenerateResources/{FunctionNames}@{benchmark}.pdf\")\n",
    "#             elif(FunctionNames==\"CFFTZ\"):\n",
    "#                 modelIP = ModelIP(raw_x=x_list, raw_y=y_list, benchmark_name=benchmark, function_name=FunctionNames)\n",
    "#                 modelIP.train_x = modelIP.raw_x\n",
    "#                 modelIP.train_y = modelIP.raw_y\n",
    "#                 modelIP.calc_lr()\n",
    "#                 modelIP.plot_graph(save=True, fileName=f\"./tmp_GenerateResources/{FunctionNames}@{benchmark}.pdf\")\n",
    "#             elif(FunctionNames==\"ICNVRT\"):\n",
    "#                 modelBranch = ModelBranch(raw_x=x_list, raw_y=y_list, benchmark_name=benchmark, function_name=FunctionNames)\n",
    "#                 modelBranch.train_x = modelBranch.raw_x\n",
    "#                 modelBranch.train_y = modelBranch.raw_y\n",
    "#                 modelBranch.calc_lr()\n",
    "#                 modelBranch.plot_graph(save=True, fileName=f\"./tmp_GenerateResources/{FunctionNames}@{benchmark}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 論文で必要なグラフを生成する1\n",
    "print(\"↓プロセス数を固定したもの\")\n",
    "GenGraphAveragePerProfileNum(benchmarks=benchmarks, Fixed=\"Process\", Fix=64, Predict=\"D\", EnableTitle=False, SaveGraph=True)\n",
    "print(\"↓ベンチマーククラスを固定したもの\")\n",
    "GenGraphAveragePerProfileNum(benchmarks=benchmarks, Fixed=\"Class\", Fix=\"B\", Predict=256, EnableTitle=False, SaveGraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 論文で必要なグラフを生成する2\n",
    "for benchmark in benchmarks:\n",
    "    print(benchmark)\n",
    "    print(\"↓プロセス数を固定したもの\")\n",
    "    GenGraphTotalTimePerProfileNum(benchmark=benchmark, Fixed=\"Process\", Fix=64, Predict=\"D\", EnableTitle=False, SaveGraph=True)\n",
    "    print(\"↓ベンチマーククラスを固定したもの\")\n",
    "    GenGraphTotalTimePerProfileNum(benchmark=benchmark, Fixed=\"Class\", Fix=\"B\", Predict=256, EnableTitle=False, SaveGraph=True)\n",
    "    \n",
    "\n",
    "# # 論文で必要なグラフを生成する3\n",
    "# benchmarks01 = ['cg', 'ep', 'mg']\n",
    "# benchmarks02 = ['ft', 'lu']\n",
    "# benchmarks03 = [\"bt\", \"sp\"]\n",
    "# benchmarks04 = ['is']\n",
    "# print(f\"benchmarks01 = {benchmarks01}, benchmarks02 = {benchmarks02}, benchmarks03 = {benchmarks03}\")\n",
    "# print(f\"↓ベンチマーククラスを固定したもの@{benchmarks01}\")\n",
    "# GenGraphAveragePerProfileNum(benchmarks=benchmarks01, Fixed=\"Class\", Fix=\"B\", Predict=256, EnableTitle=True)\n",
    "# print(f\"↓ベンチマーククラスを固定したもの@{benchmarks02}\")\n",
    "# GenGraphAveragePerProfileNum(benchmarks=benchmarks02, Fixed=\"Class\", Fix=\"B\", Predict=256, EnableTitle=True)\n",
    "# print(f\"↓ベンチマーククラスを固定したもの@{benchmarks03}\")\n",
    "# GenGraphAveragePerProfileNum(benchmarks=benchmarks03, Fixed=\"Class\", Fix=\"B\", Predict=256, EnableTitle=True)\n",
    "# print(f\"↓ベンチマーククラスを固定したもの@{benchmarks04}\")\n",
    "# GenGraphAveragePerProfileNum(benchmarks=benchmarks04, Fixed=\"Class\", Fix=\"B\", Predict=256, EnableTitle=True)\n",
    "\n",
    "# # 論文で必要なグラフを生成する4\n",
    "# print(f\"↓ベンチマーククラスを固定したもの@{benchmarks04}\")\n",
    "# for benchmark in benchmarks:\n",
    "#     GenGraphAveragePerProfileNum(benchmarks=[benchmark], Fixed=\"Class\", Fix=\"B\", Predict=256, EnableTitle=False, EnableScatter=True, SaveGraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### mgの挙動を確認するためのセル\n",
    "\n",
    "# benchmark = \"mg\"\n",
    "# FixClass = \"B\"\n",
    "# predict_process = 256\n",
    "# DF = return_ErrorRateFixedClassDF(benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "# DF\n",
    "\n",
    "# lists = return_fixed_class(BenchMark=\"mg\", Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchMarkClass=\"B\")\n",
    "# DF = pd.concat(lists, axis=1)\n",
    "# index = DF.index.tolist()\n",
    "# for i in index:\n",
    "#     data = DF.loc[i].tolist()\n",
    "#     if(does_include_nan(data)):\n",
    "#         pass\n",
    "#     else:\n",
    "#         print(f\"{i}:{data}\")\n",
    "#         plt.figure()\n",
    "#         plt.plot([1, 2, 4, 8, 16, 32, 64, 128, 256], data)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # benchmark = \"cg\"\n",
    "# # benchmarks = [benchmark]\n",
    "# Fix = \"B\"\n",
    "# Predict = 256\n",
    "# Fixed = \"Class\"\n",
    "\n",
    "# def PlotRateOfCosts(benchmark = [\"cg\"], Fix=\"B\", Predict=256, Fixed=\"Class\"):\n",
    "#     plt.figure()\n",
    "#     for benchmark in benchmarks :\n",
    "#         if(Fixed == \"Process\"):\n",
    "#             DF = return_ErrorRateFixedProcess_AveragePredictCostRealCost(benchmark=benchmark, FixProcess=Fix, predict_class=Predict)\n",
    "#         else:\n",
    "#             DF = return_ErrorRateFixedClass_AveragePredictCostRealCost(benchmark=benchmark, FixClass=Fix, predict_process=Predict)\n",
    "#         ConvertedDF = ConvertIndexNameToNumOfProfile(DF, Fixed=Fixed)\n",
    "# #         print(DF)\n",
    "#         x = ConvertedDF.index.tolist()\n",
    "#         RealCost = DF[\"RealCost\"].tolist()[0]\n",
    "#         PredictCosts = DF[\"PredictCosts\"].tolist()\n",
    "#         y = []\n",
    "#         for PredictCost in PredictCosts:\n",
    "#             y.append(PredictCost/RealCost)\n",
    "#         plt.plot(x, y)\n",
    "        \n",
    "#         plt.plot(x, [1]*len(x), label=benchmark)\n",
    "#     plt.legend()\n",
    "#     plt.xlabel(\"使用したプロファイル数\")\n",
    "#     plt.ylabel(\"予測にかかる総計算時間を実測にかかる総計算時間で割った値\")\n",
    "#     plt.show()\n",
    "    \n",
    "# PlotRateOfCosts(benchmark=benchmarks, Fix=64, Predict=\"D\", Fixed=\"Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 論文で必要な表を生成する\n",
    "# # FTの実行時間の表とFFTの実行回数の表\n",
    "\n",
    "# # ipynb形式のライブラリのインポート\n",
    "# %run ./lib.ipynb\n",
    "\n",
    "# Classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "# FunctionCalled = []\n",
    "# columns = 0\n",
    "# for Class in Classes:\n",
    "\n",
    "#     Example = return_fixed_class(BenchMark=\"ft\", FixedBenchMarkClass=Class)\n",
    "#     DF = pd.concat(Example, axis=1)\n",
    "#     columns = DF.columns.tolist()\n",
    "#     sample = DF.T[\"FFT\"]\n",
    "#     FunctionCalled.append(sample)\n",
    "\n",
    "# DF = pd.DataFrame(FunctionCalled)\n",
    "# DF.index = Classes\n",
    "# DF.to_latex(\"table_LatexForm/FunctionCalls.table\")\n",
    "\n",
    "\n",
    "# DF = pd.read_csv(\"csv_files/ExecTime@ft.csv\", index_col=0)\n",
    "# DF = DF[[\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\"]]\n",
    "# DF = DF.T[[\"A\", \"D\"]].T\n",
    "# DF = DF[[\"8\", \"16\", \"32\", \"64\", \"128\", \"256\"]]\n",
    "# DF.to_latex(\"table_LatexForm/ExecTimes.table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      平均誤差率   予測計算時間   実測計算時間    時間短縮率\n",
      "BT  35.6674  1852.16  29752.3  6.22526\n",
      "CG  10.4759   513.92  14012.8   3.6675\n",
      "EP        0   507.52  6827.52  7.43345\n",
      "FT  15.7267   541.44  10331.5  5.24066\n",
      "IS  3.74143    37.12   615.04  6.03538\n",
      "LU  15.1996  1416.32  17210.2  8.22952\n",
      "MG   52.837   128.64  2328.32  5.52501\n",
      "SP  20.9777   1737.6  40019.8  4.34185\n",
      "平均  19.3282   841.84  15137.2  5.83733\n"
     ]
    }
   ],
   "source": [
    "# 論文で必要な表を生成する\n",
    "# 平均誤差率の表\n",
    "\n",
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib.ipynb\n",
    "\n",
    "\n",
    "def returnDF_GenSummaryTable(benchmarks=[\"cg\"], Fixed=\"Process\", Fix=64, Predict=\"D\"):\n",
    "    ListBeforeDF = []\n",
    "    for benchmark in benchmarks:\n",
    "        if(Fixed==\"Process\"):\n",
    "            FixedDF = return_ErrorRateFixedProcess_AveragePredictCostRealCost(benchmark=benchmark, FixProcess=Fix, predict_class=Predict)\n",
    "            GraphTitle = f\"ベンチマーク{benchmarks}で実行プロセス数を固定\"\n",
    "        else:\n",
    "            FixedDF = return_ErrorRateFixedClass_AveragePredictCostRealCost(benchmark=benchmark, FixClass=Fix, predict_process=Predict)\n",
    "            GraphTitle = f\"ベンチマーク{benchmarks}でベンチマーククラスを固定\"\n",
    "        index = FixedDF.index.tolist()\n",
    "        FixedDF = FixedDF.T[[index[1]]].T\n",
    "        FixedDF.index = [benchmark.upper()]\n",
    "        ListBeforeDF.append(FixedDF)\n",
    "    returnDF = pd.concat(ListBeforeDF)\n",
    "    return(returnDF)\n",
    "    \n",
    "def add_Average(DF):\n",
    "    mean = DF.mean()\n",
    "    mean.name = \"平均\"\n",
    "    DF = DF.append(mean)\n",
    "    return(DF)\n",
    "\n",
    "def add_CompressionRate(DF):\n",
    "    DF[\"時間短縮率\"] = DF[\"予測計算時間\"]/DF[\"実測計算時間\"]*100\n",
    "    return(DF)\n",
    "    \n",
    "columns = [\"平均誤差率\", \"予測計算時間\", \"実測計算時間\"]\n",
    "FixedProcessDF = returnDF_GenSummaryTable(benchmarks=benchmarks, Fixed=\"Process\", Fix=64, Predict=\"D\")\n",
    "FixedProcessDF.columns = columns\n",
    "FixedProcessDF = add_CompressionRate(FixedProcessDF)\n",
    "FixedProcessDF = add_Average(FixedProcessDF)\n",
    "FixedProcessDF.to_latex(\"table_LatexForm/FixProcessAverageAndCosts.table\")\n",
    "print(FixedProcessDF)\n",
    "FixedClassDF = returnDF_GenSummaryTable(benchmarks=benchmarks, Fixed=\"Class\", Fix=\"B\", Predict=256)\n",
    "FixedClassDF.columns = columns\n",
    "FixedClassDF = add_CompressionRate(FixedClassDF)\n",
    "FixedClassDF = add_Average(FixedClassDF)\n",
    "FixedClassDF.to_latex(\"table_LatexForm/FixClassAverageAndCosts.table\")\n",
    "\n",
    "print(FixedClassDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
