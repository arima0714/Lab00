{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:hello\n",
      "DEBUG:lib.lab_lib:hello\n"
     ]
    }
   ],
   "source": [
    "jupyter_pwd = %pwd\n",
    "if jupyter_pwd == \"/\":\n",
    "    %cd /workspace\n",
    "\n",
    "# %pdb on\n",
    "\n",
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb\n",
    "\n",
    "# ipynb形式のライブラリノートを.py形式に変更したものをインポート\n",
    "import lib\n",
    "import lib.lab_lib\n",
    "from lib.lab_lib import *\n",
    "\n",
    "# 生データの入ったCSVファイルの保持されたディレクトリ名を格納している変数\n",
    "csvDirPath = \"./csv_files/\"\n",
    "\n",
    "# NPBのベンチマーク名のリスト\n",
    "benchmarkNames = [\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"]\n",
    "\n",
    "# NPBのプロセス数\n",
    "npb_process :list[int] = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "train_npb_process :list[int] = npb_process[:-1]\n",
    "test_npb_process :list[int] = npb_process[-1:]\n",
    "# NPBのCGの初期変数\n",
    "cg_na: list[int] = [14000, 30000, 75000, 100000, 1500000]\n",
    "cg_nonzer: list[int] = [11, 12, 13, 14, 15, 18, 21]\n",
    "cg_niter: list[int] = [15, 30, 75, 90, 100]\n",
    "cg_shift: list[int] = [20, 40, 60, 80, 110, 200]\n",
    "\n",
    "train_cg_na: list[int] = cg_na[:-1]\n",
    "train_cg_nonzer: list[int] = cg_nonzer[:-1]\n",
    "train_cg_niter: list[int] = cg_niter[:-1]\n",
    "train_cg_shift: list[int] = cg_shift[:-1]\n",
    "\n",
    "test_cg_na: list[int] = cg_na[-1:]\n",
    "test_cg_nonzer: list[int] = cg_nonzer[-1:]\n",
    "test_cg_niter: list[int] = cg_niter[-1:]\n",
    "test_cg_shift: list[int] = cg_shift[-1:]\n",
    "\n",
    "# NPBのEPの初期変数\n",
    "train_ep_process :list[int] = [4, 8, 16, 32, 64]\n",
    "train_ep_size :list[str] = [24, 25, 28, 30, 32]\n",
    "test_ep_process :list[int] = [128, 256, 512]\n",
    "test_ep_size :list[str] = [36, 40, 44]\n",
    "\n",
    "# NPBのFTの初期変数\n",
    "train_ft_process :list[int] = [2, 4, 8, 16, 32, 64]\n",
    "train_ft_grid_size :list[int] = [32, 64, 128, 256, 512]\n",
    "train_ft_nit :list[int] = [5, 10, 15, 20, 25]\n",
    "\n",
    "test_ft_process :list[int] = [128, 256, 512]\n",
    "test_ft_grid_size :list[int] = [1024, 2048, 4096]\n",
    "test_ft_nit :list[int] = [30, 40, 50]\n",
    "\n",
    "# NPBのISの初期変数\n",
    "train_is_process :list[int] = [2, 4, 8, 16]\n",
    "train_is_no_of_keys :list[int] = [18, 20, 22, 24, 26]\n",
    "train_is_max_value :list[int] = [9, 11, 13, 15]\n",
    "\n",
    "test_is_process :list[int] = [32, 64, 128]\n",
    "test_is_no_of_keys :list[int] = [28, 29, 30]\n",
    "test_is_max_value :list[int] = [18, 19, 20]\n",
    "\n",
    "# NPBのMGの初期変数\n",
    "mg_size :list[int] = [32, 64, 128, 256, 512]\n",
    "mg_nit: list[int] = [4, 10, 20, 35, 50]\n",
    "\n",
    "train_mg_process :list[int] = [4, 8, 16, 32, 64]\n",
    "train_mg_size :list[int] = [4, 8, 16, 32, 64]\n",
    "train_mg_nit :list[int] = [5, 10, 15, 20, 25]\n",
    "\n",
    "test_mg_process :list[int] = [128, 256, 512]\n",
    "test_mg_size :list[int] = [128, 256, 512]\n",
    "test_mg_nit :list[int] = [30, 40, 50]\n",
    "\n",
    "# LULESH ベンチマークプログラムのプロセス数・問題サイズ・イテレーション数\n",
    "lulesh_processes: list[int] = [8, 27, 64, 125, 216, 343, 512]\n",
    "lulesh_iterations: list[int] = [8, 16, 32, 64, 128, 256]\n",
    "lulesh_sizes: list[int] = [16, 24, 32, 48, 64, 128]\n",
    "\n",
    "train_lulesh_processes: list[int] = [8, 27, 64, 125, 216, 343]\n",
    "train_lulesh_iterations: list[int] = [8, 16, 32, 64, 128]\n",
    "train_lulesh_sizes: list[int] = [16, 24, 32, 48]\n",
    "\n",
    "test_lulesh_processes: list[int] = [512, 729, 1000]\n",
    "test_lulesh_iterations: list[int] = [256, 512, 1024]\n",
    "test_lulesh_sizes: list[int] = [64, 96, 128]\n",
    "\n",
    "# Extra-Pのオプション\n",
    "modelerNames: list[str] = [\n",
    "    # \"refining\", \n",
    "    \"multi-parameter\",\n",
    "    \"default\", \n",
    "    # \"basic --options poly_exponents=-1,0,1,2,3 log_exponents=0,1 force_combination_exponents=1 allow_negative_exponents=1\"\n",
    "    ]\n",
    "\n",
    "modelerOption: str = \"\"\" --options \\#spm=Basic \\#spo=poly_exponents=-1,0,1,2,3,log_exponents=0,1,force_combination_exponents=1,allow_negative_exponents=True\"\"\"\n",
    "\n",
    "list_csvDir = [\n",
    "    \"./csv_files/lulesh_1st/\",\n",
    "    \"./csv_files/lulesh_2nd/\",\n",
    "    \"./csv_files/lulesh_3rd/\",\n",
    "]\n",
    "\n",
    "list_csvDir_mg = [\n",
    "    \"./csv_files/mg_1st/\",\n",
    "    \"./csv_files/mg_2nd/\",\n",
    "    \"./csv_files/mg_3rd/\",\n",
    "]\n",
    "\n",
    "list_csvDir_ft :list[str] = [\n",
    "    \"./csv_files/ft_1st/\",\n",
    "    \"./csv_files/ft_2nd/\",\n",
    "    \"./csv_files/ft_3rd/\",\n",
    "]\n",
    "\n",
    "list_csvDir_ep :list[str] = [\n",
    "    \"./csv_files/ep_1st/\",\n",
    "    \"./csv_files/ep_2nd/\",\n",
    "    \"./csv_files/ep_3rd/\",\n",
    "]\n",
    "\n",
    "list_csvDir_is :list[str] = [\n",
    "    \"./csv_files/is_1st/\",\n",
    "    \"./csv_files/is_2nd/\",\n",
    "    \"./csv_files/is_3rd/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rawDF_is(\n",
    "    list_process: list[int],\n",
    "    list_no_of_keys: list[int],\n",
    "    list_key_max_value :list[int],\n",
    "    csvDir: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"return_rawDF_is()\n",
    "\n",
    "    ベンチマークプログラムISの手動で変更した初期変数におけるプロファイルを取得する関数\n",
    "\n",
    "    Args:\n",
    "        list_process(list[int]):プロセス数のリスト\n",
    "        list_no_of_keys(list[int]):初期変数no_of_keysのリスト\n",
    "        list_key_max_value(list[int]):初期変数key_max_valueのリスト\n",
    "        csvDir(str):CSVファイルの保持されているディレクトリ\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    list_before_concat_DF: list[pd.DataFrame] = []\n",
    "\n",
    "    for elem_process in list_process:\n",
    "        for elem_no_of_keys in list_no_of_keys:\n",
    "            for elem_key_max_value in list_key_max_value:\n",
    "                filePath: str = f\"{csvDir}is_no_of_keys{elem_no_of_keys}_key_max_value{elem_key_max_value}_process{elem_process}.csv\"\n",
    "                if os.path.isfile(filePath):\n",
    "                    try:\n",
    "                        DF_read_raw: pd.DataFrame = pd.read_csv(filePath)\n",
    "                        DF_read_raw[\"process\"] = elem_process\n",
    "                        DF_read_raw[\"no_of_keys\"] = elem_no_of_keys\n",
    "                        DF_read_raw[\"key_max_value\"] = elem_key_max_value\n",
    "                        list_before_concat_DF.append(DF_read_raw)\n",
    "                    except:\n",
    "                        warnings.warn(f\"{filePath} is empty.\")\n",
    "                else:\n",
    "                    warnings.warn(f\"{filePath} doesn't exist\")\n",
    "                    continue\n",
    "    return pd.concat(objs=list_before_concat_DF, axis=0)\n",
    "\n",
    "\n",
    "def ret_averaged_rawDF_is(\n",
    "    list_process: list[int],\n",
    "    list_no_of_keys: list[int],\n",
    "    list_key_max_value :list[int],\n",
    "    list_csvDir: list[str],\n",
    "    resVar: str,\n",
    "):\n",
    "    \"\"\"複数のCSVからDFを取得する関数（ベンチマークプログラムEP）\n",
    "\n",
    "    列Inclusiveおよび列Exclusiveが秒に変換され、InclusivePerCallもしくはExclusivePerCall列が生成される。\n",
    "\n",
    "    Args:\n",
    "        list_process(list[int]):プロセス数のリスト\n",
    "        list_no_of_keys(list[int]):初期変数no_of_keysのリスト\n",
    "        list_key_max_value(list[int]):初期変数key_max_valueのリスト\n",
    "        list_csvDir(list[str]):CSVを保持したディレクトリ名のリスト\n",
    "        resVar(str):説明変数の文字列\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    list_DFs_for_return: list[pd.DataFrame] = []\n",
    "    for elem_process in list_process:\n",
    "        for elem_no_of_keys in list_no_of_keys:\n",
    "            for elem_key_max_value in list_key_max_value:\n",
    "                list_inputDFs_for_averaged: list[pd.DataFrame] = []\n",
    "                for elem_csvDir in list_csvDir:\n",
    "                    try:\n",
    "                        _raw_DF: pd.DataFrame = return_rawDF_is(\n",
    "                            list_process=[elem_process],\n",
    "                            list_no_of_keys=[elem_no_of_keys],\n",
    "                            list_key_max_value=[elem_key_max_value],\n",
    "                            csvDir=elem_csvDir,\n",
    "                        )\n",
    "\n",
    "                        if resVar in [\"Exclusive\", \"Inclusive\", \"#Call\", \"#Subrs\"]:\n",
    "                            # resVar 列の整形\n",
    "                            if resVar in [\"Exclusive\", \"Inclusive\"]:\n",
    "                                _tmp_converted = map(\n",
    "                                    convertPprofTime, list(_raw_DF[resVar])\n",
    "                                )\n",
    "                                _raw_DF[resVar] = list(_tmp_converted)\n",
    "                            # {resVar}PerCall 列の生成\n",
    "                            _raw_DF = add_perCallColumn(\n",
    "                                inputDF=_raw_DF,\n",
    "                                divisorColName=\"#Call\",\n",
    "                                dividendColName=resVar,\n",
    "                                targetColumnName=f\"{resVar}PerCall\",\n",
    "                            )\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    list_inputDFs_for_averaged.append(_raw_DF)\n",
    "                list_DFs_for_return.append(\n",
    "                    ret_averagedDF(inputDFs=list_inputDFs_for_averaged, resVar=resVar)\n",
    "                )\n",
    "    return pd.concat(objs=list_DFs_for_return, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF_FT :pd.DataFrame = ret_averaged_rawDF_ft(list_process=train_ft_process, list_grid_size=train_ft_grid_size, list_nit=train_ft_nit, list_csvDir=list_csvDir_ft, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "testDF_FT :pd.DataFrame = ret_averaged_rawDF_ft(list_process=test_ft_process, list_grid_size=test_ft_grid_size, list_nit=test_ft_nit, list_csvDir=list_csvDir_ft, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "trainDF_MG :pd.DataFrame = ret_averaged_rawDF_mg(list_process=train_mg_process, list_size=train_mg_size, list_nit=train_mg_nit, list_csvDir=list_csvDir_mg, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "testDF_MG :pd.DataFrame = ret_averaged_rawDF_mg(list_process=test_mg_process, list_size=test_mg_size, list_nit=test_mg_nit, list_csvDir=list_csvDir_mg, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "trainDF_EP :pd.DataFrame = ret_averaged_rawDF_ep(list_process=train_ep_process, list_size=train_ep_size, list_csvDir=list_csvDir_ep, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "testDF_EP :pd.DataFrame = ret_averaged_rawDF_ep(list_process=test_ep_process, list_size=test_ep_size, list_csvDir=list_csvDir_ep, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "trainDF_IS :pd.DataFrame = ret_averaged_rawDF_is(list_process=train_is_process, list_key_max_value=train_is_max_value, list_no_of_keys=train_is_no_of_keys, list_csvDir=list_csvDir_is, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "testDF_IS :pd.DataFrame = ret_averaged_rawDF_is(list_process=test_is_process, list_key_max_value=test_is_max_value, list_no_of_keys=test_is_no_of_keys, list_csvDir=list_csvDir_is, resVar=\"Exclusive\")\n",
    "\n",
    "trainDF_lulesh :pd.DataFrame = ret_averaged_rawDF_lulesh(\n",
    "    list_process=train_lulesh_processes,\n",
    "    list_iteration=train_lulesh_iterations,\n",
    "    list_size = train_lulesh_sizes,\n",
    "    list_csvDir=list_csvDir,\n",
    "    resVar=\"Exclusive\"\n",
    ")\n",
    "\n",
    "testDF_lulesh :pd.DataFrame = ret_averaged_rawDF_lulesh(\n",
    "    list_process=test_lulesh_processes,\n",
    "    list_iteration=test_lulesh_iterations,\n",
    "    list_size = test_lulesh_sizes,\n",
    "    list_csvDir=list_csvDir,\n",
    "    resVar=\"Exclusive\"\n",
    ")\n",
    "\n",
    "expVars_ep :list[str] = [\"process\", \"size\"]\n",
    "expVars_ft :list[str] = [\"process\", \"grid_size\", \"nit\"]\n",
    "expVars_is :list[str] = [\"process\", \"no_of_keys\", \"key_max_value\"]\n",
    "expVars_mg :list[str] = [\"process\", \"problem_size\", \"nit\"]\n",
    "expVars_lulesh :list[str] = [\"process\", \"iteration\", \"size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必要な結果\n",
    "\n",
    "* 関数コール回数予測\n",
    "    * モデル適合度(MAPE表)\n",
    "    * モデル予測精度(MAPEが縦軸のグラフ、重み付きMAPEが縦軸のグラフ)\n",
    "* 実行時間予測\n",
    "    * モデル適合度(MAPE表)\n",
    "    * モデル予測精度(MAPEが縦軸のグラフ、重み付きMAPEが縦軸のグラフ)\n",
    "\n",
    "# 必要なブツ\n",
    "\n",
    "* ✅モデル構築用データ\n",
    "* ✅予測対象用データ\n",
    "* ✅関数コール回数の予測モデル\n",
    "* ✅関数の総実行時間の予測モデル\n",
    "* ✅1コール当たりの関数の総実行時間の予測モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentsForExtraP:\n",
    "    \"\"\"class ContentsForExtraP\n",
    "\n",
    "    ExtraPによるモデルの生成\n",
    "\n",
    "    生成されるモデルは次の通り。\n",
    "\n",
    "    * 関数コール回数の予測モデル\n",
    "    * 関数の総実行時間の予測モデル\n",
    "    * 1コール当たりの関数の総実行時間の予測モデル\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trainDF: pd.DataFrame,\n",
    "        testDF: pd.DataFrame,\n",
    "        expVars: list[str],\n",
    "        resVar: str,\n",
    "        resVarPerCall: str,\n",
    "        benchmarkName :str,\n",
    "        modelerName :str = \"multi-parameter\",\n",
    "        modelerOption :str = \"\"\" --options \\#spm=Basic \\#spo=poly_exponents=-1,0,1,2,3,log_exponents=0,1,force_combination_exponents=1,allow_negative_exponents=True\"\"\"\n",
    "    ):\n",
    "        \"\"\"__init__()\n",
    "\n",
    "        初期化変数\n",
    "\n",
    "        Args:\n",
    "            trainDF(pd.DataFrame):モデル構築用データ\n",
    "            testDF(pd.DataFrame):予測対象用データ\n",
    "            expVars(list[str]):説明変数を格納したリスト\n",
    "            resVar(str):目的変数を示した文字列\n",
    "            resVarsPerCall(str):1コール当たりの目的変数を示した文字列\n",
    "\n",
    "        \"\"\"\n",
    "        self.trainDF = trainDF.reset_index()\n",
    "        self.testDF = testDF.reset_index()\n",
    "        self.expVars = expVars\n",
    "        self.resVar = resVar\n",
    "        self.resVarPerCall = resVarPerCall\n",
    "        self.functionNames :list[str] = sorted(list(set(trainDF[\"Name\"].to_list())))\n",
    "        self.benchmarkName :str = benchmarkName\n",
    "        self.modelerName :str = modelerName\n",
    "        self.modelerOption :str = modelerOption\n",
    "        self.dict_symbols :dict[str, str] = {}\n",
    "        for elem in expVars:\n",
    "            self.dict_symbols[elem] = symbols(elem, real=True)\n",
    "        \n",
    "        self.dict_resVar = {}\n",
    "        self.dict_resVarPerCall = {}\n",
    "        self.dict_call = {}\n",
    "\n",
    "    def build_all_models(self):\n",
    "        \"\"\"build_all_models()\n",
    "\n",
    "        モデルを構築する関数。目的変数はself.resVars, self.resVarsPerCallを利用。\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        for functionName in self.functionNames:\n",
    "            trainDF_perFunc :pd.DataFrame = self.trainDF[self.trainDF[\"Name\"] == functionName]\n",
    "\n",
    "            model_resVar :str = get_ExtraP_model(\n",
    "                inputDF_perFunc=trainDF_perFunc,\n",
    "                expVar = self.expVars,\n",
    "                resVar = self.resVar,\n",
    "                functionName = functionName,\n",
    "                dict_symbols=self.dict_symbols,\n",
    "                benchmarkName = self.benchmarkName,\n",
    "                modelerName = self.modelerName,\n",
    "                modelerOption = self.modelerOption\n",
    "            )\n",
    "\n",
    "            model_resVarPerCall :str = get_ExtraP_model(\n",
    "                inputDF_perFunc=trainDF_perFunc,\n",
    "                expVar = self.expVars,\n",
    "                resVar = self.resVarPerCall,\n",
    "                functionName = functionName,\n",
    "                dict_symbols=self.dict_symbols,\n",
    "                benchmarkName = self.benchmarkName,\n",
    "                modelerName = self.modelerName,\n",
    "                modelerOption = self.modelerOption\n",
    "            )\n",
    "\n",
    "            model_call :str = get_ExtraP_model(\n",
    "                inputDF_perFunc=trainDF_perFunc,\n",
    "                expVar = self.expVars,\n",
    "                resVar = \"#Call\",\n",
    "                functionName = functionName,\n",
    "                dict_symbols=self.dict_symbols,\n",
    "                benchmarkName = self.benchmarkName,\n",
    "                modelerName = self.modelerName,\n",
    "                modelerOption = self.modelerOption\n",
    "            )\n",
    "\n",
    "            self.dict_resVar[functionName] = model_resVar\n",
    "            self.dict_resVarPerCall[functionName] = model_resVarPerCall\n",
    "            self.dict_call[functionName] = model_call\n",
    "\n",
    "    def predict_train(self):\n",
    "        \"\"\"predict_train(self)\n",
    "\n",
    "        学習データに対して予測を実施する関数\n",
    "\n",
    "        列構成は下記の通り\n",
    "\n",
    "        |<expVar>|Name(関数名)|#Call(コール回数実測値)|predicted_resVar（）|predicted_ExclusivePerCall\tpredicted_#Call>|<>|\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        _list_series :list[pd.Series] = []\n",
    "        for index, _sr in self.trainDF.iterrows():\n",
    "            _series :pd.Series = pd.Series(dtype=\"object\")\n",
    "            functionName :str = _sr[\"Name\"]\n",
    "\n",
    "            _target_env :list[set[any]] = []\n",
    "            for _expVar in self.expVars:\n",
    "                _series[_expVar] = _sr[_expVar]\n",
    "                _target_env.append((self.dict_symbols[_expVar], _sr[_expVar]))\n",
    "\n",
    "            _series[\"Name\"] = _sr[\"Name\"]\n",
    "            _series[\"#Call\"] = _sr[\"#Call\"]\n",
    "            _series[f\"{self.resVar}\"] = _sr[f\"{self.resVar}\"]\n",
    "\n",
    "            _predicted_resVar: float\n",
    "            _predicted_resVarPerCall :float\n",
    "            _predicted_call :float\n",
    "\n",
    "            _predicted_resVar = self.dict_resVar[functionName].subs(_target_env).evalf()\n",
    "            _predicted_resVarPerCall = self.dict_resVarPerCall[functionName].subs(_target_env).evalf()\n",
    "            _predicted_call = self.dict_call[functionName].subs(_target_env).evalf()\n",
    "\n",
    "            _series[f\"predicted_{self.resVar}\"] = _predicted_resVar\n",
    "            _series[f\"predicted_{self.resVarPerCall}\"] = _predicted_resVarPerCall\n",
    "            _series[f\"predicted_#Call\"] = _predicted_call\n",
    "            _series[f\"predicted_{self.resVar}_indirectly\"] = _predicted_resVarPerCall * _predicted_call\n",
    "\n",
    "            _list_series.append(_series)\n",
    "\n",
    "        self.DF_predicted_by_train :pd.DataFrame = pd.DataFrame(data=_list_series).astype({\n",
    "            f\"predicted_{self.resVar}\": float,\n",
    "            f\"predicted_{self.resVarPerCall}\": float,\n",
    "            f\"predicted_#Call\": float,\n",
    "            f\"predicted_{self.resVar}_indirectly\": float,\n",
    "        })\n",
    "\n",
    "    def predict_test(self):\n",
    "        \"\"\"predict_test(self)\n",
    "        \n",
    "        予測対象データに対して予測を実施する関数\n",
    "        \"\"\"\n",
    "\n",
    "        _list_series :list[pd.Series] = []\n",
    "\n",
    "        for index, _sr in self.testDF.iterrows():\n",
    "            _series :pd.Series = pd.Series(dtype=\"object\")\n",
    "            functionName :str = _sr[\"Name\"]\n",
    "\n",
    "            _target_env :list[set[any]] = []\n",
    "            for _expVar in self.expVars:\n",
    "                _series[_expVar] = _sr[_expVar]\n",
    "                _target_env.append((self.dict_symbols[_expVar], _sr[_expVar]))\n",
    "\n",
    "            _series[\"Name\"] = functionName\n",
    "            _series[\"#Call\"] = _sr[\"#Call\"]\n",
    "            _series[f\"{self.resVar}\"] = _sr[f\"{self.resVar}\"]\n",
    "\n",
    "            _predicted_resVar :float = self.dict_resVar[functionName].subs(_target_env).evalf()\n",
    "            _predicted_resVarPerCall :float = self.dict_resVarPerCall[functionName].subs(_target_env).evalf()\n",
    "            _predicted_call :float = self.dict_call[functionName].subs(_target_env).evalf()\n",
    "\n",
    "            _series[f\"predicted_{self.resVar}\"] = _predicted_resVar\n",
    "            _series[f\"predicted_{self.resVarPerCall}\"] = _predicted_resVarPerCall\n",
    "            _series[f\"predicted_#Call\"] = _predicted_call\n",
    "            _series[f\"predicted_{self.resVar}_indirectly\"] = _predicted_resVarPerCall * _predicted_call\n",
    "\n",
    "            _list_series.append(_series)\n",
    "\n",
    "        self.DF_predicted_by_test :pd.DataFrame = pd.DataFrame(data=_list_series)\n",
    "\n",
    "    def get_result_of_predict_train(self):\n",
    "        return self.DF_predicted_by_train\n",
    "\n",
    "    def get_DF_all_fitness_with_relativeErrorRate(self) -> pd.DataFrame:\n",
    "        \"\"\"get_DF_all_fitness_with_relativeErrorRate()\n",
    "        \n",
    "        モデル適合度のテーブルを返す関数\n",
    "\n",
    "        列APE～の平均をとるとMAPEになる。\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        _returnDF :pd.DataFrame = add_relativeErrorRateCol(\n",
    "            inputDF = self.DF_predicted_by_train,\n",
    "            real_colName=\"#Call\",\n",
    "            predicted_colName=\"predicted_#Call\",\n",
    "            targetColName=\"APE_#Call\"\n",
    "        )\n",
    "        _returnDF = add_relativeErrorRateCol(\n",
    "            inputDF = _returnDF,\n",
    "            real_colName=f\"{self.resVar}\",\n",
    "            predicted_colName=f\"predicted_{self.resVar}\",\n",
    "            targetColName=f\"APE_{self.resVar}\",\n",
    "        )\n",
    "        _returnDF = add_relativeErrorRateCol(\n",
    "            inputDF = _returnDF,\n",
    "            real_colName=f\"{self.resVar}\",\n",
    "            predicted_colName=f\"predicted_{self.resVar}_indirectly\",\n",
    "            targetColName=f\"APE_{self.resVarPerCall}\",\n",
    "        )\n",
    "\n",
    "        return _returnDF\n",
    "\n",
    "    def get_DF_weightedMAPE(self) -> pd.DataFrame:\n",
    "        \"\"\"get_DF_all_fitness_with_weightedMAPE()\"\"\"\n",
    "        _dict_expVar_set :dict[str, set[int]]= {}\n",
    "        for _expVar in self.expVars:\n",
    "            _dict_expVar_set[_expVar] = set(sorted(self.testDF[_expVar]))\n",
    "\n",
    "        if len(self.expVars) == 2:\n",
    "            _product = itertools.product(\n",
    "                _dict_expVar_set[self.expVars[0]],\n",
    "                _dict_expVar_set[self.expVars[1]]\n",
    "            )\n",
    "        elif len(self.expVars) == 3:\n",
    "            _product = itertools.product(\n",
    "                _dict_expVar_set[self.expVars[0]],\n",
    "                _dict_expVar_set[self.expVars[1]],\n",
    "                _dict_expVar_set[self.expVars[2]]\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(f\"self.expVars == {len(self.expVars)}\")\n",
    "            return -1\n",
    "\n",
    "        def ret_query (list_expVar :list[str], list_product :list[int]) -> str:\n",
    "            retStr :str= \"\"\n",
    "            for i_expVar in range(len(list_expVar)):\n",
    "                retStr += f\"{list_expVar[i_expVar]}=={list_product[i_expVar]}\"\n",
    "                if i_expVar != len(list_expVar)-1:\n",
    "                    retStr += \" & \"\n",
    "            return retStr\n",
    "\n",
    "        _list_series :pd.Series = []\n",
    "        for _elem_product in _product:\n",
    "\n",
    "            _query :str = ret_query(list_expVar = self.expVars, list_product=list(_elem_product))\n",
    "\n",
    "            _targetDF :pd.DataFrame = self.testDF.query(_query)\n",
    "            _target_env :list[set[any]] = []\n",
    "\n",
    "            _series :pd.Series = pd.Series(dtype=\"object\")\n",
    "\n",
    "            for i_expVar in range(len(self.expVars)):\n",
    "                _target_env.append((self.dict_symbols[self.expVars[i_expVar]], _elem_product[i_expVar]))\n",
    "                _series[self.expVars[i_expVar]] = _elem_product[i_expVar]\n",
    "\n",
    "            _c_sum :float = sum(_targetDF[\"#Call\"].tolist())\n",
    "            _target_weightedMAPE_resVar :float = 0.0\n",
    "            _target_weightedMAPE_resVarPerCall :float = 0.0\n",
    "            _target_weightedMAPE_call :float = 0.0\n",
    "            for i, sr in _targetDF.iterrows():\n",
    "                _c_t :float = sr[\"#Call\"]\n",
    "                functionName :str = sr[\"Name\"]\n",
    "\n",
    "                _A_t_resVar :float = sr[self.resVar]\n",
    "                _A_t_resVarPerCall :float = sr[self.resVarPerCall]\n",
    "                _A_t_call :float = sr[\"#Call\"]\n",
    "\n",
    "                _F_t_resVar :float = self.dict_resVar[functionName].subs(_target_env).evalf()\n",
    "                _F_t_resVarPerCall :float = self.dict_resVarPerCall[functionName].subs(_target_env).evalf()\n",
    "                _F_t_call :float = self.dict_call[functionName].subs(_target_env).evalf()\n",
    "\n",
    "                \n",
    "                _target_weightedMAPE_resVar += abs(_A_t_resVar - _F_t_resVar)/_A_t_resVar\n",
    "                _target_weightedMAPE_resVarPerCall += abs(_A_t_resVarPerCall - _F_t_resVarPerCall)/_A_t_resVarPerCall\n",
    "                _target_weightedMAPE_call += abs(_A_t_call - _F_t_call)/_A_t_call\n",
    "\n",
    "            _target_weightedMAPE_resVar *= 100/_c_sum\n",
    "            _target_weightedMAPE_resVarPerCall *= 100/_c_sum\n",
    "            _target_weightedMAPE_call *= 100/_c_sum\n",
    "            \n",
    "            _series[f\"weightedMAPE_{self.resVar}\"] = _target_weightedMAPE_resVar\n",
    "            _series[f\"weightedMAPE_{self.resVarPerCall}\"] = _target_weightedMAPE_resVarPerCall\n",
    "            _series[f\"weightedMAPE_call\"] = _target_weightedMAPE_call\n",
    "\n",
    "            _list_series.append(_series)\n",
    "        self.DF_weightedMAPE :pd.DataFrame = pd.DataFrame(data=_list_series)\n",
    "        # self.DF_weightedMAPE :pd.DataFrame = pd.DataFrame(data=_list_series).astype({f\"weightedMAPE_{self.resVar}\": float, f\"weightedMAPE_{self.resVarPerCall}\": float, f\"weightedMAPE_call\": float})\n",
    "        return self.DF_weightedMAPE\n",
    "                \n",
    "def return_summarizedDF_on_functionName(\n",
    "    inputDF :pd.DataFrame,\n",
    "    resVar :str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"return_summarizedDF_on_functionName()\n",
    "    \n",
    "    入力されたDFを関数ごとにまとめ、各数値を平均値化し、そのDFを返す関数\n",
    "\n",
    "    Args:\n",
    "        inputDF(pd.DataFrame): 入力DF\n",
    "        resVar(string): 目的変数名\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    functionNames :list[str] = list(set(inputDF[\"Name\"].to_list()))\n",
    "    _list_series :list[pd.Series] = []\n",
    "    for functionName in functionNames:\n",
    "        inputDF_perFunc :pd.DataFrame = inputDF[inputDF[\"Name\"] == functionName]\n",
    "        list_APE_resVar :list[float] = inputDF_perFunc[f\"APE_{resVar}\"].to_list()\n",
    "        list_APE_resVarPerCall :list[float] = inputDF_perFunc[f\"APE_{resVar}PerCall\"].to_list()\n",
    "        list_APE_call :list[float] = inputDF_perFunc[f\"APE_#Call\"].to_list()\n",
    "\n",
    "        float_MAPE_resVar :float = np.mean(list_APE_resVar)\n",
    "        float_MAPE_resVarPerCall :float = np.mean(list_APE_resVarPerCall)\n",
    "        float_MAPE_call :float = np.mean(list_APE_call)\n",
    "\n",
    "        _series :pd.Series = pd.Series(dtype=\"object\")\n",
    "        _series[\"Name\"] = functionName\n",
    "        _series[f\"MAPE_#Call\"] = float_MAPE_call\n",
    "        _series[f\"MAPE_{resVar}\"] = float_MAPE_resVar\n",
    "        _series[f\"MAPE_{resVar}PerCall\"] = float_MAPE_resVarPerCall\n",
    "\n",
    "        _list_series.append(_series)\n",
    "\n",
    "    return pd.DataFrame(data=_list_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkName :str = \"mg\"\n",
    "\n",
    "mg_contents = ContentsForExtraP(trainDF = trainDF_MG, testDF= testDF_MG, resVar = \"Exclusive\", expVars= expVars_mg ,resVarPerCall = \"ExclusivePerCall\", benchmarkName = \"mg\")\n",
    "mg_contents.build_all_models()\n",
    "mg_contents.predict_train()\n",
    "mg_contents.predict_test()\n",
    "mg_適合度 :pd.DataFrame = mg_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "mg_重み付きMAPE :pd.DataFrame = mg_contents.get_DF_weightedMAPE()\n",
    "mg_予測精度 :pd.DataFrame = mg_contents.get_result_of_predict_train()\n",
    "mg_MAPE_on_functionName :pd.DataFrame = return_summarizedDF_on_functionName(inputDF = mg_適合度, resVar = \"Exclusive\")\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    mg_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    mg_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n",
    "    mg_予測精度.to_excel(writer, sheet_name=\"予測精度\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()\n",
    "\n",
    "benchmarkName :str = \"ep\"\n",
    "\n",
    "ep_contents = ContentsForExtraP(trainDF = trainDF_EP, testDF= testDF_EP, resVar = \"Exclusive\", expVars= expVars_ep ,resVarPerCall = \"ExclusivePerCall\", benchmarkName = benchmarkName)\n",
    "ep_contents.build_all_models()\n",
    "ep_contents.predict_train()\n",
    "ep_contents.predict_test()\n",
    "ep_適合度 :pd.DataFrame = ep_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "ep_重み付きMAPE :pd.DataFrame = ep_contents.get_DF_weightedMAPE()\n",
    "ep_予測精度 :pd.DataFrame = ep_contents.get_result_of_predict_train()\n",
    "ep_MAPE_on_functionName :pd.DataFrame = return_summarizedDF_on_functionName(inputDF = ep_適合度, resVar = \"Exclusive\")\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    ep_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    ep_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n",
    "    ep_予測精度.to_excel(writer, sheet_name=\"予測精度\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkName :str = \"ft\"\n",
    "\n",
    "ft_contents = ContentsForExtraP(trainDF = trainDF_FT, testDF= testDF_FT, resVar = \"Exclusive\", expVars= expVars_ft ,resVarPerCall = \"ExclusivePerCall\", benchmarkName = benchmarkName)\n",
    "ft_contents.build_all_models()\n",
    "ft_contents.predict_train()\n",
    "ft_contents.predict_test()\n",
    "ft_適合度 :pd.DataFrame = ft_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "ft_重み付きMAPE :pd.DataFrame = ft_contents.get_DF_weightedMAPE()\n",
    "ft_予測精度 :pd.DataFrame = ft_contents.get_result_of_predict_train()\n",
    "ft_MAPE_on_functionName :pd.DataFrame = return_summarizedDF_on_functionName(inputDF = ft_適合度, resVar = \"Exclusive\")\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    ft_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    ft_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n",
    "    ft_予測精度.to_excel(writer, sheet_name=\"予測精度\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkName :str = \"is\"\n",
    "\n",
    "is_contents = ContentsForExtraP(trainDF = trainDF_IS, testDF= testDF_IS, resVar = \"Exclusive\", expVars= expVars_is, resVarPerCall = \"ExclusivePerCall\", benchmarkName = benchmarkName)\n",
    "is_contents.build_all_models()\n",
    "is_contents.predict_train()\n",
    "is_contents.predict_test()\n",
    "is_適合度 :pd.DataFrame = is_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "is_重み付きMAPE :pd.DataFrame = is_contents.get_DF_weightedMAPE()\n",
    "is_予測精度 :pd.DataFrame = is_contents.get_result_of_predict_train()\n",
    "is_MAPE_on_functionName :pd.DataFrame = return_summarizedDF_on_functionName(inputDF = is_適合度, resVar = \"Exclusive\")\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    is_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    is_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n",
    "    is_予測精度.to_excel(writer, sheet_name=\"予測精度\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkName :str = \"lulesh\"\n",
    "\n",
    "lulesh_contents = ContentsForExtraP(trainDF = trainDF_lulesh, testDF= testDF_lulesh, resVar = \"Exclusive\", expVars= expVars_lulesh ,resVarPerCall = \"ExclusivePerCall\", benchmarkName = \"lulesh\")\n",
    "lulesh_contents.build_all_models()\n",
    "lulesh_contents.predict_train()\n",
    "lulesh_contents.predict_test()\n",
    "lulesh_適合度 :pd.DataFrame = lulesh_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "lulesh_重み付きMAPE :pd.DataFrame = lulesh_contents.get_DF_weightedMAPE()\n",
    "lulesh_予測精度 :pd.DataFrame = lulesh_contents.get_result_of_predict_train()\n",
    "lulesh_MAPE_on_functionName :pd.DataFrame = return_summarizedDF_on_functionName(inputDF = lulesh_適合度, resVar = \"Exclusive\")\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    lulesh_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    lulesh_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n",
    "    lulesh_予測精度.to_excel(writer, sheet_name=\"予測精度\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benchmarkName</th>\n",
       "      <th>MAPE_#Call</th>\n",
       "      <th>MAPE_Exclusive</th>\n",
       "      <th>MAPE_ExclusivePerCall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EP</td>\n",
       "      <td>324.788035</td>\n",
       "      <td>603.094144</td>\n",
       "      <td>578.133169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FT</td>\n",
       "      <td>1.327231</td>\n",
       "      <td>20107.307838</td>\n",
       "      <td>385.155521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IS</td>\n",
       "      <td>64.667726</td>\n",
       "      <td>535.749682</td>\n",
       "      <td>516.144029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MG</td>\n",
       "      <td>11.575241</td>\n",
       "      <td>110.614697</td>\n",
       "      <td>47.061738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LULESH</td>\n",
       "      <td>2.638311</td>\n",
       "      <td>152.021226</td>\n",
       "      <td>106.569120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  benchmarkName  MAPE_#Call  MAPE_Exclusive  MAPE_ExclusivePerCall\n",
       "0            EP  324.788035      603.094144             578.133169\n",
       "1            FT    1.327231    20107.307838             385.155521\n",
       "2            IS   64.667726      535.749682             516.144029\n",
       "3            MG   11.575241      110.614697              47.061738\n",
       "4        LULESH    2.638311      152.021226             106.569120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_適合度_mean :pd.Series = ep_MAPE_on_functionName.mean(numeric_only=True)\n",
    "ft_適合度_mean :pd.Series = ft_MAPE_on_functionName.mean(numeric_only=True)\n",
    "is_適合度_mean :pd.Series = is_MAPE_on_functionName.mean(numeric_only=True)\n",
    "mg_適合度_mean :pd.Series = mg_MAPE_on_functionName.mean(numeric_only=True)\n",
    "lulesh_適合度_mean :pd.Series = lulesh_MAPE_on_functionName.mean(numeric_only=True)\n",
    "\n",
    "_list :list[pd.Series] = [\n",
    "    pd.Series([\"EP\", ep_適合度_mean[\"MAPE_#Call\"], ep_適合度_mean[\"MAPE_Exclusive\"], ep_適合度_mean[\"MAPE_ExclusivePerCall\"]],index=[\"benchmarkName\", \"MAPE_#Call\", \"MAPE_Exclusive\", \"MAPE_ExclusivePerCall\"]),\n",
    "    pd.Series([\"FT\", ft_適合度_mean[\"MAPE_#Call\"], ft_適合度_mean[\"MAPE_Exclusive\"], ft_適合度_mean[\"MAPE_ExclusivePerCall\"]],index=[\"benchmarkName\", \"MAPE_#Call\", \"MAPE_Exclusive\", \"MAPE_ExclusivePerCall\"]),\n",
    "    pd.Series([\"IS\", is_適合度_mean[\"MAPE_#Call\"], is_適合度_mean[\"MAPE_Exclusive\"], is_適合度_mean[\"MAPE_ExclusivePerCall\"]],index=[\"benchmarkName\", \"MAPE_#Call\", \"MAPE_Exclusive\", \"MAPE_ExclusivePerCall\"]),\n",
    "    pd.Series([\"MG\", mg_適合度_mean[\"MAPE_#Call\"], mg_適合度_mean[\"MAPE_Exclusive\"], mg_適合度_mean[\"MAPE_ExclusivePerCall\"]],index=[\"benchmarkName\", \"MAPE_#Call\", \"MAPE_Exclusive\", \"MAPE_ExclusivePerCall\"]),\n",
    "    pd.Series([\"LULESH\", lulesh_適合度_mean[\"MAPE_#Call\"], lulesh_適合度_mean[\"MAPE_Exclusive\"], lulesh_適合度_mean[\"MAPE_ExclusivePerCall\"]],index=[\"benchmarkName\", \"MAPE_#Call\", \"MAPE_Exclusive\", \"MAPE_ExclusivePerCall\"]),\n",
    "]\n",
    "適合度_mape :pd.DataFrame = pd.DataFrame(data=_list)\n",
    "\n",
    "適合度_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
