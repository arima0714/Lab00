{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_pwd = %pwd\n",
    "if jupyter_pwd == \"/\":\n",
    "    %cd /workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:hello\n"
     ]
    }
   ],
   "source": [
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb\n",
    "\n",
    "# 生データの入ったCSVファイルの保持されたディレクトリ名を格納している変数\n",
    "csvDirPath = \"./csv_files/\"\n",
    "\n",
    "# NPBのベンチマーク名のリスト\n",
    "benchmarkNames = [\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"]\n",
    "\n",
    "# LULESH ベンチマークプログラムのプロセス数・問題サイズ・イテレーション数\n",
    "lulesh_processes: list[int] = [8, 27, 64, 125, 216, 343, 512]\n",
    "lulesh_iterations: list[int] = [8, 16, 32, 64, 128, 256]\n",
    "lulesh_sizes: list[int] = [16, 24, 32, 48, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:lib.lab_lib:hello\n"
     ]
    }
   ],
   "source": [
    "# ipynb形式のライブラリノートを.py形式に変更したものをインポート\n",
    "import lib\n",
    "import lib.lab_lib\n",
    "from lib.lab_lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. ✅CG, MGで時間をExtra-Pから取得できることを確認\n",
    "2. ✅時間の単位を「秒」にする\n",
    "    * 列「Inclusive total msec」に一部存在している”：”で区切られた値の単位は、msec ではなく分と秒を区切る”：”\n",
    "3. ✅相対コストの算出関数の実装\n",
    "4. Extra-P のDocker環境の構築\n",
    "4. Extra-P による予測との組み合わせを行う\n",
    "    * なにをどう組み合わせるのかがわかっていないので、それは確かめる\n",
    "    * 元データ, Extra-P単体で予測したデータ, <何か> で結果を作成\n",
    "\n",
    "# 予測結果に必要なもの\n",
    "\n",
    "* 元データ（予測環境の生データ）\n",
    "* Extra-P単体で予測したデータ\n",
    "* Extra-Pで各関数の実行時間を予測し、それにコール回数を掛けた値で予測したデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/src/lib/lab_lib.py:8781: UserWarning: ./csv_files/cg_na75000_nonzer18_niter30_shift60_process4.csv is empty.\n",
      "  warnings.warn(f\"{filePath} is empty.\")\n",
      "/root/src/lib/lab_lib.py:8781: UserWarning: ./csv_files/cg_na75000_nonzer15_niter90_shift80_process8.csv is empty.\n",
      "  warnings.warn(f\"{filePath} is empty.\")\n"
     ]
    }
   ],
   "source": [
    "# 元データによる結果を取得する\n",
    "# ✅1. Extra-Pに読み込ますデータの取得\n",
    "# 2. 取得できた数式から予測\n",
    "\n",
    "\n",
    "input_list_process: list[int] = [2, 4, 8, 16, 32, 64, 128]\n",
    "cg_input_list_na: list[int] = [14000, 30000, 75000, 100000]\n",
    "cg_input_list_nonzer: list[int] = [11, 12, 13, 14, 15, 18]\n",
    "cg_input_list_niter: list[int] = [15, 30, 75, 90]\n",
    "cg_input_list_shift: list[int] = [20, 40, 60, 80, 110]\n",
    "\n",
    "str_ExtraPinputData :str = gen_ExtraPinputData(\n",
    "    benchmarkName = \"cg\",\n",
    "    conditions={\n",
    "        \"process\":input_list_process,\n",
    "        \"na\":cg_input_list_na,\n",
    "        \"nonzer\":cg_input_list_nonzer,\n",
    "        \"niter\":cg_input_list_niter,\n",
    "        \"shift\":cg_input_list_shift\n",
    "    }\n",
    ")\n",
    "\n",
    "filePath = \"./extra-p_docker/share/input_cg.txt\"\n",
    "with open(filePath, mode=\"w\") as f:\n",
    "    f.write(str_ExtraPinputData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mDockerfile\u001b[0m*    \u001b[34;42mcsv_files\u001b[0m/           \u001b[34;42mroot_jupyter\u001b[0m/       \u001b[01;32musedNote2.ipynb\u001b[0m*\n",
      "\u001b[01;32mPipfile\u001b[0m*       \u001b[01;32mdocker-compose.yml\u001b[0m*  \u001b[01;32mtesting_pri.ipynb\u001b[0m*  \u001b[01;32musedNote3.ipynb\u001b[0m*\n",
      "\u001b[01;32mPipfile.lock\u001b[0m*  \u001b[34;42mdocs\u001b[0m/                \u001b[01;32mtesting_pub.ipynb\u001b[0m*  \u001b[01;32musedNote4.ipynb\u001b[0m*\n",
      "\u001b[01;32mREADME.md\u001b[0m*     \u001b[01;34mextra-p_docker\u001b[0m/      \u001b[34;42mtxt_files\u001b[0m/          \u001b[34;42mused_scripts\u001b[0m/\n",
      "\u001b[34;42mapp\u001b[0m/           \u001b[34;42mlib\u001b[0m/                 \u001b[01;32musedNote.ipynb\u001b[0m*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Loading file: |          | [00:00<?]ERROR: The given file path is not valid.',\n",
       " '',\n",
       " 'Loading file: |          | [00:00<?]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sx pwd\n",
    "%sx which extrap\n",
    "%ls\n",
    "%sx extrap --text /workspace/extra-p_docker/share/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Inclusive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Inclusive'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m test_lulesh_iterations: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m256\u001b[39m]\n\u001b[1;32m      7\u001b[0m test_lulesh_sizes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m128\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m str_ExtraPinputData :\u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mgen_ExtraPinputData\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmarkName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlulesh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocess\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_lulesh_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miterations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_lulesh_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msizes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_lulesh_sizes\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m filePath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./extra-p_docker/share/input_lulesh.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filePath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/src/lib/lab_lib.py:9524\u001b[0m, in \u001b[0;36mgen_ExtraPinputData\u001b[0;34m(benchmarkName, conditions, csvDir, version, timeColumnName)\u001b[0m\n\u001b[1;32m   9515\u001b[0m target_rawDF_lulesh: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m return_rawDFinLULESH(\n\u001b[1;32m   9516\u001b[0m     processes\u001b[38;5;241m=\u001b[39mconditions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   9517\u001b[0m     iterations\u001b[38;5;241m=\u001b[39mconditions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   9518\u001b[0m     sizes\u001b[38;5;241m=\u001b[39mconditions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msizes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   9519\u001b[0m     csvDirPath\u001b[38;5;241m=\u001b[39mcsvDir,\n\u001b[1;32m   9520\u001b[0m )\n\u001b[1;32m   9521\u001b[0m target_rawDF_lulesh \u001b[38;5;241m=\u001b[39m target_rawDF_lulesh[\n\u001b[1;32m   9522\u001b[0m     target_rawDF_lulesh[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.TAU_application\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   9523\u001b[0m ]\n\u001b[0;32m-> 9524\u001b[0m target_rawDF_cg[timeColumnName_converted] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_rawDF_lulesh\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   9525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeColumnName\u001b[49m\n\u001b[1;32m   9526\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmap(convertPprofTime)\n\u001b[1;32m   9528\u001b[0m \u001b[38;5;66;03m# PARAMETER\u001b[39;00m\n\u001b[1;32m   9529\u001b[0m ss_PARAMETER: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Inclusive'"
     ]
    }
   ],
   "source": [
    "train_lulesh_processes: list[int] = [8, 27, 64, 125, 216, 343]\n",
    "train_lulesh_iterations: list[int] = [8, 16, 32, 64, 128]\n",
    "train_lulesh_sizes: list[int] = [16, 24, 32, 48, 64]\n",
    "\n",
    "test_lulesh_processes: list[int] = [512]\n",
    "test_lulesh_iterations: list[int] = [256]\n",
    "test_lulesh_sizes: list[int] = [128]\n",
    "\n",
    "str_ExtraPinputData :str = gen_ExtraPinputData(\n",
    "    benchmarkName=\"lulesh\",\n",
    "    conditions = {\n",
    "        \"process\" : train_lulesh_processes,\n",
    "        \"iterations\" : train_lulesh_iterations,\n",
    "        \"sizes\" : train_lulesh_sizes\n",
    "    },\n",
    "    version=2\n",
    ")\n",
    "\n",
    "filePath = \"./extra-p_docker/share/input_lulesh.txt\"\n",
    "with open(filePath, mode=\"w\") as f:\n",
    "    f.write(str_ExtraPinputData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF_lulesh :pd.DataFrame = return_rawDFinLULESH (\n",
    "    processes=train_lulesh_processes,\n",
    "    iterations=train_lulesh_iterations,\n",
    "    sizes=train_lulesh_sizes,\n",
    "    csvDirPath=csvDirPath\n",
    ")\n",
    "\n",
    "rawDF_lulesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rawDF_cg(\n",
    "    list_process :list[int],\n",
    "    list_iteration :list[int],\n",
    "    list_size: list[int],\n",
    "    csvDir :str\n",
    "):\n",
    "    \"\"\"ベンチマークプログラムLULESHのプロファイルを取得する関数\n",
    "    \n",
    "    Args:\n",
    "        list_process(list[int]):プロセス数のリスト\n",
    "        list_iteration(list[int]):初期変数iterationのリスト\n",
    "        list_size(list[int]):初期変数sizeのリスト\n",
    "        csvDir(str):CSVファイルを格納したディレクトリのパスを表す文字列\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    list_before_concat_DF: list[pd.DataFrame] = []\n",
    "    for elem_process in list_process:\n",
    "        for elem_iteration in list_iteration:\n",
    "            for elem_size in list_size:\n",
    "                filePath :str = f\"{csvDir}pprof_lulesh_p{elem_process}i{elem_iteration}s{elem_size}.csv\"\n",
    "                if os.path.isfile(filePath):\n",
    "                    try:\n",
    "                        DF_read_raw: pd.DataFrame = pd.read_csv(filePath)\n",
    "                        DF_read_raw[\"process\"] = elem_process\n",
    "                        DF_read_raw[\"iteration\"] = elem_iteration\n",
    "                        DF_read_raw[\"size\"] = elem_size\n",
    "                        list_before_concat_DF.appen(DF_read_raw)\n",
    "                    except:\n",
    "                        warnings.warn(f\"{filePath} is empty.\")\n",
    "                else:\n",
    "                    warnings.warn(f\"{filePath} doesn't exist\")\n",
    "    print(list_before_concat_DF)\n",
    "    return pd.concat(objs=list_before_concat_DF, axis=0)\n",
    "\n",
    "return_rawDF_cg(\n",
    "    list_process = test_lulesh_processes,\n",
    "    list_iteration = test_lulesh_iterations,\n",
    "    list_size = test_lulesh_sizes,\n",
    "    csvDir=csvDirPath\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "---\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2022年4月17日～\n",
    "\n",
    "次のような表を作成する\n",
    "\n",
    "採用される割合 (MAPE の最大値 [%] ，MAPE の最小値 [%]) [%]\n",
    "\n",
    "| ベンチマークプログラム名 | 線形モデル               | 対数モデル               | 反比例モデル              |\n",
    "|--------------|---------------------|---------------------|---------------------|\n",
    "| str          | float(float, float) | float(float, float) | float(float, float) |\n",
    "\n",
    "\n",
    "目標となるのは一気にこのベンチマークプログラムを作成することだが、既存のライブラリ関数などを利用し、まずはベンチマークごとに作成可能にする。\n",
    "\n",
    "メモ\n",
    "\n",
    "## 実装予定\n",
    "\n",
    "1. 行方向に最小値を検出\n",
    "2. 最小値以外をNaNに変更\n",
    "3. 列方向に最小値と最大値を検出\n",
    "\n",
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "---\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
