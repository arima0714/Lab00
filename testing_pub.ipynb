{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:hello\n"
     ]
    }
   ],
   "source": [
    "jupyter_pwd = %pwd\n",
    "if jupyter_pwd == \"/\":\n",
    "    %cd /workspace\n",
    "\n",
    "# %pdb on\n",
    "\n",
    "# ipynb形式のライブラリのインポート\n",
    "%run ./lib/lib.ipynb\n",
    "\n",
    "# ipynb形式のライブラリノートを.py形式に変更したものをインポート\n",
    "import lib\n",
    "import lib.lab_lib\n",
    "from lib.lab_lib import *\n",
    "\n",
    "# 生データの入ったCSVファイルの保持されたディレクトリ名を格納している変数\n",
    "csvDirPath = \"./csv_files/\"\n",
    "\n",
    "# NPBのベンチマーク名のリスト\n",
    "benchmarkNames = [\"cg\", \"ep\", \"ft\", \"is\", \"lu\", \"mg\"]\n",
    "\n",
    "# NPBのプロセス数\n",
    "npb_process :list[int] = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "train_npb_process :list[int] = npb_process[:-1]\n",
    "test_npb_process :list[int] = npb_process[-1:]\n",
    "# NPBのCGの初期変数\n",
    "cg_na: list[int] = [14000, 30000, 75000, 100000, 1500000]\n",
    "cg_nonzer: list[int] = [11, 12, 13, 14, 15, 18, 21]\n",
    "cg_niter: list[int] = [15, 30, 75, 90, 100]\n",
    "cg_shift: list[int] = [20, 40, 60, 80, 110, 200]\n",
    "\n",
    "train_cg_na: list[int] = cg_na[:-1]\n",
    "train_cg_nonzer: list[int] = cg_nonzer[:-1]\n",
    "train_cg_niter: list[int] = cg_niter[:-1]\n",
    "train_cg_shift: list[int] = cg_shift[:-1]\n",
    "\n",
    "test_cg_na: list[int] = cg_na[-1:]\n",
    "test_cg_nonzer: list[int] = cg_nonzer[-1:]\n",
    "test_cg_niter: list[int] = cg_niter[-1:]\n",
    "test_cg_shift: list[int] = cg_shift[-1:]\n",
    "\n",
    "# NPBのEPの初期変数\n",
    "train_ep_process :list[int] = [4, 8, 16, 32, 64]\n",
    "train_ep_size :list[str] = [\"S\", \"W\", \"A\", \"B\", \"C\"]\n",
    "test_ep_process :list[int] = [128, 256, 512]\n",
    "test_ep_size :list[str] = [\"D\", \"E\", \"F\"]\n",
    "\n",
    "# NPBのFTの初期変数\n",
    "train_ft_process :list[int] = [2, 4, 8, 16, 32, 64]\n",
    "train_ft_grid_size :list[int] = [32, 64, 128, 256, 512]\n",
    "train_ft_nit :list[int] = [5, 10, 15, 20, 25]\n",
    "\n",
    "test_ft_process :list[int] = [128, 256, 512]\n",
    "test_ft_grid_size :list[int] = [1024, 2048, 4096]\n",
    "test_ft_nit :list[int] = [30, 40, 50]\n",
    "\n",
    "# NPBのISの初期変数\n",
    "train_is_process :list[int] = [2, 4, 8, 16]\n",
    "train_is_no_of_keys :list[int] = [18, 20, 22, 24, 26]\n",
    "train_is_max_value :list[int] = [9, 11, 13, 15]\n",
    "\n",
    "test_is_process :list[int] = [32, 64, 128]\n",
    "test_is_no_of_keys :list[int] = [28, 29, 30]\n",
    "test_is_max_value :list[int] = [18, 19, 20]\n",
    "\n",
    "# NPBのMGの初期変数\n",
    "mg_size :list[int] = [32, 64, 128, 256, 512]\n",
    "mg_nit: list[int] = [4, 10, 20, 35, 50]\n",
    "\n",
    "train_mg_process :list[int] = [4, 8, 16, 32, 64]\n",
    "train_mg_size :list[int] = [4, 8, 16, 32, 64]\n",
    "train_mg_nit :list[int] = [5, 10, 15, 20, 25]\n",
    "\n",
    "test_mg_process :list[int] = [128, 256, 512]\n",
    "test_mg_size :list[int] = [128, 256, 512]\n",
    "test_mg_nit :list[int] = [30, 40, 50]\n",
    "\n",
    "# LULESH ベンチマークプログラムのプロセス数・問題サイズ・イテレーション数\n",
    "lulesh_processes: list[int] = [8, 27, 64, 125, 216, 343, 512]\n",
    "lulesh_iterations: list[int] = [8, 16, 32, 64, 128, 256]\n",
    "lulesh_sizes: list[int] = [16, 24, 32, 48, 64, 128]\n",
    "\n",
    "train_lulesh_processes: list[int] = [8, 27, 64, 125, 216, 343]\n",
    "train_lulesh_iterations: list[int] = [8, 16, 32, 64, 128]\n",
    "train_lulesh_sizes: list[int] = [16, 24, 32, 48]\n",
    "\n",
    "test_lulesh_processes: list[int] = [512, 729, 1000]\n",
    "test_lulesh_iterations: list[int] = [256, 512, 1024]\n",
    "test_lulesh_sizes: list[int] = [64, 96, 128]\n",
    "\n",
    "# Extra-Pのオプション\n",
    "modelerNames: list[str] = [\n",
    "    # \"refining\", \n",
    "    \"multi-parameter\",\n",
    "    \"default\", \n",
    "    # \"basic --options poly_exponents=-1,0,1,2,3 log_exponents=0,1 force_combination_exponents=1 allow_negative_exponents=1\"\n",
    "    ]\n",
    "\n",
    "modelerOption: str = \"\"\" --options \\#spm=Basic \\#spo=poly_exponents=-1,0,1,2,3,log_exponents=0,1,force_combination_exponents=1,allow_negative_exponents=True\"\"\"\n",
    "\n",
    "list_csvDir = [\n",
    "    \"./csv_files/lulesh_1st/\",\n",
    "    \"./csv_files/lulesh_2nd/\",\n",
    "    \"./csv_files/lulesh_3rd/\",\n",
    "]\n",
    "\n",
    "list_csvDir_mg = [\n",
    "    \"./csv_files/mg_1st/\",\n",
    "    \"./csv_files/mg_2nd/\",\n",
    "    \"./csv_files/mg_3rd/\",\n",
    "]\n",
    "\n",
    "list_csvDir_ft :list[str] = [\n",
    "    \"./csv_files/ft_1st/\",\n",
    "    \"./csv_files/ft_2nd/\",\n",
    "    \"./csv_files/ft_3rd/\",\n",
    "]\n",
    "\n",
    "list_csvDir_ep :list[str] = [\n",
    "    \"./csv_files/ep_1st/\",\n",
    "    \"./csv_files/ep_2nd/\",\n",
    "    \"./csv_files/ep_3rd/\",\n",
    "]\n",
    "\n",
    "list_csvDir_is :list[str] = [\n",
    "    \"./csv_files/is_1st/\",\n",
    "    \"./csv_files/is_2nd/\",\n",
    "    \"./csv_files/is_3rd/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rawDF_is(\n",
    "    list_process: list[int],\n",
    "    list_no_of_keys: list[int],\n",
    "    list_key_max_value :list[int],\n",
    "    csvDir: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"return_rawDF_is()\n",
    "\n",
    "    ベンチマークプログラムISの手動で変更した初期変数におけるプロファイルを取得する関数\n",
    "\n",
    "    Args:\n",
    "        list_process(list[int]):プロセス数のリスト\n",
    "        list_no_of_keys(list[int]):初期変数no_of_keysのリスト\n",
    "        list_key_max_value(list[int]):初期変数key_max_valueのリスト\n",
    "        csvDir(str):CSVファイルの保持されているディレクトリ\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    list_before_concat_DF: list[pd.DataFrame] = []\n",
    "\n",
    "    for elem_process in list_process:\n",
    "        for elem_no_of_keys in list_no_of_keys:\n",
    "            for elem_key_max_value in list_key_max_value:\n",
    "                filePath: str = f\"{csvDir}is_no_of_keys{elem_no_of_keys}_key_max_value{elem_key_max_value}_process{elem_process}.csv\"\n",
    "                if os.path.isfile(filePath):\n",
    "                    try:\n",
    "                        DF_read_raw: pd.DataFrame = pd.read_csv(filePath)\n",
    "                        DF_read_raw[\"process\"] = elem_process\n",
    "                        DF_read_raw[\"no_of_keys\"] = elem_no_of_keys\n",
    "                        DF_read_raw[\"key_max_value\"] = elem_key_max_value\n",
    "                        list_before_concat_DF.append(DF_read_raw)\n",
    "                    except:\n",
    "                        warnings.warn(f\"{filePath} is empty.\")\n",
    "                else:\n",
    "                    warnings.warn(f\"{filePath} doesn't exist\")\n",
    "                    continue\n",
    "    return pd.concat(objs=list_before_concat_DF, axis=0)\n",
    "\n",
    "\n",
    "def ret_averaged_rawDF_is(\n",
    "    list_process: list[int],\n",
    "    list_no_of_keys: list[int],\n",
    "    list_key_max_value :list[int],\n",
    "    list_csvDir: list[str],\n",
    "    resVar: str,\n",
    "):\n",
    "    \"\"\"複数のCSVからDFを取得する関数（ベンチマークプログラムEP）\n",
    "\n",
    "    列Inclusiveおよび列Exclusiveが秒に変換され、InclusivePerCallもしくはExclusivePerCall列が生成される。\n",
    "\n",
    "    Args:\n",
    "        list_process(list[int]):プロセス数のリスト\n",
    "        list_no_of_keys(list[int]):初期変数no_of_keysのリスト\n",
    "        list_key_max_value(list[int]):初期変数key_max_valueのリスト\n",
    "        list_csvDir(list[str]):CSVを保持したディレクトリ名のリスト\n",
    "        resVar(str):説明変数の文字列\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    list_DFs_for_return: list[pd.DataFrame] = []\n",
    "    for elem_process in list_process:\n",
    "        for elem_no_of_keys in list_no_of_keys:\n",
    "            for elem_key_max_value in list_key_max_value:\n",
    "                list_inputDFs_for_averaged: list[pd.DataFrame] = []\n",
    "                for elem_csvDir in list_csvDir:\n",
    "                    try:\n",
    "                        _raw_DF: pd.DataFrame = return_rawDF_is(\n",
    "                            list_process=[elem_process],\n",
    "                            list_no_of_keys=[elem_no_of_keys],\n",
    "                            list_key_max_value=[elem_key_max_value],\n",
    "                            csvDir=elem_csvDir,\n",
    "                        )\n",
    "\n",
    "                        if resVar in [\"Exclusive\", \"Inclusive\", \"#Call\", \"#Subrs\"]:\n",
    "                            # resVar 列の整形\n",
    "                            if resVar in [\"Exclusive\", \"Inclusive\"]:\n",
    "                                _tmp_converted = map(\n",
    "                                    convertPprofTime, list(_raw_DF[resVar])\n",
    "                                )\n",
    "                                _raw_DF[resVar] = list(_tmp_converted)\n",
    "                            # {resVar}PerCall 列の生成\n",
    "                            _raw_DF = add_perCallColumn(\n",
    "                                inputDF=_raw_DF,\n",
    "                                divisorColName=\"#Call\",\n",
    "                                dividendColName=resVar,\n",
    "                                targetColumnName=f\"{resVar}PerCall\",\n",
    "                            )\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    list_inputDFs_for_averaged.append(_raw_DF)\n",
    "                list_DFs_for_return.append(\n",
    "                    ret_averagedDF(inputDFs=list_inputDFs_for_averaged, resVar=resVar)\n",
    "                )\n",
    "    return pd.concat(objs=list_DFs_for_return, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF_FT :pd.DataFrame = ret_averaged_rawDF_ft(list_process=train_ft_process, list_grid_size=train_ft_grid_size, list_nit=train_ft_nit, list_csvDir=list_csvDir_ft, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "testDF_FT :pd.DataFrame = ret_averaged_rawDF_ft(list_process=test_ft_process, list_grid_size=test_ft_grid_size, list_nit=test_ft_nit, list_csvDir=list_csvDir_ft, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "trainDF_MG :pd.DataFrame = ret_averaged_rawDF_mg(list_process=train_mg_process, list_size=train_mg_size, list_nit=train_mg_nit, list_csvDir=list_csvDir_mg, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "testDF_MG :pd.DataFrame = ret_averaged_rawDF_mg(list_process=test_mg_process, list_size=test_mg_size, list_nit=test_mg_nit, list_csvDir=list_csvDir_mg, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "trainDF_EP :pd.DataFrame = ret_averaged_rawDF_ep(list_process=train_ep_process, list_size=train_ep_size, list_csvDir=list_csvDir_ep, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "testDF_EP :pd.DataFrame = ret_averaged_rawDF_ep(list_process=test_ep_process, list_size=test_ep_size, list_csvDir=list_csvDir_ep, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "trainDF_IS :pd.DataFrame = ret_averaged_rawDF_is(list_process=train_is_process, list_key_max_value=train_is_max_value, list_no_of_keys=train_is_no_of_keys, list_csvDir=list_csvDir_is, resVar=\"Exclusive\")\n",
    "\n",
    "\n",
    "testDF_IS :pd.DataFrame = ret_averaged_rawDF_is(list_process=test_is_process, list_key_max_value=test_is_max_value, list_no_of_keys=test_is_no_of_keys, list_csvDir=list_csvDir_is, resVar=\"Exclusive\")\n",
    "\n",
    "expVars_ep :list[str] = [\"process\", \"size\"]\n",
    "expVars_ft :list[str] = [\"process\", \"grid_size\", \"nit\"]\n",
    "expVars_is :list[str] = [\"process\", \"no_of_keys\", \"key_max_value\"]\n",
    "expVars_mg :list[str] = [\"process\", \"problem_size\", \"nit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   %Time  Exclusive Inclusive   #Call    #Subrs              Name  process  \\\n",
      "0  100.0   0.000006    54,584     1.0       1.0  .TAU_application      128   \n",
      "1  100.0  24.626000    54,584     1.0  237605.0             EMBAR      128   \n",
      "2   49.2   9.965000    26,845     4.0       0.0   MPI_Allreduce()      128   \n",
      "3    2.8   1.073333     1,536  8194.0       0.0            VRANLC      128   \n",
      "4    1.4   0.656000       747     1.0       0.0        MPI_Init()      128   \n",
      "\n",
      "  size  ExclusivePerCall  \n",
      "0    D          0.000006  \n",
      "1    D         24.912000  \n",
      "2    D          6.711250  \n",
      "3    D          0.000187  \n",
      "4    D          0.747000  \n",
      "   %Time  Exclusive Inclusive       #Call        #Subrs  \\\n",
      "0  100.0   0.000007    11,173         1.0  1.000000e+00   \n",
      "1  100.0   0.000095    11,173         1.0  3.306250e+01   \n",
      "2   73.6   3.485000     8,218         1.0  3.355440e+07   \n",
      "3   42.3   4.719667     4,728  33554500.0  0.000000e+00   \n",
      "4   10.6   0.872667     1,189        11.0  3.300000e+01   \n",
      "\n",
      "                               Name  process  no_of_keys  key_max_value  \\\n",
      "0                  .TAU_application       32          28             18   \n",
      "1             int_main(int_char_**)       32          28             18   \n",
      "2    void_create_seq(double_double)       32          28             18   \n",
      "3  double_randlc(double_*_double_*)       32          28             18   \n",
      "4                    void_rank(int)       32          28             18   \n",
      "\n",
      "   ExclusivePerCall  \n",
      "0      7.410000e-06  \n",
      "1      9.990000e-05  \n",
      "2      3.490000e+00  \n",
      "3      1.409051e-07  \n",
      "4      8.000000e-02  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(testDF_EP.head())\n",
    "print(testDF_IS.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必要な結果\n",
    "\n",
    "* 関数コール回数予測\n",
    "    * モデル適合度(MAPE表)\n",
    "    * モデル予測精度(MAPEが縦軸のグラフ、重み付きMAPEが縦軸のグラフ)\n",
    "* 実行時間予測\n",
    "    * モデル適合度(MAPE表)\n",
    "    * モデル予測精度(MAPEが縦軸のグラフ、重み付きMAPEが縦軸のグラフ)\n",
    "\n",
    "# 必要なブツ\n",
    "\n",
    "* ✅モデル構築用データ\n",
    "* ✅予測対象用データ\n",
    "* ✅関数コール回数の予測モデル\n",
    "* ✅関数の総実行時間の予測モデル\n",
    "* ✅1コール当たりの関数の総実行時間の予測モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentsForExtraP:\n",
    "    \"\"\"class ContentsForExtraP\n",
    "\n",
    "    ExtraPによるモデルの生成\n",
    "\n",
    "    生成されるモデルは次の通り。\n",
    "\n",
    "    * 関数コール回数の予測モデル\n",
    "    * 関数の総実行時間の予測モデル\n",
    "    * 1コール当たりの関数の総実行時間の予測モデル\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trainDF: pd.DataFrame,\n",
    "        testDF: pd.DataFrame,\n",
    "        expVars: list[str],\n",
    "        resVar: str,\n",
    "        resVarPerCall: str,\n",
    "        benchmarkName :str,\n",
    "        modelerName :str = \"multi-parameter\",\n",
    "        modelerOption :str = \"\"\" --options \\#spm=Basic \\#spo=poly_exponents=-1,0,1,2,3,log_exponents=0,1,force_combination_exponents=1,allow_negative_exponents=True\"\"\"\n",
    "    ):\n",
    "        \"\"\"__init__()\n",
    "\n",
    "        初期化変数\n",
    "\n",
    "        Args:\n",
    "            trainDF(pd.DataFrame):モデル構築用データ\n",
    "            testDF(pd.DataFrame):予測対象用データ\n",
    "            expVars(list[str]):説明変数を格納したリスト\n",
    "            resVar(str):目的変数を示した文字列\n",
    "            resVarsPerCall(str):1コール当たりの目的変数を示した文字列\n",
    "\n",
    "        \"\"\"\n",
    "        self.trainDF = trainDF.reset_index()\n",
    "        self.testDF = testDF.reset_index()\n",
    "        self.expVars = expVars\n",
    "        self.resVar = resVar\n",
    "        self.resVarPerCall = resVarPerCall\n",
    "        self.functionNames :list[str] = sorted(list(set(trainDF[\"Name\"].to_list())))\n",
    "        self.benchmarkName :str = benchmarkName\n",
    "        self.modelerName :str = modelerName\n",
    "        self.modelerOption :str = modelerOption\n",
    "        self.dict_symbols :dict[str, str] = {}\n",
    "        for elem in expVars:\n",
    "            self.dict_symbols[elem] = symbols(elem, real=True)\n",
    "        \n",
    "        self.dict_resVar = {}\n",
    "        self.dict_resVarPerCall = {}\n",
    "        self.dict_call = {}\n",
    "\n",
    "    def build_all_models(self):\n",
    "        \"\"\"build_all_models()\n",
    "\n",
    "        モデルを構築する関数。目的変数はself.resVars, self.resVarsPerCallを利用。\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        for functionName in self.functionNames:\n",
    "            trainDF_perFunc :pd.DataFrame = self.trainDF[self.trainDF[\"Name\"] == functionName]\n",
    "\n",
    "            model_resVar :str = get_ExtraP_model(\n",
    "                inputDF_perFunc=trainDF_perFunc,\n",
    "                expVar = self.expVars,\n",
    "                resVar = self.resVar,\n",
    "                functionName = functionName,\n",
    "                dict_symbols=self.dict_symbols,\n",
    "                benchmarkName = self.benchmarkName,\n",
    "                modelerName = self.modelerName,\n",
    "                modelerOption = self.modelerOption\n",
    "            )\n",
    "\n",
    "            model_resVarPerCall :str = get_ExtraP_model(\n",
    "                inputDF_perFunc=trainDF_perFunc,\n",
    "                expVar = self.expVars,\n",
    "                resVar = self.resVarPerCall,\n",
    "                functionName = functionName,\n",
    "                dict_symbols=self.dict_symbols,\n",
    "                benchmarkName = self.benchmarkName,\n",
    "                modelerName = self.modelerName,\n",
    "                modelerOption = self.modelerOption\n",
    "            )\n",
    "\n",
    "            model_call :str = get_ExtraP_model(\n",
    "                inputDF_perFunc=trainDF_perFunc,\n",
    "                expVar = self.expVars,\n",
    "                resVar = \"#Call\",\n",
    "                functionName = functionName,\n",
    "                dict_symbols=self.dict_symbols,\n",
    "                benchmarkName = self.benchmarkName,\n",
    "                modelerName = self.modelerName,\n",
    "                modelerOption = self.modelerOption\n",
    "            )\n",
    "\n",
    "            self.dict_resVar[functionName] = model_resVar\n",
    "            self.dict_resVarPerCall[functionName] = model_resVarPerCall\n",
    "            self.dict_call[functionName] = model_call\n",
    "\n",
    "    def predict_train(self):\n",
    "        \"\"\"predict_train(self)\n",
    "\n",
    "        学習データに対して予測を実施する関数\n",
    "\n",
    "        列構成は下記の通り\n",
    "\n",
    "        |<expVar>|Name(関数名)|#Call(コール回数実測値)|predicted_resVar（）|predicted_ExclusivePerCall\tpredicted_#Call>|<>|\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        _list_series :list[pd.Series] = []\n",
    "        for index, _sr in self.trainDF.iterrows():\n",
    "            _series :pd.Series = pd.Series(dtype=\"object\")\n",
    "            functionName :str = _sr[\"Name\"]\n",
    "\n",
    "            _target_env :list[set[any]] = []\n",
    "            for _expVar in self.expVars:\n",
    "                _series[_expVar] = _sr[_expVar]\n",
    "                _target_env.append((self.dict_symbols[_expVar], _sr[_expVar]))\n",
    "\n",
    "            _series[\"Name\"] = _sr[\"Name\"]\n",
    "            _series[\"#Call\"] = _sr[\"#Call\"]\n",
    "            _series[f\"{self.resVar}\"] = _sr[f\"{self.resVar}\"]\n",
    "\n",
    "            _predicted_resVar: float\n",
    "            _predicted_resVarPerCall :float\n",
    "            _predicted_call :float\n",
    "\n",
    "            _predicted_resVar = self.dict_resVar[functionName].subs(_target_env).evalf()\n",
    "            _predicted_resVarPerCall = self.dict_resVarPerCall[functionName].subs(_target_env).evalf()\n",
    "            _predicted_call = self.dict_call[functionName].subs(_target_env).evalf()\n",
    "\n",
    "            _series[f\"predicted_{self.resVar}\"] = _predicted_resVar\n",
    "            _series[f\"predicted_{self.resVarPerCall}\"] = _predicted_resVarPerCall\n",
    "            _series[f\"predicted_#Call\"] = _predicted_call\n",
    "            _series[f\"predicted_{self.resVar}_indirectly\"] = _predicted_resVarPerCall * _predicted_call\n",
    "\n",
    "            _list_series.append(_series)\n",
    "\n",
    "        self.DF_predicted_by_train :pd.DataFrame = pd.DataFrame(data=_list_series)\n",
    "\n",
    "    def predict_test(self):\n",
    "        \"\"\"predict_test(self)\n",
    "        \n",
    "        予測対象データに対して予測を実施する関数\n",
    "        \"\"\"\n",
    "\n",
    "        _list_series :list[pd.Series] = []\n",
    "\n",
    "        for index, _sr in self.testDF.iterrows():\n",
    "            _series :pd.Series = pd.Series(dtype=\"object\")\n",
    "            functionName :str = _sr[\"Name\"]\n",
    "\n",
    "            _target_env :list[set[any]] = []\n",
    "            for _expVar in self.expVars:\n",
    "                _series[_expVar] = _sr[_expVar]\n",
    "                _target_env.append((self.dict_symbols[_expVar], _sr[_expVar]))\n",
    "\n",
    "            _series[\"Name\"] = functionName\n",
    "            _series[\"#Call\"] = _sr[\"#Call\"]\n",
    "            _series[f\"{self.resVar}\"] = _sr[f\"{self.resVar}\"]\n",
    "\n",
    "            _predicted_resVar :float = self.dict_resVar[functionName].subs(_target_env).evalf()\n",
    "            _predicted_resVarPerCall :float = self.dict_resVarPerCall[functionName].subs(_target_env).evalf()\n",
    "            _predicted_call :float = self.dict_call[functionName].subs(_target_env).evalf()\n",
    "\n",
    "            _series[f\"predicted_{self.resVar}\"] = _predicted_resVar\n",
    "            _series[f\"predicted_{self.resVarPerCall}\"] = _predicted_resVarPerCall\n",
    "            _series[f\"predicted_#Call\"] = _predicted_call\n",
    "            _series[f\"predicted_{self.resVar}_indirectly\"] = _predicted_resVarPerCall * _predicted_call\n",
    "\n",
    "            _list_series.append(_series)\n",
    "\n",
    "        self.DF_predicted_by_test :pd.DataFrame = pd.DataFrame(data=_list_series)\n",
    "\n",
    "    def get_result_of_predict_train(self):\n",
    "        return self.DF_predicted_by_train\n",
    "\n",
    "    def get_DF_all_fitness_with_relativeErrorRate(self) -> pd.DataFrame:\n",
    "        \"\"\"get_DF_all_fitness_with_relativeErrorRate()\n",
    "        \n",
    "        モデル適合度のテーブルを返す関数\n",
    "\n",
    "        列APE～の平均をとるとMAPEになる。\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        _returnDF :pd.DataFrame = add_relativeErrorRateCol(\n",
    "            inputDF = self.DF_predicted_by_train,\n",
    "            real_colName=\"#Call\",\n",
    "            predicted_colName=\"predicted_#Call\",\n",
    "            targetColName=\"APE_#Call\"\n",
    "        )\n",
    "        _returnDF = add_relativeErrorRateCol(\n",
    "            inputDF = _returnDF,\n",
    "            real_colName=f\"{self.resVar}\",\n",
    "            predicted_colName=f\"predicted_{self.resVar}\",\n",
    "            targetColName=f\"APE_{self.resVar}\",\n",
    "        )\n",
    "        _returnDF = add_relativeErrorRateCol(\n",
    "            inputDF = _returnDF,\n",
    "            real_colName=f\"{self.resVar}\",\n",
    "            predicted_colName=f\"predicted_{self.resVar}_indirectly\",\n",
    "            targetColName=f\"APE_{self.resVarPerCall}\",\n",
    "        )\n",
    "\n",
    "        return _returnDF\n",
    "\n",
    "    def get_DF_weightedMAPE(self) -> pd.DataFrame:\n",
    "        \"\"\"get_DF_all_fitness_with_weightedMAPE()\"\"\"\n",
    "        _dict_expVar_set :dict[str, set[int]]= {}\n",
    "        for _expVar in self.expVars:\n",
    "            _dict_expVar_set[_expVar] = set(sorted(self.testDF[_expVar]))\n",
    "\n",
    "        if len(self.expVars) == 2:\n",
    "            _product = itertools.product(\n",
    "                _dict_expVar_set[self.expVars[0]],\n",
    "                _dict_expVar_set[self.expVars[1]]\n",
    "            )\n",
    "        elif len(self.expVars) == 3:\n",
    "            _product = itertools.product(\n",
    "                _dict_expVar_set[self.expVars[0]],\n",
    "                _dict_expVar_set[self.expVars[1]],\n",
    "                _dict_expVar_set[self.expVars[2]]\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(f\"self.expVars == {len(self.expVars)}\")\n",
    "            return -1\n",
    "\n",
    "        def ret_query (list_expVar :list[str], list_product :list[int]) -> str:\n",
    "            retStr :str= \"\"\n",
    "            for i_expVar in range(len(list_expVar)):\n",
    "                retStr += f\"{list_expVar[i_expVar]}=={list_product[i_expVar]}\"\n",
    "                if i_expVar != len(list_expVar)-1:\n",
    "                    retStr += \" & \"\n",
    "            return retStr\n",
    "\n",
    "        _list_series :pd.Series = []\n",
    "        for _elem_product in _product:\n",
    "\n",
    "            _query :str = ret_query(list_expVar = self.expVars, list_product=list(_elem_product))\n",
    "\n",
    "            _targetDF :pd.DataFrame = self.testDF.query(_query)\n",
    "            _target_env :list[set[any]] = []\n",
    "\n",
    "            _series :pd.Series = pd.Series(dtype=\"object\")\n",
    "\n",
    "            for i_expVar in range(len(self.expVars)):\n",
    "                _target_env.append((self.dict_symbols[self.expVars[i_expVar]], _elem_product[i_expVar]))\n",
    "                _series[self.expVars[i_expVar]] = _elem_product[i_expVar]\n",
    "\n",
    "            _c_sum :float = sum(_targetDF[\"#Call\"].tolist())\n",
    "            _target_weightedMAPE_resVar :float = 0.0\n",
    "            _target_weightedMAPE_resVarPerCall :float = 0.0\n",
    "            _target_weightedMAPE_call :float = 0.0\n",
    "            for i, sr in _targetDF.iterrows():\n",
    "                _c_t :float = sr[\"#Call\"]\n",
    "                functionName :str = sr[\"Name\"]\n",
    "\n",
    "                _A_t_resVar :float = sr[self.resVar]\n",
    "                _A_t_resVarPerCall :float = sr[self.resVarPerCall]\n",
    "                _A_t_call :float = sr[\"#Call\"]\n",
    "\n",
    "                _F_t_resVar :float = self.dict_resVar[functionName].subs(_target_env).evalf()\n",
    "                _F_t_resVarPerCall :float = self.dict_resVarPerCall[functionName].subs(_target_env).evalf()\n",
    "                _F_t_call :float = self.dict_call[functionName].subs(_target_env).evalf()\n",
    "\n",
    "                \n",
    "                _target_weightedMAPE_resVar += abs(_A_t_resVar - _F_t_resVar)/_A_t_resVar\n",
    "                _target_weightedMAPE_resVarPerCall += abs(_A_t_resVarPerCall - _F_t_resVarPerCall)/_A_t_resVarPerCall\n",
    "                _target_weightedMAPE_call += abs(_A_t_call - _F_t_call)/_A_t_call\n",
    "\n",
    "            _target_weightedMAPE_resVar *= 100/_c_sum\n",
    "            _target_weightedMAPE_resVarPerCall *= 100/_c_sum\n",
    "            _target_weightedMAPE_call *= 100/_c_sum\n",
    "            \n",
    "            _series[f\"weightedMAPE_{self.resVar}\"] = _target_weightedMAPE_resVar\n",
    "            _series[f\"weightedMAPE_{self.resVarPerCall}\"] = _target_weightedMAPE_resVarPerCall\n",
    "            _series[f\"weightedMAPE_call\"] = _target_weightedMAPE_call\n",
    "\n",
    "            _list_series.append(_series)\n",
    "        \n",
    "        self.DF_weightedMAPE :pd.DataFrame = pd.DataFrame(data=_list_series)\n",
    "        return self.DF_weightedMAPE\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dt_now = datetime.datetime.now()\n",
    "\n",
    "benchmarkName :str = \"mg\"\n",
    "\n",
    "mg_contents = ContentsForExtraP(trainDF = trainDF_MG, testDF= testDF_MG, resVar = \"Exclusive\", expVars= expVars_mg ,resVarPerCall = \"ExclusivePerCall\", benchmarkName = \"mg\")\n",
    "mg_contents.build_all_models()\n",
    "mg_contents.predict_train()\n",
    "mg_contents.predict_test()\n",
    "mg_適合度 :pd.DataFrame = mg_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "mg_重み付きMAPE :pd.DataFrame = mg_contents.get_DF_weightedMAPE()\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    mg_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    mg_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filePath = './extra-p_docker/share/input_ep_Exclusive@-TAU_application.txt'\n",
      "modelerName = 'multi-parameter'\n",
      "modelerOption = ' --options \\\\#spm=Basic \\\\#spo=poly_exponents=-1,0,1,2,3,log_exponents=0,1,force_combination_exponents=1,allow_negative_exponents=True'\n",
      "str_ExtraPinputData = 'PARAMETER process\\nPARAMETER size\\n\\nPOINTS ( 4 S )\\nPOINTS ( 4 W )\\nPOINTS ( 4 A )\\nPOINTS ( 4 B )\\nPOINTS ( 4 C )\\nPOINTS ( 8 S )\\nPOINTS ( 8 W )\\nPOINTS ( 8 A )\\nPOINTS ( 8 B )\\nPOINTS ( 8 C )\\nPOINTS ( 16 S )\\nPOINTS ( 16 W )\\nPOINTS ( 16 A )\\nPOINTS ( 16 B )\\nPOINTS ( 16 C )\\nPOINTS ( 32 S )\\nPOINTS ( 32 W )\\nPOINTS ( 32 A )\\nPOINTS ( 32 B )\\nPOINTS ( 32 C )\\nPOINTS ( 64 S )\\nPOINTS ( 64 W )\\nPOINTS ( 64 A )\\nPOINTS ( 64 B )\\nPOINTS ( 64 C )\\n\\nREGION reg\\nMETRIC time\\n\\nDATA 7.75e-06 7.75e-06 7.75e-06\\nDATA 7.499999999999999e-06 7.499999999999999e-06 7.499999999999999e-06\\nDATA 7e-06 7e-06 7e-06\\nDATA 7e-06 7e-06 7e-06\\nDATA 7.166666666666667e-06 7.166666666666667e-06 7.166666666666667e-06\\nDATA 7.499999999999999e-06 7.499999999999999e-06 7.499999999999999e-06\\nDATA 7.333333333333333e-06 7.333333333333333e-06 7.333333333333333e-06\\nDATA 7.413333333333332e-06 7.413333333333332e-06 7.413333333333332e-06\\nDATA 8.793333333333333e-06 8.793333333333333e-06 8.793333333333333e-06\\nDATA 7.499999999999999e-06 7.499999999999999e-06 7.499999999999999e-06\\nDATA 7.086666666666667e-06 7.086666666666667e-06 7.086666666666667e-06\\nDATA 6.976666666666666e-06 6.976666666666666e-06 6.976666666666666e-06\\nDATA 7.146666666666667e-06 7.146666666666667e-06 7.146666666666667e-06\\nDATA 7.043333333333333e-06 7.043333333333333e-06 7.043333333333333e-06\\nDATA 6.9400000000000005e-06 6.9400000000000005e-06 6.9400000000000005e-06\\nDATA 7.2199999999999995e-06 7.2199999999999995e-06 7.2199999999999995e-06\\nDATA 7.236666666666667e-06 7.236666666666667e-06 7.236666666666667e-06\\nDATA 7.426666666666667e-06 7.426666666666667e-06 7.426666666666667e-06\\nDATA 7.323333333333334e-06 7.323333333333334e-06 7.323333333333334e-06\\nDATA 7.043333333333333e-06 7.043333333333333e-06 7.043333333333333e-06\\nDATA 6.010000000000001e-06 6.010000000000001e-06 6.010000000000001e-06\\nDATA 6.183333333333332e-06 6.183333333333332e-06 6.183333333333332e-06\\nDATA 5.9e-06 5.9e-06 5.9e-06\\nDATA 6.010000000000001e-06 6.010000000000001e-06 6.010000000000001e-06\\nDATA 5.9766666666666665e-06 5.9766666666666665e-06 5.9766666666666665e-06\\n'\n"
     ]
    },
    {
     "ename": "SympifyError",
     "evalue": "Sympify of expression 'could not parse ''' failed, because of exception being raised:\nSyntaxError: invalid syntax (<string>, line 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mValueError\u001b[0m: Error from parse_expr with transformed code: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sympy/core/sympify.py:496\u001b[0m, in \u001b[0;36msympify\u001b[0;34m(a, locals, convert_xor, strict, rational, evaluate)\u001b[0m\n\u001b[1;32m    495\u001b[0m     a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 496\u001b[0m     expr \u001b[39m=\u001b[39m parse_expr(a, local_dict\u001b[39m=\u001b[39;49m\u001b[39mlocals\u001b[39;49m, transformations\u001b[39m=\u001b[39;49mtransformations, evaluate\u001b[39m=\u001b[39;49mevaluate)\n\u001b[1;32m    497\u001b[0m \u001b[39mexcept\u001b[39;00m (TokenError, \u001b[39mSyntaxError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1101\u001b[0m, in \u001b[0;36mparse_expr\u001b[0;34m(s, local_dict, transformations, global_dict, evaluate)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     local_dict[i] \u001b[39m=\u001b[39m null\n\u001b[0;32m-> 1101\u001b[0m \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError from parse_expr with transformed code: \u001b[39m\u001b[39m{\u001b[39;00mcode\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1092\u001b[0m, in \u001b[0;36mparse_expr\u001b[0;34m(s, local_dict, transformations, global_dict, evaluate)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1092\u001b[0m     rv \u001b[39m=\u001b[39m eval_expr(code, local_dict, global_dict)\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# restore neutral definitions for names\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:907\u001b[0m, in \u001b[0;36meval_expr\u001b[0;34m(code, local_dict, global_dict)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39mEvaluate Python code generated by ``stringify_expr``.\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \n\u001b[1;32m    905\u001b[0m \u001b[39mGenerally, ``parse_expr`` should be used.\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m expr \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m(\n\u001b[1;32m    908\u001b[0m     code, global_dict, local_dict)  \u001b[39m# take local objects in preference\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[39mreturn\u001b[39;00m expr\n",
      "\u001b[0;31mSyntaxError\u001b[0m: invalid syntax (<string>, line 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSympifyError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m benchmarkName :\u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mep\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m ep_contents \u001b[39m=\u001b[39m ContentsForExtraP(trainDF \u001b[39m=\u001b[39m trainDF_EP, testDF\u001b[39m=\u001b[39m testDF_EP, resVar \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mExclusive\u001b[39m\u001b[39m\"\u001b[39m, expVars\u001b[39m=\u001b[39m expVars_ep ,resVarPerCall \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mExclusivePerCall\u001b[39m\u001b[39m\"\u001b[39m, benchmarkName \u001b[39m=\u001b[39m benchmarkName)\n\u001b[0;32m----> 6\u001b[0m ep_contents\u001b[39m.\u001b[39;49mbuild_all_models()\n\u001b[1;32m      7\u001b[0m ep_contents\u001b[39m.\u001b[39mpredict_train()\n\u001b[1;32m      8\u001b[0m ep_contents\u001b[39m.\u001b[39mpredict_test()\n",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m, in \u001b[0;36mContentsForExtraP.build_all_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m functionName \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunctionNames:\n\u001b[1;32m     62\u001b[0m     trainDF_perFunc :pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainDF[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainDF[\u001b[39m\"\u001b[39m\u001b[39mName\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m functionName]\n\u001b[0;32m---> 64\u001b[0m     model_resVar :\u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m get_ExtraP_model(\n\u001b[1;32m     65\u001b[0m         inputDF_perFunc\u001b[39m=\u001b[39;49mtrainDF_perFunc,\n\u001b[1;32m     66\u001b[0m         expVar \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpVars,\n\u001b[1;32m     67\u001b[0m         resVar \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresVar,\n\u001b[1;32m     68\u001b[0m         functionName \u001b[39m=\u001b[39;49m functionName,\n\u001b[1;32m     69\u001b[0m         dict_symbols\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdict_symbols,\n\u001b[1;32m     70\u001b[0m         benchmarkName \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbenchmarkName,\n\u001b[1;32m     71\u001b[0m         modelerName \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodelerName,\n\u001b[1;32m     72\u001b[0m         modelerOption \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodelerOption\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     75\u001b[0m     model_resVarPerCall :\u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m get_ExtraP_model(\n\u001b[1;32m     76\u001b[0m         inputDF_perFunc\u001b[39m=\u001b[39mtrainDF_perFunc,\n\u001b[1;32m     77\u001b[0m         expVar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpVars,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m         modelerOption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodelerOption\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     model_call :\u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m get_ExtraP_model(\n\u001b[1;32m     87\u001b[0m         inputDF_perFunc\u001b[39m=\u001b[39mtrainDF_perFunc,\n\u001b[1;32m     88\u001b[0m         expVar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpVars,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m         modelerOption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodelerOption\n\u001b[1;32m     95\u001b[0m     )\n",
      "File \u001b[0;32m~/src/lib/lab_lib.py:10213\u001b[0m, in \u001b[0;36mget_ExtraP_model\u001b[0;34m(inputDF_perFunc, expVar, resVar, functionName, dict_symbols, benchmarkName, modelerName, modelerOption)\u001b[0m\n\u001b[1;32m  10196\u001b[0m     model_fromExtraP = sympify(str_resFromExtraP, locals=dict_symbols)\n\u001b[1;32m  10197\u001b[0m     return model_fromExtraP\n\u001b[1;32m  10200\u001b[0m # In[85]:\n\u001b[1;32m  10201\u001b[0m \n\u001b[1;32m  10202\u001b[0m \n\u001b[1;32m  10203\u001b[0m # def return_rawDF_mg(\n\u001b[1;32m  10204\u001b[0m #     list_process: list[int],\n\u001b[1;32m  10205\u001b[0m #     list_nit: list[int],\n\u001b[1;32m  10206\u001b[0m #     list_size: list[int],\n\u001b[1;32m  10207\u001b[0m #     csvDir: str,\n\u001b[1;32m  10208\u001b[0m # ) -> pd.DataFrame:\n\u001b[1;32m  10209\u001b[0m \n\u001b[1;32m  10210\u001b[0m #     \"\"\"return_rawDF_mg()\n\u001b[1;32m  10211\u001b[0m \n\u001b[1;32m  10212\u001b[0m #     ベンチマークプログラムMGの手動で変更した初期変数におけるプロファイルを取得する関数\n\u001b[0;32m> 10213\u001b[0m \n\u001b[1;32m  10214\u001b[0m #     Args:\n\u001b[1;32m  10215\u001b[0m #         list_process(list[int]):プロセス数のリスト\n\u001b[1;32m  10216\u001b[0m #         list_nit(list[int]):初期変数nitのリスト\n\u001b[1;32m  10217\u001b[0m #         list_size(list[int]):初期変数sizeのリスト\n\u001b[1;32m  10218\u001b[0m #         csvDir(str):CSVファイルを格納したディレクトリのパスを表す文字列\n\u001b[1;32m  10219\u001b[0m \n\u001b[1;32m  10220\u001b[0m #     Returns:\n\u001b[1;32m  10221\u001b[0m #         pd.DataFrame\n\u001b[1;32m  10222\u001b[0m \n\u001b[1;32m  10223\u001b[0m #     \"\"\"\n\u001b[1;32m  10224\u001b[0m \n\u001b[1;32m  10225\u001b[0m #     list_before_concat_DF: list[pd.DataFrame] = []\n\u001b[1;32m  10226\u001b[0m \n\u001b[1;32m  10227\u001b[0m #     for elem_process in list_process:\n\u001b[1;32m  10228\u001b[0m #         for elem_nit in list_nit:\n\u001b[1;32m  10229\u001b[0m #             for elem_size in list_size:\n\u001b[1;32m  10230\u001b[0m #                 filePath: str = f\"{csvDir}mg_problem_size{elem_size}_nit{elem_nit}_process{elem_process}.csv\"\n\u001b[1;32m  10231\u001b[0m #                 if os.path.isfile(filePath):\n\u001b[1;32m  10232\u001b[0m #                     try:\n\u001b[1;32m  10233\u001b[0m #                         DF_read_raw: pd.DataFrame = pd.read_csv(filePath)\n\u001b[1;32m  10234\u001b[0m #                         DF_read_raw[\"process\"] = elem_process\n\u001b[1;32m  10235\u001b[0m #                         DF_read_raw[\"size\"] = elem_size\n\u001b[1;32m  10236\u001b[0m #                         DF_read_raw[\"nit\"] = elem_nit\n\u001b[1;32m  10237\u001b[0m #                         list_before_concat_DF.append(DF_read_raw)\n\u001b[1;32m  10238\u001b[0m #                     except:\n\u001b[1;32m  10239\u001b[0m #                         warnings.warn(f\"{filePath} is empty.\")\n\u001b[1;32m  10240\u001b[0m #                 else:\n\u001b[1;32m  10241\u001b[0m #                     warnings.warn(f\"{filePath} doesn't exist\")\n\u001b[1;32m  10242\u001b[0m #     return pd.concat(objs=list_before_concat_DF, axis=0)\n\u001b[1;32m  10243\u001b[0m \n\u001b[1;32m  10244\u001b[0m \n\u001b[1;32m  10245\u001b[0m # In[86]:\n\u001b[1;32m  10248\u001b[0m def returnConvertedTargetPprofTimeDF(\n\u001b[1;32m  10249\u001b[0m     inputDF: pd.DataFrame,\n\u001b[1;32m  10250\u001b[0m     resVars: list[str],\n\u001b[1;32m  10251\u001b[0m ):\n\u001b[1;32m  10252\u001b[0m     \"\"\"\n\u001b[1;32m  10253\u001b[0m     学習用データおよび予測対象用データの任意のデータ列を変換する関数\n\u001b[1;32m  10254\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10260\u001b[0m         pd.DataFrame : 指定された列が変換されたデータフレーム\n\u001b[1;32m  10261\u001b[0m     \"\"\"\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sympy/core/sympify.py:498\u001b[0m, in \u001b[0;36msympify\u001b[0;34m(a, locals, convert_xor, strict, rational, evaluate)\u001b[0m\n\u001b[1;32m    496\u001b[0m     expr \u001b[39m=\u001b[39m parse_expr(a, local_dict\u001b[39m=\u001b[39m\u001b[39mlocals\u001b[39m, transformations\u001b[39m=\u001b[39mtransformations, evaluate\u001b[39m=\u001b[39mevaluate)\n\u001b[1;32m    497\u001b[0m \u001b[39mexcept\u001b[39;00m (TokenError, \u001b[39mSyntaxError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mraise\u001b[39;00m SympifyError(\u001b[39m'\u001b[39m\u001b[39mcould not parse \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m a, exc)\n\u001b[1;32m    500\u001b[0m \u001b[39mreturn\u001b[39;00m expr\n",
      "\u001b[0;31mSympifyError\u001b[0m: Sympify of expression 'could not parse ''' failed, because of exception being raised:\nSyntaxError: invalid syntax (<string>, line 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dt_now = datetime.datetime.now()\n",
    "\n",
    "benchmarkName :str = \"ep\"\n",
    "\n",
    "ep_contents = ContentsForExtraP(trainDF = trainDF_EP, testDF= testDF_EP, resVar = \"Exclusive\", expVars= expVars_ep ,resVarPerCall = \"ExclusivePerCall\", benchmarkName = benchmarkName)\n",
    "ep_contents.build_all_models()\n",
    "ep_contents.predict_train()\n",
    "ep_contents.predict_test()\n",
    "ep_適合度 :pd.DataFrame = ep_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "ep_重み付きMAPE :pd.DataFrame = ep_contents.get_DF_weightedMAPE()\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    ep_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    ep_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()\n",
    "\n",
    "benchmarkName :str = \"ft\"\n",
    "\n",
    "ft_contents = ContentsForExtraP(trainDF = trainDF_FT, testDF= testDF_FT, resVar = \"Exclusive\", expVars= expVars_ft ,resVarPerCall = \"ExclusivePerCall\", benchmarkName = benchmarkName)\n",
    "ft_contents.build_all_models()\n",
    "ft_contents.predict_train()\n",
    "ft_contents.predict_test()\n",
    "ft_適合度 :pd.DataFrame = ft_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "ft_重み付きMAPE :pd.DataFrame = ft_contents.get_DF_weightedMAPE()\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    ft_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    ft_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()\n",
    "\n",
    "benchmarkName :str = \"is\"\n",
    "\n",
    "is_contents = ContentsForExtraP(trainDF = trainDF_IS, testDF= testDF_IS, resVar = \"Exclusive\", expVars= expVars_is, resVarPerCall = \"ExclusivePerCall\", benchmarkName = benchmarkName)\n",
    "is_contents.build_all_models()\n",
    "is_contents.predict_train()\n",
    "is_contents.predict_test()\n",
    "is_適合度 :pd.DataFrame = is_contents.get_DF_all_fitness_with_relativeErrorRate()\n",
    "is_重み付きMAPE :pd.DataFrame = is_contents.get_DF_weightedMAPE()\n",
    "\n",
    "outputFilePath :str = f\"./outputs/{benchmarkName}_{dt_now.strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "with pd.ExcelWriter(\n",
    "    outputFilePath,\n",
    "    engine=\"xlsxwriter\",\n",
    "    engine_kwargs={'options': {'strings_to_numbers': True}},\n",
    "    ) as writer:\n",
    "    is_適合度.to_excel(writer, sheet_name=\"適合度\"),\n",
    "    is_重み付きMAPE.to_excel(writer, sheet_name=\"重み付きMAPE\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ep_適合度.mean(numeric_only=True)\n",
    "ft_適合度.mean(numeric_only=True)\n",
    "is_適合度.mean(numeric_only=True)\n",
    "mg_適合度.mean(numeric_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
