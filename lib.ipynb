{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作成した各種関数などをまとめるノート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import japanize_matplotlib\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as sp\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各種値を宣言\n",
    "benchmarks = ['bt', 'cg', 'ep', 'ft', 'is', 'lu', 'mg', 'sp']\n",
    "classes = [\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "fix_process = 4\n",
    "fix_benchmark_class = \"C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_fixed_class(BenchMark=\"bt\", Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchMarkClass=\"C\"):\n",
    "        path = './csv_files/'\n",
    "        \n",
    "        # fixed_Class にはベンチマーククラスFixedBenchMarkClassで実行プロセス数がProcessesに該当するものの結果が入る\n",
    "        fixed_Class = []\n",
    "\n",
    "        for process in Processes:\n",
    "            file_name = (\"pprof_\"+BenchMark+FixedBenchMarkClass+str(process)+\".csv\")\n",
    "            file_path = path+file_name\n",
    "            if (os.path.exists(file_path) and os.stat(file_path).st_size != 0):\n",
    "                data_frame = pd.read_csv(path+file_name)\n",
    "                data_frame = data_frame.set_index(['Name'])\n",
    "                fixed_Class.append(data_frame.rename(columns = {'#Call': process}).sort_index())\n",
    "        return(fixed_Class)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fixed_class_graph(BenchMark=\"bt\", Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchMarkClass=\"C\"):\n",
    "\n",
    "        markers = [\".\", \",\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\", \"d\", \"|\", \"_\", \"None\", None, \"\", \"$x$\",\n",
    "            \"$\\\\alpha$\", \"$\\\\beta$\", \"$\\\\gamma$\"]\n",
    "        colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628', '#f781bf'] \n",
    "        fixed_Class = return_fixed_class(BenchMark=BenchMark, Processes=Processes, FixedBenchMarkClass=FixedBenchMarkClass)\n",
    "        if(len(fixed_Class) != 0):\n",
    "            summary_fixed_Class = pd.concat(fixed_Class, axis=1)\n",
    "            summary_fixed_Class.sort_index(axis=1, inplace=True)\n",
    "            summary_fixed_Class_title = BenchMark+\"においてベンチマーククラスをCに固定し実行プロセス数を変化させたときの実行された全ての関数のコール回数\"\n",
    "\n",
    "            x_axes = summary_fixed_Class.columns.tolist()\n",
    "\n",
    "            y_axes = summary_fixed_Class.index.tolist()\n",
    "\n",
    "            plt.figure()\n",
    "            for y_axis in y_axes:\n",
    "                color = random.choice(colors)\n",
    "                label = y_axis\n",
    "                marker = random.choice(markers)\n",
    "                plt.plot(x_axes, summary_fixed_Class.T[y_axis], marker=marker, label=y_axis)\n",
    "            plt.legend()\n",
    "            plt.title(BenchMark+\"_FixedBenchMarkClass=\"+FixedBenchMarkClass)\n",
    "            plt.show()\n",
    "            \n",
    "# 使用例\n",
    "# show_fixed_class_graph(BenchMark=\"cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_fixed_process(BenchMark=\"bt\", BenchMarkClasses=[\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"], FixedProcess=32):\n",
    "    path = './csv_files/'\n",
    "\n",
    "    # fixed_process には実行プロセス数が64でベンチマーククラスがA ~ Dまでの結果が入る\n",
    "    fixed_process = list()\n",
    "\n",
    "    for bench_mark_class in BenchMarkClasses:\n",
    "        file_name = (\"pprof_\"+BenchMark+bench_mark_class+str(FixedProcess)+\".csv\")\n",
    "        file_path = path+file_name\n",
    "        if (os.path.exists(file_path) and os.stat(file_path).st_size != 0):\n",
    "            data_frame = pd.read_csv(path+file_name)\n",
    "            data_frame = data_frame.set_index(['Name'])\n",
    "            fixed_process.append(data_frame.rename(columns = {'#Call': bench_mark_class}).sort_index())\n",
    "    return(fixed_process)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "def show_fixed_process_graph(BenchMark=\"bt\", BenchMarkClasses=[\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"], FixedProcess=32):\n",
    "    fixed_process = return_fixed_process(BenchMark=BenchMark, BenchMarkClasses=BenchMarkClasses, FixedProcess=FixedProcess)\n",
    "    markers = [\".\", \",\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\", \"d\", \"|\", \"_\", \"None\", None, \"\", \"$x$\",\n",
    " \"$\\\\alpha$\", \"$\\\\beta$\", \"$\\\\gamma$\"]\n",
    "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628', '#f781bf']\n",
    "    if (len(fixed_process) != 0):\n",
    "            summary_fixed_process = pd.concat(fixed_process, axis=1)\n",
    "\n",
    "            x_axes = summary_fixed_process.columns.tolist()\n",
    "\n",
    "            y_axes = summary_fixed_process.index.tolist()\n",
    "\n",
    "            plt.figure()\n",
    "            for y_axis in y_axes:\n",
    "                color = random.choice(colors)\n",
    "                label = y_axis\n",
    "                marker = random.choice(markers)\n",
    "                plt.plot(x_axes, summary_fixed_process.T[y_axis], marker=marker, label=y_axis)\n",
    "            plt.legend()\n",
    "            plt.title(BenchMark+\"_FixedProcess=\"+str(FixedProcess))\n",
    "            plt.show()\n",
    "            \n",
    "# 使用例            \n",
    "# show_fixed_process_graph(BenchMark=\"cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(BenchMarks=[], Processes=[], BenchMarkClasses=[], fix_process=4, fix_benchmark_class=\"C\"):\n",
    "\n",
    "    if (BenchMarks==[] or Processes==[] or BenchMarkClasses==[]):\n",
    "        print(\"関数の引数となっている配列が空です。\")\n",
    "    else:\n",
    "        for bench_mark in BenchMarks:\n",
    "            show_fixed_class_graph(BenchMark=bench_mark, Processes=Processes, FixedBenchMarkClass=fix_benchmark_class)\n",
    "            show_fixed_process_graph(BenchMark=bench_mark, BenchMarkClasses=BenchMarkClasses, FixedProcess=fix_process)            \n",
    "\n",
    "bench_marks = ['bt', 'cg', 'ep', 'ft', 'is', 'lu', 'mg', 'sp']\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "classes = [\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# 使用例\n",
    "# show_graph(bench_marks, processes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_summarized_Fixed_dataframe(BenchMark_name = \"cg\", fixed=\"class\"):\n",
    "\n",
    "    def evaluate_dataframes(df1, df2):\n",
    "        for i in range(len(df1.values.tolist()[0])):\n",
    "            if(df1.values.tolist()[0][i] != df2.values.tolist()[0][i]):\n",
    "                return False\n",
    "        return True\n",
    "    fixed_df = 0\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_df = return_fixed_class(BenchMark=BenchMark_name)\n",
    "    elif (fixed == \"process\"):\n",
    "        fixed_df = return_fixed_process(BenchMark=BenchMark_name)\n",
    "    summary_fixed_df = pd.concat(fixed_df, axis=1)\n",
    "    dropped_summary_fixed_df = summary_fixed_df.drop_duplicates()\n",
    "    dropped_summary_fixed_df_renamed = dropped_summary_fixed_df\n",
    "\n",
    "    for dropped_index_name in dropped_summary_fixed_df.index.values:\n",
    "        dropped_index_name_data = summary_fixed_df.loc[[dropped_index_name]]\n",
    "        replace_name = dropped_index_name\n",
    "        for all_index_name in summary_fixed_df.index.values:\n",
    "            all_index_name_data = summary_fixed_df.loc[[all_index_name]]\n",
    "            if(dropped_index_name == all_index_name):\n",
    "                pass\n",
    "            elif(evaluate_dataframes(dropped_index_name_data, all_index_name_data)):\n",
    "                replace_name += f\", {all_index_name}\"\n",
    "        dropped_summary_fixed_df_renamed = dropped_summary_fixed_df_renamed.rename(index={dropped_index_name: replace_name})\n",
    "    \n",
    "    return dropped_summary_fixed_df_renamed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均絶対パーセント誤差 (MAPE)(Mean Absolute Percent Error (MAPE))を返す関数\n",
    "# 引数として長さの同じ二つのリストをとる\n",
    "# 引数l1: 実測値のリスト\n",
    "# 引数l2: 予測値のリスト\n",
    "\n",
    "def mape_score(l1, l2):\n",
    "    return_num = 0\n",
    "    if(len(l1) != len(l2)):\n",
    "        print(\"引数のリストの長さが異なります\", end=\", \")\n",
    "        return -1\n",
    "    for i in range(len(l1)):\n",
    "        l1_num = l1[i]\n",
    "        l2_num = l2[i]\n",
    "        \n",
    "        return_num += abs((l1_num - l2_num)/l1_num)\n",
    "\n",
    "    return_num /= len(l1)\n",
    "    return_num *= 100\n",
    "    return return_num\n",
    "\n",
    "# 使用例：mape_score([1,2,3,4], [4,3,2,1])\n",
    "type(mape_score([1,2,3,4], [4,3,2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_ratio(base_list :list, test_ratio :float):\n",
    "    test_index = math.floor(len(base_list) * float(test_ratio))\n",
    "    train_list = base_list[:-test_index]\n",
    "    test_list = base_list[-test_index:]\n",
    "    return train_list, test_list\n",
    "\n",
    "class ModelBase:\n",
    "    def __init__(self, raw_x, raw_y, benchmark_name=\"benchmark_name\", function_name=\"function_name\", test_ratio=0.3):\n",
    "        self.benchmark_name = benchmark_name\n",
    "        self.function_name = function_name\n",
    "        self.xlabel = \"実行時のプロセス数\"\n",
    "        self.ylabel = \"関数のコール回数\"\n",
    "\n",
    "        self.raw_x = np.reshape(raw_x, (-1, 1))\n",
    "        self.raw_y = np.reshape(raw_y, (-1, 1))\n",
    "        self.train_x, self.test_x = split_by_ratio(self.raw_x, test_ratio)\n",
    "        self.train_y, self.test_y = split_by_ratio(self.raw_y, test_ratio)\n",
    "        \n",
    "        if(len(self.train_x) == 0 or len(self.train_y) == 0 or len(self.test_x) == 0 or len(self.test_y) == 0):\n",
    "            print(f\"学習用とテスト用にデータを分割するのに問題が生じています。@{benchmark_name}\")\n",
    "            print(f\"len(self.train_x) == {len(self.train_x)}\")\n",
    "            print(f\"len(self.train_y) ==  {len(self.train_y)}\")\n",
    "            print(f\"len(self.test_x) == {len(self.test_x)}\")\n",
    "            print(f\"len(self.test_y) == {len(self.test_y)}\")\n",
    "        \n",
    "        self.x_model_line = np.random.rand(1024, 1) * self.raw_x.max()\n",
    "        self.x_model_line.sort(axis=0)\n",
    "        self.y_model_line = 0\n",
    "        \n",
    "        self.lr = 0\n",
    "        self.r2_score = 0\n",
    "        \n",
    "    def calc_lr(self):\n",
    "        self.lr = 0\n",
    "     \n",
    "    def calc_r2_score(self):\n",
    "        self.r2_score = 0\n",
    "        \n",
    "    def calc_mae_score(self):\n",
    "        self.mae_score = 0\n",
    "        \n",
    "    def calc_mse_score(self):\n",
    "        self.mse_score = 0\n",
    "        \n",
    "    def calc_rmse_score(self):\n",
    "        self.rmse_score = 0\n",
    "    \n",
    "    def calc_mape_score(self):\n",
    "        self.mape_score = 0\n",
    "        \n",
    "    def plot_graph(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.raw_x, self.raw_y, color=\"red\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLin(ModelBase):\n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.train_x, self.train_y)\n",
    "        \n",
    "    def calc_r2_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.r2_score = r2_score(self.test_y, test_y_predicted)\n",
    "        \n",
    "    def calc_mae_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mae_score = mean_absolute_error(self.test_y, test_y_predicted)\n",
    "        \n",
    "    def calc_mse_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mse_score = mean_squared_error(self.test_y, test_y_predicted)\n",
    "        \n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "        \n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted = self.lr.predict(self.train_x)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "        \n",
    "    def plot_graph(self):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        self.y_model_line = self.lr.predict(self.x_model_line)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        \n",
    "    def predict(self, num):\n",
    "        predicted = self.lr.predict(num)\n",
    "        return(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverter_log10_func(x):\n",
    "    return 10**x\n",
    "\n",
    "class ModelLog10(ModelBase):\n",
    "    \n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.transformer_log10 = sp.FunctionTransformer(func=np.log10, inverse_func=inverter_log10_func)\n",
    "        x_train_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        y_train_log10 = self.transformer_log10.transform(self.train_y)\n",
    "        \n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(x_train_log10, y_train_log10)\n",
    "        \n",
    "    def calc_r2_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.r2_score = r2_score(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_mae_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.mae_score = mean_absolute_error(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_mse_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.mse_score = mean_squared_error(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "        \n",
    "    def calc_mape_score(self):\n",
    "        test_x_log10 = self.transformer_log10.transform(self.test_x)\n",
    "        test_y_predicted_log10 = self.lr.predict(test_x_log10)\n",
    "        test_y_predicted = self.transformer_log10.inverse_transform(test_y_predicted_log10)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "        \n",
    "    def plot_graph(self):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        x_model_line_log10 = self.transformer_log10.transform(self.x_model_line)\n",
    "        y_model_line_log10 = self.lr.predict(x_model_line_log10)\n",
    "        self.y_model_line = self.transformer_log10.inverse_transform(y_model_line_log10)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "    \n",
    "    def predict(self, num):\n",
    "        num_log10 = self.transformer_log10.transform(num)\n",
    "        predicted_log10 = self.lr.predict(num_log10)\n",
    "        predicted = self.transformer_log10.inverse_transform(predicted_log10)\n",
    "        return(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse proportion\n",
    "def ip_func(x):\n",
    "    return 1/x\n",
    "\n",
    "class ModelIP(ModelBase):\n",
    "    \n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.transformer_ip = sp.FunctionTransformer(func=ip_func, inverse_func=ip_func)\n",
    "        y_train_ip = self.transformer_ip.transform(self.train_y)        \n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.train_x, y_train_ip)\n",
    "        \n",
    "    def calc_r2_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.r2_score = r2_score(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_mae_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.mae_score = mean_absolute_error(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_mse_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.mse_score = mean_squared_error(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "        \n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted_ip = self.lr.predict(self.test_x)\n",
    "        test_y_predicted = self.transformer_ip.inverse_transform(test_y_predicted_ip)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "        \n",
    "    def plot_graph(self):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        y_model_line_ip = self.lr.predict(self.x_model_line)\n",
    "        self.y_model_line = self.transformer_ip.inverse_transform(y_model_line_ip)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        \n",
    "    def predict(self, num):\n",
    "        predicted_ip = self.lr.predict(num)\n",
    "        predicted = self.transformer_ip.inverse_transform(predicted_ip)\n",
    "        return(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBranch(ModelBase):\n",
    "    \n",
    "    def calc_lr(self, AllData=False):\n",
    "        # 後述する t を算出するための処理\n",
    "        max_in_train_y = max(self.train_y)\n",
    "        max_in_train_y_first_index = self.train_y.tolist().index(max_in_train_y)\n",
    "        # 分岐点のインデックスを t とする\n",
    "        t = max_in_train_y_first_index\n",
    "        self.t = t\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.train_x, self.train_y)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.train_x, self.train_y)\n",
    "        else:\n",
    "            self.x_train_1 = self.train_x[:t]\n",
    "            self.x_train_2 = self.train_x[t:]\n",
    "            self.y_train_1 = self.train_y[:t]\n",
    "            self.y_train_2 = self.train_y[t:]\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.x_train_1, self.y_train_1)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.x_train_2, self.y_train_2)\n",
    "        \n",
    "    def calc_mape_score(self):\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            test_y_predicted = self.lr1.predict(self.test_x)\n",
    "            self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "        else:\n",
    "            x_test = self.test_x\n",
    "            y_test = self.test_y\n",
    "            y_test_predicted = self.lr2.predict(x_test)\n",
    "            self.mape_score = float(mape_score(y_test, y_test_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            train_y_predicted = self.lr1.predict(self.train_x)\n",
    "            self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "        else:\n",
    "            train_y_predicted_1 = self.lr1.predict(self.x_train_1)\n",
    "            train_y_predicted_2 = self.lr2.predict(self.x_train_2)\n",
    "            mape_1 = float(mape_score(self.y_train_1, train_y_predicted_1))\n",
    "            mape_2 = float(mape_score(self.y_train_2, train_y_predicted_2))\n",
    "            self.mape_score_InTrain = (mape_1 + mape_2) / 2\n",
    "        \n",
    "    def plot_graph(self):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            y_model_line = self.lr.predict(x_model_line)\n",
    "            plt.plot(self.x_model_line, y_model_line, color=\"red\")\n",
    "        else:\n",
    "            # 回帰曲線を二つのモデルで分割するための処理\n",
    "            x_model_line = self.x_model_line\n",
    "            t_in_model_line = 0\n",
    "            for i in range(len(x_model_line)):\n",
    "                if (self.train_x[self.t] < x_model_line[i]):\n",
    "                    t_in_model_line = i\n",
    "                    break\n",
    "                else:\n",
    "                    t_in_model_line = i\n",
    "            \n",
    "            x_model_line1 = self.x_model_line[:t_in_model_line]\n",
    "            x_model_line2 = self.x_model_line[t_in_model_line:]\n",
    "            y_model_line1 = self.lr1.predict(x_model_line1)\n",
    "            y_model_line2 = self.lr2.predict(x_model_line2)\n",
    "\n",
    "            plt.plot(x_model_line1, y_model_line1, color=\"red\")\n",
    "            plt.plot(x_model_line2, y_model_line2, color=\"red\")\n",
    "    #         plt.plot(self.test_x, self.test_y, color=\"yellow\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        \n",
    "    def predict(self, num):\n",
    "        predicted = self.lr2.predict(num)\n",
    "        return(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_dict_summary_fixed(benchmark_name=\"cg\", fixed=\"class\"):\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_ = return_fixed_class(BenchMark=benchmark_name)\n",
    "    else:\n",
    "        fixed_ = return_fixed_process(BenchMark=benchmark_name)\n",
    "    summary_fixed_ = pd.concat(fixed_ , axis=1)\n",
    "    columns = summary_fixed_.columns.to_numpy()\n",
    "    index = summary_fixed_.index.to_numpy()\n",
    "    if(fixed == \"class\"):\n",
    "        dict_summary_fixed_ = {\"processes\":columns}\n",
    "    else:\n",
    "        dict_summary_fixed_ = {\"class\":columns}\n",
    "    for index_name in index:\n",
    "        dict_summary_fixed_[index_name] = summary_fixed_.T[index_name].to_numpy()\n",
    "    return dict_summary_fixed_\n",
    "\n",
    "# NaNが入った引数のリストをNaNのみを0にして返す関数\n",
    "def return_non_NaN_list(target_list):\n",
    "    for i in range(len(target_list)):\n",
    "        if (math.isnan(target_list[i])):\n",
    "            target_list[i] = 0\n",
    "    return target_list\n",
    "# NaNが入ったリストが引数として渡されるとTrueを返す関数\n",
    "def does_include_nan(target_list):\n",
    "    for i in range(len(target_list)):\n",
    "        if(math.isnan(target_list[i])):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形・対数・反比例モデルでフィッティングを行い、MAPE値をまとめたCSVファイルを作成する関数\n",
    "def generateScoreTable(benchmark_name=\"cg\"):\n",
    "    list_ScoreTable = []\n",
    "    dict_summary_fixed_class = return_dict_summary_fixed(benchmark_name=benchmark_name, fixed=\"class\")\n",
    "    raw_x = dict_summary_fixed_class[\"processes\"]\n",
    "    for content in dict_summary_fixed_class:\n",
    "        if(content == \"processes\"):\n",
    "            continue\n",
    "        raw_y = dict_summary_fixed_class[content]\n",
    "        if(does_include_nan(raw_y)):\n",
    "            continue\n",
    "        # 線形モデル\n",
    "        model_lin = ModelLin(raw_x, raw_y, benchmark_name, content)\n",
    "        model_lin.calc_lr()\n",
    "        model_lin.calc_r2_score()\n",
    "        model_lin.calc_mae_score()\n",
    "        model_lin.calc_mse_score()\n",
    "        model_lin.calc_rmse_score()\n",
    "        model_lin.calc_mape_score()\n",
    "        # logモデル\n",
    "        model_log10 = ModelLog10(raw_x, raw_y, benchmark_name, content)\n",
    "        model_log10.calc_lr()\n",
    "        model_log10.calc_r2_score()\n",
    "        model_log10.calc_mae_score()\n",
    "        model_log10.calc_mse_score()\n",
    "        model_log10.calc_rmse_score()\n",
    "        model_log10.calc_mape_score()\n",
    "        # 反比例モデル\n",
    "        model_ip = ModelIP(raw_x, raw_y, benchmark_name, content)\n",
    "        model_ip.calc_lr()\n",
    "        model_ip.calc_r2_score()\n",
    "        model_ip.calc_mae_score()\n",
    "        model_ip.calc_mse_score()\n",
    "        model_ip.calc_rmse_score()\n",
    "        model_ip.calc_mape_score()\n",
    "        \n",
    "        list_ScoreTable.append([content, model_lin.mape_score, model_log10.mape_score, model_ip.mape_score])\n",
    "    df_ScoreTable = pd.DataFrame(list_ScoreTable)\n",
    "    df_ScoreTable.columns = [\"\", \"x mape\", \"logx mape\", \"1/x mape\"]\n",
    "    df_ScoreTable.set_index(\"\",inplace=True)\n",
    "    df_ScoreTable.to_csv(\"./tmp_GenerateScoreTable/\"+benchmark_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行の内容が同じものをまとめ、行タイトルに重複した行タイトルがまとめられた、データフレームが返される関数\n",
    "\n",
    "def return_summarized_Fixed_dataframe(BenchMark_name = \"cg\", fixed=\"class\"):\n",
    "\n",
    "    def evaluate_dataframes(df1, df2):\n",
    "        for i in range(len(df1.values.tolist()[0])):\n",
    "            if(df1.values.tolist()[0][i] != df2.values.tolist()[0][i]):\n",
    "                return False\n",
    "        return True\n",
    "    fixed_df = 0\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_df = return_fixed_class(BenchMark=BenchMark_name)\n",
    "    elif (fixed == \"process\"):\n",
    "        fixed_df = return_fixed_process(BenchMark=BenchMark_name)\n",
    "    summary_fixed_df = pd.concat(fixed_df, axis=1)\n",
    "    dropped_summary_fixed_df = summary_fixed_df.drop_duplicates()\n",
    "    dropped_summary_fixed_df_renamed = dropped_summary_fixed_df\n",
    "\n",
    "    for dropped_index_name in dropped_summary_fixed_df.index.values:\n",
    "        dropped_index_name_data = summary_fixed_df.loc[[dropped_index_name]]\n",
    "        replace_name = dropped_index_name\n",
    "        for all_index_name in summary_fixed_df.index.values:\n",
    "            all_index_name_data = summary_fixed_df.loc[[all_index_name]]\n",
    "            if(dropped_index_name == all_index_name):\n",
    "                pass\n",
    "            elif(evaluate_dataframes(dropped_index_name_data, all_index_name_data)):\n",
    "                replace_name += f\", {all_index_name}\"\n",
    "        dropped_summary_fixed_df_renamed = dropped_summary_fixed_df_renamed.rename(index={dropped_index_name: replace_name})\n",
    "    \n",
    "    return dropped_summary_fixed_df_renamed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~_excludeBTSP, ~~~_onlyBTSP はそれぞれのベンチマークで取得したプロセス数\n",
    "processes_excludeBTSP = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "processes_onlyBTSP = [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 169, 196, 225]\n",
    "\n",
    "# 引数に横軸：プロセス数orベンチマーククラス, 縦軸：関数名となっているデータフレームを取る\n",
    "# 返値として\n",
    "# rowData:プロセス数のリスト もしくは プロセス数のリスト (引数に由来)\n",
    "# 各種関数名：実行回数のリスト\n",
    "# 以上のような関係の辞書を返す\n",
    "\n",
    "def return_dict_Data(DataFrame):\n",
    "    columns = DataFrame.columns.to_numpy()\n",
    "    index = DataFrame.index.to_numpy()\n",
    "    # 返値となる辞書return_dictに引数のデータフレームの列名(プロセス数orベンチマーククラス)を格納\n",
    "    return_dict = {\"rowData\":columns}\n",
    "    for index_name in index:\n",
    "        return_dict[index_name] = DataFrame.T[index_name].to_numpy()\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "\n",
    "### 引数はx軸となる値のリスト, y軸となる値のリスト, 関数名の文字列, 訓練データでMAPEを算出するかどうかの真偽\n",
    "### 返り値は次のようなリスト\n",
    "### [<関数名の文字列>, <線形モデルのMAPE値>, <対数モデルのMAPE値>, <反比例モデルのMAPE値>, <分岐モデルのMAPE値>]\n",
    "def return_Mape_row_list(x :list, y :list, function_name :str, test_ratio=0.3, train=False):\n",
    "\n",
    "    # 変数：model_lin\n",
    "    # 線形モデル\n",
    "    model_lin = ModelLin(x, y, test_ratio=test_ratio)\n",
    "    model_lin.calc_lr()\n",
    "    model_lin.calc_mape_score()\n",
    "    model_lin.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_log\n",
    "    # 対数モデル\n",
    "    model_log = ModelLog10(x, y, test_ratio=test_ratio)\n",
    "    model_log.calc_lr()\n",
    "    model_log.calc_mape_score()\n",
    "    model_log.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_ip\n",
    "    # 反比例モデル\n",
    "    model_ip = ModelIP(x, y, test_ratio=test_ratio)\n",
    "    model_ip.calc_lr()\n",
    "    model_ip.calc_mape_score()\n",
    "    model_ip.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_branch\n",
    "    # 特異点付き条件分岐モデル\n",
    "    model_branch = ModelBranch(x, y, test_ratio=test_ratio)\n",
    "    model_branch.calc_lr()\n",
    "    model_branch.calc_mape_score()\n",
    "    model_branch.calc_mape_score_InTrain()\n",
    "\n",
    "    if(train==True):\n",
    "        lin_score = model_lin.mape_score_InTrain\n",
    "        log_score = model_log.mape_score_InTrain\n",
    "        ip_score = model_ip.mape_score_InTrain\n",
    "        branch_score = model_branch.mape_score_InTrain\n",
    "    else:\n",
    "        lin_score = model_lin.mape_score\n",
    "        log_score = model_log.mape_score\n",
    "        ip_score = model_ip.mape_score\n",
    "        branch_score = model_branch.mape_score\n",
    "\n",
    "    # 変数：return_list\n",
    "    # 返り値となるリスト\n",
    "    return_list = [function_name, lin_score, log_score, ip_score, branch_score]\n",
    "    return(return_list)\n",
    "\n",
    "### 引数は「return_dict_DataFrame()」の返値, テストとして何割のデータを用いるかの割合, 訓練データでMAPEを算出するかの真偽\n",
    "### 返り値は行・列がモデル名・関数名で要素がMAPE値となっているDataFrame\n",
    "def return_MapeTable_per_benchmark(dict_data :dict, test_ratio, train=False):\n",
    "\n",
    "    # 変数：_names\n",
    "    # 引数の辞書のプロセス数もしくはベンチマーククラスの文字列のリスト\n",
    "    _names = dict_data['rowData']\n",
    "    # 変数：function_names\n",
    "    # 引数の辞書の関数名の文字列のリスト\n",
    "    function_names = list(dict_data.keys())\n",
    "    function_names.remove('rowData')\n",
    "\n",
    "    # リスト変数：before_DataFrame_list\n",
    "    # 最終的にDataFrameとする元となるリスト\n",
    "    before_DataFrame_list = []\n",
    "    collumn_names = [\"function name\", \"Linear model\", \"Log10 model\", \"Inverse model\", \"Branch model\"]\n",
    "    for function_name in function_names:\n",
    "        if(does_include_nan(dict_data[function_name])):\n",
    "            continue\n",
    "        before_DataFrame_list.append(return_Mape_row_list(x = _names,y = dict_data[function_name],function_name = function_name, test_ratio=test_ratio, train=train))\n",
    "    \n",
    "    # 変数：return_df\n",
    "    # 返り値となるリスト\n",
    "    return_df = pd.DataFrame(before_DataFrame_list)\n",
    "    return_df.columns = collumn_names\n",
    "    return_df = return_df.set_index(\"function name\")\n",
    "\n",
    "    return(return_df)\n",
    "\n",
    "\n",
    "### 構造体的に利用可能なクラス MapeData\n",
    "### 各ベンチマークの各モデルごとに作成される。\n",
    "### 要素として、割合, 最大値, 最小値 がある。\n",
    "class MapeData:\n",
    "    def __init__(self):\n",
    "        self.ratio = 0\n",
    "        self.max = np.nan\n",
    "        self.min = np.nan\n",
    "        self.appearance = 0\n",
    "\n",
    "    def printData(self):\n",
    "        print(f\"{self.ratio}({self.min}, {self.max})\")\n",
    "\n",
    "    def return_Data(self):\n",
    "        max_min = \"\"\n",
    "        if(self.min is np.nan):\n",
    "            max_min = \"(NoData)\"\n",
    "        else:\n",
    "            max_min = f\"({self.min}, {self.max})\"\n",
    "        return(f\"{self.ratio}%{max_min}\")\n",
    "\n",
    "### 引数に「return_MapeTable_per_benchmark()」の返り値, ベンチマーク名, (オプショナル)中間データの詳細をとる\n",
    "### 返り値として次のようなリストを返す\n",
    "### [<線形モデルのMAPEに関する奴>, <対数モデルのMAPEに関する奴>, <反比例モデルのMAPEに関する奴>, <ベンチマーク名>]\n",
    "\n",
    "def return_MapeTable_row(MapeDataframe_detail, benchmark_name:str):\n",
    "    \n",
    "    # 引数として渡されたデータフレームの行列名をindex, columnsに格納\n",
    "    columns = MapeDataframe_detail.columns.to_numpy()\n",
    "    index = MapeDataframe_detail.index.to_numpy()\n",
    "\n",
    "    # この関数で返すリストの要素の準備\n",
    "    MapeLin = MapeData()\n",
    "    MapeLog = MapeData()\n",
    "    MapeIP = MapeData()\n",
    "    MapeBr = MapeData()\n",
    "    return_list = [MapeLin, MapeLog, MapeIP, MapeBr, benchmark_name ]\n",
    "\n",
    "    # 返り値のリストの各要素の値を更新\n",
    "    for function_name in index:\n",
    "        MapeData_per_function = MapeDataframe_detail.loc[function_name].to_list()\n",
    "        min_mape = min(MapeData_per_function)\n",
    "        min_mape_index = MapeData_per_function.index(min_mape)\n",
    "        rounded_min_mape = int(min_mape * 10) / 10\n",
    "        return_list[min_mape_index].appearance += 1\n",
    "        if(return_list[min_mape_index].max is np.nan):\n",
    "            return_list[min_mape_index].max = rounded_min_mape\n",
    "            return_list[min_mape_index].min = rounded_min_mape\n",
    "        if(return_list[min_mape_index].min > min_mape):\n",
    "            return_list[min_mape_index].min = rounded_min_mape\n",
    "        elif(return_list[min_mape_index].max < min_mape):\n",
    "            return_list[min_mape_index].max = rounded_min_mape\n",
    "    sum_num = 0\n",
    "    # 集計データから割合を算出\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        sum_num += return_list[i].appearance\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        return_list[i].ratio = int(100 * return_list[i].appearance/sum_num)\n",
    "    # 割合の合計が100になるように調整\n",
    "    exclude_index0_ratios = 0\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        exclude_index0_ratios += return_list[i].ratio\n",
    "    return_list[0].ratio = 100 - exclude_index0_ratios\n",
    "\n",
    "    return(return_list)\n",
    "\n",
    "def save_MapeTable(MapeTable, suffix=\"\"):\n",
    "    tmp_table = MapeTable.copy()\n",
    "    columns = MapeTable.columns.to_numpy()\n",
    "    index = MapeTable.index.to_numpy()\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(len(index)):\n",
    "            tmp_table.iat[j, i] = tmp_table.iat[j, i].return_Data()\n",
    "    tmp_table.to_csv(f\"./tmp_GenerateResources/MapeTable_{str(suffix)}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数はx軸となる値のリスト, y軸となる値のリスト, 関数名の文字列\n",
    "### 返り値は次のようなリスト\n",
    "### [<関数名の文字列>, <線形モデル>, <対数モデル>, <反比例モデル>, <分岐モデル>]\n",
    "def return_Model_row_list(x :list, y :list, function_name :str, test_ratio=0.3, train=False):\n",
    "\n",
    "    # 変数：model_lin\n",
    "    # 線形モデル\n",
    "    model_lin = ModelLin(x, y, test_ratio=test_ratio)\n",
    "    model_lin.calc_lr()\n",
    "    model_lin.calc_mape_score()\n",
    "    model_lin.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_log\n",
    "    # 対数モデル\n",
    "    model_log = ModelLog10(x, y, test_ratio=test_ratio)\n",
    "    model_log.calc_lr()\n",
    "    model_log.calc_mape_score()\n",
    "    model_log.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_ip\n",
    "    # 反比例モデル\n",
    "    model_ip = ModelIP(x, y, test_ratio=test_ratio)\n",
    "    model_ip.calc_lr()\n",
    "    model_ip.calc_mape_score()\n",
    "    model_ip.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_branch\n",
    "    # 特異点付き条件分岐モデル\n",
    "    model_branch = ModelBranch(x, y, test_ratio=test_ratio)\n",
    "    model_branch.calc_lr()\n",
    "    model_branch.calc_mape_score()\n",
    "    model_branch.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：return_list\n",
    "    # 返り値となるリスト\n",
    "    return_list = [function_name, model_lin, model_log, model_ip, model_branch]\n",
    "    return(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、収集するベンチマークのリスト、実行したい学習の割合、固定したいベンチマーククラス\n",
    "### 返値は無し\n",
    "### 実行すると、\"./tmpGenerateResources\" に \"<ベンチマーク名>_FixedClassTrain_<テスト割合>.csv\" という形式でファイルが生成される\n",
    "\n",
    "def GenerateMapeTableFixClass(Benchmarks=[\"cg\"], TestRatios=[\"0.3\"], FixBenchmarksClass=\"C\"):\n",
    "    for test_ratio in TestRatios:\n",
    "        print(f\"test_ratio={test_ratio}\")\n",
    "        print(f\"train_list, test_list = {split_by_ratio(processes_excludeBTSP, test_ratio)} on processes_excludeBTSP\")\n",
    "        print(f\"train_list, test_list = {split_by_ratio(processes_onlyBTSP, test_ratio)} on processes_onlyBTSP\")\n",
    "        print(f\"\\n\")\n",
    "        fixed_class_list = [0] * len(Benchmarks)\n",
    "        for i in range(len(Benchmarks)):\n",
    "            if(Benchmarks[i] == \"bt\" or Benchmarks[i] == \"sp\"):\n",
    "                process = processes_onlyBTSP\n",
    "            else:\n",
    "                process = processes_excludeBTSP\n",
    "            fixed_class_list[i] = return_fixed_class(BenchMark=Benchmarks[i], Processes=processes, FixedBenchMarkClass=FixBenchmarksClass)\n",
    "        fixed_class_DataFrame = [0] * len(fixed_class_list)\n",
    "        for i in range(len(fixed_class_list)):\n",
    "            fixed_class_DataFrame[i] = pd.concat(fixed_class_list[i], axis=1)\n",
    "        for i in range(len(fixed_class_DataFrame)):\n",
    "            dict_data = return_dict_Data(fixed_class_DataFrame[i])\n",
    "            MapeTable_per_benchmark = return_MapeTable_per_benchmark(dict_data, test_ratio=test_ratio, train=True)\n",
    "            MapeTable_per_benchmark.to_csv(f\"./tmp_GenerateResources/{Benchmarks[i]}_FixedClassTrain_{test_ratio}.csv\")\n",
    "# 使用例       \n",
    "# GenerateMapeTableFixClass(Benchmarks=[\"cg\", \"lu\"], TestRatios=[0.3, 0.7], FixBenchmarksClass=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、収集するベンチマークのリスト、実行したい学習の割合、固定したい実行プロセス数\n",
    "### 返値は無し\n",
    "### 実行すると、\"./tmpGenerateResources\" に \"<ベンチマーク名>_FixedProcessTrain_<テスト割合>.csv\" という形式でファイルが生成される\n",
    "\n",
    "def GenerateMapeTableFixProcess(Benchmarks=[\"cg\"], BenchmarkClasses=[\"A\", \"B\", \"C\", \"D\"], BenchmarkClasses_on_num=[1, 4, 16, 256], TestRatios=[\"0.3\"], FixProcess=64):\n",
    "    for test_ratio in TestRatios:\n",
    "        print(f\"test_ratio={test_ratio}\")\n",
    "        print(f\"train_list, test_list = {split_by_ratio(BenchmarkClasses, test_ratio)} on BenchmarkClasses\")\n",
    "        print(f\"\\n\")\n",
    "        fixed_Process_list = [0] * len(benchmarks)\n",
    "        for i in range(len(fixed_Process_list)):\n",
    "            fixed_Process_list[i] = return_fixed_process(BenchMark=Benchmarks[i], BenchMarkClasses=BenchmarkClasses, FixedProcess=FixProcess)\n",
    "        fixed_Process_DataFrame = [0] * len(fixed_Process_list)\n",
    "        for i in range(len(fixed_Process_DataFrame)):\n",
    "            fixed_Process_DataFrame[i] = pd.concat(fixed_Process_list[i], axis=1)\n",
    "            \n",
    "        for i in range(len(fixed_Process_DataFrame)):\n",
    "            dict_data = return_dict_Data(fixed_Process_DataFrame[i])\n",
    "            dict_data['rowData'] = BenchmarkClasses_on_num\n",
    "            try:\n",
    "                MapeTable_per_benchmark=return_MapeTable_per_benchmark(dict_data, test_ratio=test_ratio, train=True)\n",
    "            except:\n",
    "                print(f\"MAPEを算出するのに問題発生@{Benchmarks[i]}\")\n",
    "                continue\n",
    "            MapeTable_per_benchmark.to_csv(f\"./tmp_GenerateResources/{benchmarks[i]}_FixedProcessTrain_{test_ratio}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertBencharkClass_inNPB(Alphabet :str):\n",
    "    if(Alphabet == \"A\"):\n",
    "        return (1)\n",
    "    elif(Alphabet == \"B\"):\n",
    "        return (4)\n",
    "    elif(Alphabet == \"C\"):\n",
    "        return (16)\n",
    "    elif(Alphabet == \"D\"):\n",
    "        return (256)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ConvertBenchmarkClasses(InputList=[\"A\", \"B\", \"C\", \"D\"]):\n",
    "    ReturnList = []\n",
    "    for content in InputList:\n",
    "        ReturnList.append(ConvertBencharkClass_inNPB(content))\n",
    "    return(ReturnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_OptimalProcessesList(BenchmarkName=\"cg\"):\n",
    "    if(BenchmarkName==\"bt\" or BenchmarkName==\"sp\"):\n",
    "        return(processes_onlyBTSP)\n",
    "    else:\n",
    "        return(processes_excludeBTSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、ベンチマーク名、列名、固定する値、プロセスorクラスのどちらで固定するか\n",
    "### 返値は次のような形式の辞書\n",
    "### rowData:[プロセス数]or[ベンチマーククラス]\n",
    "### <関数名>:[実行回数]\n",
    "def returnDictForModelDataFrame(BenchmarkName=\"cg\", rowData=[\"A\", \"B\", \"C\", \"D\"], fix=\"64\", fixed=\"Process\"):\n",
    "    if(fixed==\"Process\"):\n",
    "        FixedProcessList = return_fixed_process(BenchMark=BenchmarkName, BenchMarkClasses=rowData, FixedProcess=fix)\n",
    "        FixedProcessDataFrame = pd.concat(FixedProcessList, axis=1)\n",
    "        DictData = return_dict_Data(FixedProcessDataFrame)\n",
    "    elif(fixed==\"Class\"):\n",
    "        FixedClassList = return_fixed_class(BenchMark=BenchmarkName, Processes=rowData, FixedBenchMarkClass=fix)\n",
    "        FixedClassDataFrame = pd.concat(FixedClassList, axis=1)\n",
    "        DictData = return_dict_Data(FixedClassDataFrame)\n",
    "    return(DictData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、returnDictForModelDataFrame()の返値, rowData, 関数名のリスト, テストに割り当てる割合\n",
    "### 返値はリスト\n",
    "### [<関数名>, <学習済みデータ1>, ... , <学習済みデータn>]\n",
    "def return_ModelDataSourceList(DictData, x_list, Index, test_ratio=0.3):\n",
    "    ModelDataSourceList = []\n",
    "    for FunctionName in Index:\n",
    "        y_list = DictData[FunctionName]\n",
    "        if(does_include_nan(y_list)):\n",
    "            continue\n",
    "        ModelDataSourceList.append(return_Model_row_list(x=x_list, y=y_list, function_name=FunctionName, test_ratio=test_ratio, train=True))\n",
    "    return(ModelDataSourceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、関数名、rowData, テストに割り当てる割合, 固定するプロセス数orクラス, クラスで固定するかプロセス数で固定するか\n",
    "### 返値はDataFrame\n",
    "### 行名は関数名で、列名はモデルの種別\n",
    "### それぞれの要素は学習済みデータ\n",
    "def return_Models_per_Benchmark(BenchmarkName=\"cg\", rowData=[1, 4, 16, 256], TestRate=0.3, fix=\"C\", fixed=\"Class\"):\n",
    "    # 変数：MapeTable\n",
    "    # ベンチマークのMAPE表\n",
    "    # fixedが\"Class\"ならクラスで固定され、fixedが\"Process\"ならプロセス数で固定されたMAPE表を読み込んでいる。\n",
    "    MapeTable = pd.read_csv(f\"./tmp_GenerateResources/{BenchmarkName}_Fixed{fixed}Train_{test_ratio}.csv\")\n",
    "    MapeTable = MapeTable.set_index(\"function name\")\n",
    "    \n",
    "    # 変数：MapeTableColumns, MapTableIndex\n",
    "    # MapeTableの列名・行名\n",
    "    MapeTableColumns = MapeTable.columns.to_numpy()\n",
    "    MapeTableIndex = MapeTable.index.to_numpy()\n",
    "    \n",
    "    # 変数：ModelDataFrame\n",
    "    # MapeTableにおける各関数の学習済みモデルが格納される\n",
    "    checked_rowData=rowData\n",
    "    if(fixed==\"Process\"):\n",
    "        checked_rowData=ConvertBenchmarkClasses(rowData)\n",
    "    DictData = returnDictForModelDataFrame(BenchmarkName, rowData=rowData, fix=fix, fixed=fixed)\n",
    "    ModelDataFrameSourceList = return_ModelDataSourceList(DictData=DictData, x_list=checked_rowData, Index=MapeTableIndex, test_ratio=TestRate)\n",
    "    ModelDataFrameSourceListCollumnsName = [\"FunctionName\", \"ModelLin\", \"ModelLog\", \"ModelIp\", \"ModelBranch\"]\n",
    "    ModelDataFrame = pd.DataFrame(ModelDataFrameSourceList)\n",
    "    ModelDataFrame.columns = ModelDataFrameSourceListCollumnsName\n",
    "    ModelDataFrame = ModelDataFrame.set_index(\"FunctionName\")\n",
    "    return(ModelDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、読み込んだベンチマークごとのMAPE表, 各関数の全てのモデルにおける学習済みモデル\n",
    "### 返値は辞書\n",
    "### キーは<関数名>でバリューは<学習済みモデル>\n",
    "def return_BestModelsDict(MapeTable, ModelDataFrame):\n",
    "    BestModelsDict = {}\n",
    "    ModelNames = ModelDataFrame.columns.to_list()\n",
    "    ModelDataFrameIndexNameList = ModelDataFrame.index.to_numpy()\n",
    "    for FunctionName in ModelDataFrameIndexNameList:\n",
    "        MapeInFunction = MapeTable.loc[FunctionName].to_list()\n",
    "        SmallestModelIndex = MapeInFunction.index(min(MapeInFunction))\n",
    "        SmallestModelName = ModelNames[SmallestModelIndex]\n",
    "        BestModelsDict[FunctionName] = ModelDataFrame.at[FunctionName, SmallestModelName]\n",
    "    return BestModelsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BenchmarkClasses =[\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "### 引数は\n",
    "### 返値は辞書\n",
    "### キーは<関数名>でバリューは<学習済みモデル>の辞書\n",
    "### 学習済みモデルのデータフレームの取得に失敗した場合はFalseを返す\n",
    "def generateBestModelDict(TestRatio=0.3, BenchmarkName=\"ft\", fixed=\"Class\", fix=\"C\", rowData=[1, 2, 4, 8, 16, 32, 64, 128, 256]):\n",
    "    \n",
    "    # 変数：MapeTable\n",
    "    # ベンチマークのMAPE表\n",
    "    if(fixed==\"Class\"):\n",
    "        file_name = f\"{BenchmarkName}_FixedClassTrain_{TestRatio}.csv\"\n",
    "    else:\n",
    "        file_name = f\"{BenchmarkName}_FixedProcessTrain_{TestRatio}.csv\"\n",
    "    file_path = f\"./tmp_GenerateResources/{file_name}\"\n",
    "    MapeTable = pd.read_csv(file_path)\n",
    "    MapeTable = MapeTable.set_index(\"function name\")\n",
    "#     try:\n",
    "#         ModelDataFrame = return_Models_per_Benchmark(BenchmarkName=benchmark, rowData=processes, TestRate=test_ratio, fix=fix, fixed=fixed)\n",
    "#     except:\n",
    "#         print(f\"\\n全てのモデル形式で学習済みモデルを作成しているor集めている最中に問題が発生しました@{benchmark}\\n\")\n",
    "#         return False\n",
    "    ModelDataFrame = return_Models_per_Benchmark(BenchmarkName=benchmark, rowData=rowData, TestRate=test_ratio, fix=fix, fixed=fixed)\n",
    "\n",
    "    BestModelsDict = return_BestModelsDict(MapeTable=MapeTable, ModelDataFrame=ModelDataFrame)\n",
    "    return(BestModelsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、テストの割合のリスト, rowData\n",
    "# 返値はrowDataのtrainのリストを文字列化したもののリスト\n",
    "def return_StringTrainList(TestRatio=[0.3, 0.5], rowData=[1,2,4,8]):\n",
    "    returnList = []\n",
    "    for test_ratio in TestRatio:\n",
    "        train_list, test_list = split_by_ratio(base_list=rowData, test_ratio=test_ratio)\n",
    "        returnList.append(f\"{train_list}\")\n",
    "    return(returnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、関数名, 予測値, ベンチマーク名, ベンチマーククラス, 実行プロセス数\n",
    "# 返値は予測値と実測値の誤差率\n",
    "def return_ErrorRate(FunctionName=\"CG\", PredictNum=256, BenchmarkName=\"cg\", BenchmarkClass=\"D\", Process=256):\n",
    "    target_csv = pd.read_csv(f\"./csv_files/pprof_{BenchmarkName}{BenchmarkClass}{Process}.csv\")\n",
    "    target_csv = target_csv.set_index(\"Name\")\n",
    "    try:\n",
    "        RealNum = target_csv.loc[FunctionName, \"#Call\"]\n",
    "    except:\n",
    "        print(f\"該当する関数はありませんでした@{Benchmakname}@{FunctionName}\")\n",
    "        RealNum = False\n",
    "    if(RealNum != False):\n",
    "        returnNum = abs(RealNum-PredictNum)/RealNum\n",
    "        return(returnNum)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_Predicted(LearnedModel, num):\n",
    "    Input = np.reshape(num, (-1, 1))\n",
    "    PredictedData = LearnedModel.predict(Input)\n",
    "    PredictedList = PredictedData.tolist()\n",
    "    Predict = PredictedList[0][0]\n",
    "    return(Predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "20ea737a692ad9331495a19ba7cc0bf6d1e8652119322fabf661903bfd24f6c5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}