{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作成した各種関数などをまとめるノート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import japanize_matplotlib\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as sp\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブック中で変数のみを記述することでデータフレームをきれいに表示させる設定の有効化\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各種値を宣言\n",
    "benchmarks = ['bt', 'cg', 'ep', 'ft', 'is', 'lu', 'mg', 'sp']\n",
    "classes = [\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"]\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "fix_process = 4\n",
    "fix_benchmark_class = \"C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_fixed_class(BenchMark=\"bt\", Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchMarkClass=\"C\"):\n",
    "        path = './csv_files/'\n",
    "        \n",
    "        # fixed_Class にはベンチマーククラスFixedBenchMarkClassで実行プロセス数がProcessesに該当するものの結果が入る\n",
    "        fixed_Class = []\n",
    "\n",
    "        for process in Processes:\n",
    "            file_name = (\"pprof_\"+BenchMark+FixedBenchMarkClass+str(process)+\".csv\")\n",
    "            file_path = path+file_name\n",
    "            if (os.path.exists(file_path) and os.stat(file_path).st_size != 0):\n",
    "                data_frame = pd.read_csv(path+file_name)\n",
    "                data_frame = data_frame.set_index(['Name'])\n",
    "                fixed_Class.append(data_frame.rename(columns = {'#Call': process}).sort_index())\n",
    "        return(fixed_Class)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fixed_class_graph(BenchMark=\"bt\", Processes=[1, 2, 4, 8, 16, 32, 64, 128, 256], FixedBenchMarkClass=\"C\"):\n",
    "\n",
    "        markers = [\".\", \",\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\", \"d\", \"|\", \"_\", \"None\", None, \"\", \"$x$\",\n",
    "            \"$\\\\alpha$\", \"$\\\\beta$\", \"$\\\\gamma$\"]\n",
    "        colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628', '#f781bf'] \n",
    "        fixed_Class = return_fixed_class(BenchMark=BenchMark, Processes=Processes, FixedBenchMarkClass=FixedBenchMarkClass)\n",
    "        if(len(fixed_Class) != 0):\n",
    "            summary_fixed_Class = pd.concat(fixed_Class, axis=1)\n",
    "            summary_fixed_Class.sort_index(axis=1, inplace=True)\n",
    "            summary_fixed_Class_title = BenchMark+\"においてベンチマーククラスをCに固定し実行プロセス数を変化させたときの実行された全ての関数のコール回数\"\n",
    "\n",
    "            x_axes = summary_fixed_Class.columns.tolist()\n",
    "\n",
    "            y_axes = summary_fixed_Class.index.tolist()\n",
    "\n",
    "            plt.figure()\n",
    "            for y_axis in y_axes:\n",
    "                color = random.choice(colors)\n",
    "                label = y_axis\n",
    "                marker = random.choice(markers)\n",
    "                plt.plot(x_axes, summary_fixed_Class.T[y_axis], marker=marker, label=y_axis)\n",
    "            plt.legend()\n",
    "            plt.title(BenchMark+\"_FixedBenchMarkClass=\"+FixedBenchMarkClass)\n",
    "            plt.show()\n",
    "            \n",
    "# 使用例\n",
    "# show_fixed_class_graph(BenchMark=\"cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_fixed_process(BenchMark=\"bt\", BenchMarkClasses=[\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"], FixedProcess=32):\n",
    "    path = './csv_files/'\n",
    "\n",
    "    # fixed_process には実行プロセス数が64でベンチマーククラスがA ~ Dまでの結果が入る\n",
    "    fixed_process = list()\n",
    "\n",
    "    for bench_mark_class in BenchMarkClasses:\n",
    "        file_name = (\"pprof_\"+BenchMark+bench_mark_class+str(FixedProcess)+\".csv\")\n",
    "        file_path = path+file_name\n",
    "        if (os.path.exists(file_path) and os.stat(file_path).st_size != 0):\n",
    "            data_frame = pd.read_csv(path+file_name)\n",
    "            data_frame = data_frame.set_index(['Name'])\n",
    "            fixed_process.append(data_frame.rename(columns = {'#Call': bench_mark_class}).sort_index())\n",
    "    return(fixed_process)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "def show_fixed_process_graph(BenchMark=\"bt\", BenchMarkClasses=[\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"], FixedProcess=32):\n",
    "    fixed_process = return_fixed_process(BenchMark=BenchMark, BenchMarkClasses=BenchMarkClasses, FixedProcess=FixedProcess)\n",
    "    markers = [\".\", \",\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\", \"d\", \"|\", \"_\", \"None\", None, \"\", \"$x$\",\n",
    " \"$\\\\alpha$\", \"$\\\\beta$\", \"$\\\\gamma$\"]\n",
    "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628', '#f781bf']\n",
    "    if (len(fixed_process) != 0):\n",
    "            summary_fixed_process = pd.concat(fixed_process, axis=1)\n",
    "\n",
    "            x_axes = summary_fixed_process.columns.tolist()\n",
    "\n",
    "            y_axes = summary_fixed_process.index.tolist()\n",
    "\n",
    "            plt.figure()\n",
    "            for y_axis in y_axes:\n",
    "                color = random.choice(colors)\n",
    "                label = y_axis\n",
    "                marker = random.choice(markers)\n",
    "                plt.plot(x_axes, summary_fixed_process.T[y_axis], marker=marker, label=y_axis)\n",
    "            plt.legend()\n",
    "            plt.title(BenchMark+\"_FixedProcess=\"+str(FixedProcess))\n",
    "            plt.show()\n",
    "            \n",
    "# 使用例            \n",
    "# show_fixed_process_graph(BenchMark=\"cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(BenchMarks=[], Processes=[], BenchMarkClasses=[], fix_process=4, fix_benchmark_class=\"C\"):\n",
    "\n",
    "    if (BenchMarks==[] or Processes==[] or BenchMarkClasses==[]):\n",
    "        print(\"関数の引数となっている配列が空です。\")\n",
    "    else:\n",
    "        for bench_mark in BenchMarks:\n",
    "            show_fixed_class_graph(BenchMark=bench_mark, Processes=Processes, FixedBenchMarkClass=fix_benchmark_class)\n",
    "            show_fixed_process_graph(BenchMark=bench_mark, BenchMarkClasses=BenchMarkClasses, FixedProcess=fix_process)            \n",
    "\n",
    "bench_marks = ['bt', 'cg', 'ep', 'ft', 'is', 'lu', 'mg', 'sp']\n",
    "processes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "classes = [\"S\", \"W\", \"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# 使用例\n",
    "# show_graph(bench_marks, processes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_summarized_Fixed_dataframe(BenchMark_name = \"cg\", fixed=\"class\"):\n",
    "\n",
    "    def evaluate_dataframes(df1, df2):\n",
    "        for i in range(len(df1.values.tolist()[0])):\n",
    "            if(df1.values.tolist()[0][i] != df2.values.tolist()[0][i]):\n",
    "                return False\n",
    "        return True\n",
    "    fixed_df = 0\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_df = return_fixed_class(BenchMark=BenchMark_name)\n",
    "    elif (fixed == \"process\"):\n",
    "        fixed_df = return_fixed_process(BenchMark=BenchMark_name)\n",
    "    summary_fixed_df = pd.concat(fixed_df, axis=1)\n",
    "    dropped_summary_fixed_df = summary_fixed_df.drop_duplicates()\n",
    "    dropped_summary_fixed_df_renamed = dropped_summary_fixed_df\n",
    "\n",
    "    for dropped_index_name in dropped_summary_fixed_df.index.values:\n",
    "        dropped_index_name_data = summary_fixed_df.loc[[dropped_index_name]]\n",
    "        replace_name = dropped_index_name\n",
    "        for all_index_name in summary_fixed_df.index.values:\n",
    "            all_index_name_data = summary_fixed_df.loc[[all_index_name]]\n",
    "            if(dropped_index_name == all_index_name):\n",
    "                pass\n",
    "            elif(evaluate_dataframes(dropped_index_name_data, all_index_name_data)):\n",
    "                replace_name += f\", {all_index_name}\"\n",
    "        dropped_summary_fixed_df_renamed = dropped_summary_fixed_df_renamed.rename(index={dropped_index_name: replace_name})\n",
    "    \n",
    "    return dropped_summary_fixed_df_renamed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均絶対パーセント誤差 (MAPE)(Mean Absolute Percent Error (MAPE))を返す関数\n",
    "# 引数として長さの同じ二つのリストをとる\n",
    "# 引数l1: 実測値のリスト\n",
    "# 引数l2: 予測値のリスト\n",
    "\n",
    "def mape_score(l1, l2):\n",
    "    return_num = 0\n",
    "    if(len(l1) != len(l2)):\n",
    "        print(\"引数のリストの長さが異なります\", end=\", \")\n",
    "        return -1\n",
    "    for i in range(len(l1)):\n",
    "        l1_num = l1[i]\n",
    "        l2_num = l2[i]\n",
    "        \n",
    "        return_num += abs((l1_num - l2_num)/l1_num)\n",
    "\n",
    "    return_num /= len(l1)\n",
    "    return_num *= 100\n",
    "    return return_num\n",
    "\n",
    "# 使用例：mape_score([1,2,3,4], [4,3,2,1])\n",
    "type(mape_score([1,2,3,4], [4,3,2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_ratio(base_list :list, test_ratio :float):\n",
    "    test_index = math.floor(len(base_list) * float(test_ratio))\n",
    "    train_list = base_list[:-test_index]\n",
    "    test_list = base_list[-test_index:]\n",
    "    if(test_ratio == 0):\n",
    "        return base_list, []\n",
    "    return train_list, test_list\n",
    "\n",
    "class ModelBase:\n",
    "    def __init__(self, raw_x, raw_y, benchmark_name=\"benchmark_name\", function_name=\"function_name\", test_ratio=0.3):\n",
    "        self.benchmark_name = benchmark_name\n",
    "        self.function_name = function_name\n",
    "        self.xlabel = \"実行時のプロセス数\"\n",
    "        self.ylabel = \"プロセスごとの関数コール回数\"\n",
    "\n",
    "        self.raw_x = np.reshape(raw_x, (-1, 1))\n",
    "        self.raw_y = np.reshape(raw_y, (-1, 1))\n",
    "        self.train_x, self.test_x = split_by_ratio(self.raw_x, test_ratio)\n",
    "        self.train_y, self.test_y = split_by_ratio(self.raw_y, test_ratio)\n",
    "        \n",
    "        if(len(self.train_x) == len(self.test_x) or len(self.train_y) == len(self.test_y)):\n",
    "            print(f\"学習用とテスト用にデータを分割するのに問題が生じています。@{benchmark_name}\")\n",
    "            print(f\"len(self.train_x) == {len(self.train_x)}\")\n",
    "            print(f\"len(self.train_y) ==  {len(self.train_y)}\")\n",
    "            print(f\"len(self.test_x) == {len(self.test_x)}\")\n",
    "            print(f\"len(self.test_y) == {len(self.test_y)}\")\n",
    "        \n",
    "        self.x_model_line = np.reshape(np.arange(start=0.1, stop=self.raw_x.max(), step=0.1), (-1, 1))\n",
    "        self.y_model_line = 0\n",
    "        \n",
    "        self.lr = 0\n",
    "        self.r2_score = 0\n",
    "        \n",
    "    def calc_lr(self):\n",
    "        self.lr = 0\n",
    "     \n",
    "    def calc_r2_score(self):\n",
    "        self.r2_score = 0\n",
    "        \n",
    "    def calc_mae_score(self):\n",
    "        self.mae_score = 0\n",
    "        \n",
    "    def calc_mse_score(self):\n",
    "        self.mse_score = 0\n",
    "        \n",
    "    def calc_rmse_score(self):\n",
    "        self.rmse_score = 0\n",
    "    \n",
    "    def calc_mape_score(self):\n",
    "        self.mape_score = 0\n",
    "        \n",
    "    def plot_graph(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.raw_x, self.raw_y, color=\"red\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLin(ModelBase):\n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.train_x, self.train_y)\n",
    "        \n",
    "    def calc_r2_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.r2_score = r2_score(self.test_y, test_y_predicted)\n",
    "        \n",
    "    def calc_mae_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mae_score = mean_absolute_error(self.test_y, test_y_predicted)\n",
    "        \n",
    "    def calc_mse_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mse_score = mean_squared_error(self.test_y, test_y_predicted)\n",
    "        \n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "        \n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.test_x)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted = self.lr.predict(self.train_x)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "        \n",
    "    def plot_graph(self, save=False, fileName=\"graph.pdf\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        self.y_model_line = self.lr.predict(self.x_model_line)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if(save):\n",
    "            plt.savefig(fileName)\n",
    "        \n",
    "    def predict(self, num):\n",
    "        predicted = self.lr.predict(num)\n",
    "        return(predicted)\n",
    "    \n",
    "    def ModelName(self):\n",
    "        return(\"ModelLin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverter_log10_func(x):\n",
    "    return 10**x\n",
    "\n",
    "class ModelLog10(ModelBase):\n",
    "    \n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.transformer_log10 = sp.FunctionTransformer(func=np.log10, inverse_func=inverter_log10_func)\n",
    "        x_train_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        y_train_log10 = self.transformer_log10.transform(self.train_y)\n",
    "        \n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(x_train_log10, y_train_log10)\n",
    "        \n",
    "    def calc_r2_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.r2_score = r2_score(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_mae_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.mae_score = mean_absolute_error(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_mse_score(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.mse_score = mean_squared_error(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "        \n",
    "    def calc_mape_score(self):\n",
    "        test_x_log10 = self.transformer_log10.transform(self.test_x)\n",
    "        test_y_predicted_log10 = self.lr.predict(test_x_log10)\n",
    "        test_y_predicted = self.transformer_log10.inverse_transform(test_y_predicted_log10)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.lr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "        \n",
    "    def plot_graph(self, save=False, fileName=\"graph.pdf\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        x_model_line_log10 = self.transformer_log10.transform(self.x_model_line)\n",
    "        y_model_line_log10 = self.lr.predict(x_model_line_log10)\n",
    "        self.y_model_line = self.transformer_log10.inverse_transform(y_model_line_log10)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if(save):\n",
    "            plt.savefig(fileName)\n",
    "    \n",
    "    def predict(self, num):\n",
    "        num_log10 = self.transformer_log10.transform(num)\n",
    "        predicted_log10 = self.lr.predict(num_log10)\n",
    "        predicted = self.transformer_log10.inverse_transform(predicted_log10)\n",
    "        return(predicted)\n",
    "    \n",
    "    def return_coef_(self):\n",
    "        return self.lr.coef_\n",
    "    \n",
    "    def return_intercept_(self):\n",
    "        return self.lr.intercept_\n",
    "    \n",
    "    def ModelName(self):\n",
    "        return(\"ModelLog10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse proportion\n",
    "def ip_func(x):\n",
    "    return 1/x\n",
    "\n",
    "class ModelIP(ModelBase):\n",
    "    \n",
    "    def calc_lr(self, AllData=False):\n",
    "        self.transformer_ip = sp.FunctionTransformer(func=ip_func, inverse_func=ip_func)\n",
    "        y_train_ip = self.transformer_ip.transform(self.train_y)        \n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.train_x, y_train_ip)\n",
    "        \n",
    "    def calc_r2_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.r2_score = r2_score(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_mae_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.mae_score = mean_absolute_error(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_mse_score(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.mse_score = mean_squared_error(self.train_y, train_y_predicted)\n",
    "        \n",
    "    def calc_rmse_score(self):\n",
    "        self.calc_mse_score()\n",
    "        self.rmse_score = np.sqrt(self.mse_score)\n",
    "        \n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted_ip = self.lr.predict(self.test_x)\n",
    "        test_y_predicted = self.transformer_ip.inverse_transform(test_y_predicted_ip)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted_ip = self.lr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "        \n",
    "    def plot_graph(self, save=False, fileName=\"graph.pdf\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "        y_model_line_ip = self.lr.predict(self.x_model_line)\n",
    "        self.y_model_line = self.transformer_ip.inverse_transform(y_model_line_ip)\n",
    "        plt.plot(self.x_model_line, self.y_model_line, color=\"red\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if(save):\n",
    "            plt.savefig(fileName)\n",
    "        \n",
    "    def predict(self, num):\n",
    "        predicted_ip = self.lr.predict(num)\n",
    "        predicted = self.transformer_ip.inverse_transform(predicted_ip)\n",
    "        return(predicted)\n",
    "    \n",
    "    def ModelName(self):\n",
    "        return(\"ModelIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBranch(ModelBase):\n",
    "    \n",
    "    def calc_lr(self, AllData=False):\n",
    "        # 後述する t を算出するための処理\n",
    "        max_in_train_y = max(self.train_y)\n",
    "        max_in_train_y_first_index = self.train_y.tolist().index(max_in_train_y)\n",
    "        # 分岐点のインデックスを t とする\n",
    "        t = max_in_train_y_first_index\n",
    "        self.t = t\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.train_x, self.train_y)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.train_x, self.train_y)\n",
    "        else:\n",
    "            self.x_train_1 = self.train_x[:t]\n",
    "            self.x_train_2 = self.train_x[t:]\n",
    "            self.y_train_1 = self.train_y[:t]\n",
    "            self.y_train_2 = self.train_y[t:]\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.x_train_1, self.y_train_1)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.x_train_2, self.y_train_2)\n",
    "        \n",
    "    def calc_mape_score(self):\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            test_y_predicted = self.lr1.predict(self.test_x)\n",
    "            self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "        else:\n",
    "            x_test = self.test_x\n",
    "            y_test = self.test_y\n",
    "            y_test_predicted = self.lr2.predict(x_test)\n",
    "            self.mape_score = float(mape_score(y_test, y_test_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            train_y_predicted = self.lr1.predict(self.train_x)\n",
    "            self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "        else:\n",
    "            train_y_predicted_1 = self.lr1.predict(self.x_train_1)\n",
    "            train_y_predicted_2 = self.lr2.predict(self.x_train_2)\n",
    "            mape_1 = float(mape_score(self.y_train_1, train_y_predicted_1))\n",
    "            mape_2 = float(mape_score(self.y_train_2, train_y_predicted_2))\n",
    "            self.mape_score_InTrain = (mape_1 + mape_2) / 2\n",
    "        \n",
    "    def plot_graph(self, save=False, fileName=\"graph.pdf\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(self.raw_x, self.raw_y)\n",
    "\n",
    "        if(self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            y_model_line = self.lr.predict(x_model_line)\n",
    "            plt.plot(self.x_model_line, y_model_line, color=\"red\")\n",
    "        else:\n",
    "            # 回帰曲線を二つのモデルで分割するための処理\n",
    "            x_model_line = self.x_model_line\n",
    "            t_in_model_line = 0\n",
    "            for i in range(len(x_model_line)):\n",
    "                if (self.train_x[self.t] < x_model_line[i]):\n",
    "                    t_in_model_line = i\n",
    "                    break\n",
    "                else:\n",
    "                    t_in_model_line = i\n",
    "            \n",
    "            x_model_line1 = self.x_model_line[:t_in_model_line]\n",
    "            x_model_line2 = self.x_model_line[t_in_model_line:]\n",
    "            y_model_line1 = self.lr1.predict(x_model_line1)\n",
    "            y_model_line2 = self.lr2.predict(x_model_line2)\n",
    "\n",
    "            plt.plot(x_model_line1, y_model_line1, color=\"red\")\n",
    "            plt.plot(x_model_line2, y_model_line2, color=\"red\")\n",
    "    #         plt.plot(self.test_x, self.test_y, color=\"yellow\")\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if(save):\n",
    "            plt.savefig(fileName)\n",
    "        \n",
    "    def predict(self, num):\n",
    "#         if num < self.raw_x[self.t]:\n",
    "#             predicted = self.lr1.predict(num)\n",
    "#         else:\n",
    "#             predicted = self.lr2.predict(num)\n",
    "        predicted = self.lr2.predict(num)\n",
    "        return(predicted)\n",
    "    \n",
    "    def ModelName(self):\n",
    "        return(\"ModelBranch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_dict_summary_fixed(benchmark_name=\"cg\", fixed=\"class\"):\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_ = return_fixed_class(BenchMark=benchmark_name)\n",
    "    else:\n",
    "        fixed_ = return_fixed_process(BenchMark=benchmark_name)\n",
    "    summary_fixed_ = pd.concat(fixed_ , axis=1)\n",
    "    columns = summary_fixed_.columns.to_numpy()\n",
    "    index = summary_fixed_.index.to_numpy()\n",
    "    if(fixed == \"class\"):\n",
    "        dict_summary_fixed_ = {\"processes\":columns}\n",
    "    else:\n",
    "        dict_summary_fixed_ = {\"class\":columns}\n",
    "    for index_name in index:\n",
    "        dict_summary_fixed_[index_name] = summary_fixed_.T[index_name].to_numpy()\n",
    "    return dict_summary_fixed_\n",
    "\n",
    "# NaNが入った引数のリストをNaNのみを0にして返す関数\n",
    "def return_non_NaN_list(target_list):\n",
    "    for i in range(len(target_list)):\n",
    "        if (math.isnan(target_list[i])):\n",
    "            target_list[i] = 0\n",
    "    return target_list\n",
    "# NaNが入ったリストが引数として渡されるとTrueを返す関数\n",
    "def does_include_nan(target_list):\n",
    "    for i in range(len(target_list)):\n",
    "        if(math.isnan(target_list[i])):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形・対数・反比例モデルでフィッティングを行い、MAPE値をまとめたCSVファイルを作成する関数\n",
    "def generateScoreTable(benchmark_name=\"cg\"):\n",
    "    list_ScoreTable = []\n",
    "    dict_summary_fixed_class = return_dict_summary_fixed(benchmark_name=benchmark_name, fixed=\"class\")\n",
    "    raw_x = dict_summary_fixed_class[\"processes\"]\n",
    "    for content in dict_summary_fixed_class:\n",
    "        if(content == \"processes\"):\n",
    "            continue\n",
    "        raw_y = dict_summary_fixed_class[content]\n",
    "        if(does_include_nan(raw_y)):\n",
    "            continue\n",
    "        # 線形モデル\n",
    "        model_lin = ModelLin(raw_x, raw_y, benchmark_name, content)\n",
    "        model_lin.calc_lr()\n",
    "        model_lin.calc_r2_score()\n",
    "        model_lin.calc_mae_score()\n",
    "        model_lin.calc_mse_score()\n",
    "        model_lin.calc_rmse_score()\n",
    "        model_lin.calc_mape_score()\n",
    "        # logモデル\n",
    "        model_log10 = ModelLog10(raw_x, raw_y, benchmark_name, content)\n",
    "        model_log10.calc_lr()\n",
    "        model_log10.calc_r2_score()\n",
    "        model_log10.calc_mae_score()\n",
    "        model_log10.calc_mse_score()\n",
    "        model_log10.calc_rmse_score()\n",
    "        model_log10.calc_mape_score()\n",
    "        # 反比例モデル\n",
    "        model_ip = ModelIP(raw_x, raw_y, benchmark_name, content)\n",
    "        model_ip.calc_lr()\n",
    "        model_ip.calc_r2_score()\n",
    "        model_ip.calc_mae_score()\n",
    "        model_ip.calc_mse_score()\n",
    "        model_ip.calc_rmse_score()\n",
    "        model_ip.calc_mape_score()\n",
    "        \n",
    "        list_ScoreTable.append([content, model_lin.mape_score, model_log10.mape_score, model_ip.mape_score])\n",
    "    df_ScoreTable = pd.DataFrame(list_ScoreTable)\n",
    "    df_ScoreTable.columns = [\"\", \"x mape\", \"logx mape\", \"1/x mape\"]\n",
    "    df_ScoreTable.set_index(\"\",inplace=True)\n",
    "    df_ScoreTable.to_csv(\"./tmp_GenerateScoreTable/\"+benchmark_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行の内容が同じものをまとめ、行タイトルに重複した行タイトルがまとめられた、データフレームが返される関数\n",
    "\n",
    "def return_summarized_Fixed_dataframe(BenchMark_name = \"cg\", fixed=\"class\"):\n",
    "\n",
    "    def evaluate_dataframes(df1, df2):\n",
    "        for i in range(len(df1.values.tolist()[0])):\n",
    "            if(df1.values.tolist()[0][i] != df2.values.tolist()[0][i]):\n",
    "                return False\n",
    "        return True\n",
    "    fixed_df = 0\n",
    "    if (fixed == \"class\"):\n",
    "        fixed_df = return_fixed_class(BenchMark=BenchMark_name)\n",
    "    elif (fixed == \"process\"):\n",
    "        fixed_df = return_fixed_process(BenchMark=BenchMark_name)\n",
    "    summary_fixed_df = pd.concat(fixed_df, axis=1)\n",
    "    dropped_summary_fixed_df = summary_fixed_df.drop_duplicates()\n",
    "    dropped_summary_fixed_df_renamed = dropped_summary_fixed_df\n",
    "\n",
    "    for dropped_index_name in dropped_summary_fixed_df.index.values:\n",
    "        dropped_index_name_data = summary_fixed_df.loc[[dropped_index_name]]\n",
    "        replace_name = dropped_index_name\n",
    "        for all_index_name in summary_fixed_df.index.values:\n",
    "            all_index_name_data = summary_fixed_df.loc[[all_index_name]]\n",
    "            if(dropped_index_name == all_index_name):\n",
    "                pass\n",
    "            elif(evaluate_dataframes(dropped_index_name_data, all_index_name_data)):\n",
    "                replace_name += f\", {all_index_name}\"\n",
    "        dropped_summary_fixed_df_renamed = dropped_summary_fixed_df_renamed.rename(index={dropped_index_name: replace_name})\n",
    "    \n",
    "    return dropped_summary_fixed_df_renamed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~_excludeBTSP, ~~~_onlyBTSP はそれぞれのベンチマークで取得したプロセス数\n",
    "processes_excludeBTSP = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "processes_onlyBTSP = [1, 4, 16, 64, 256]\n",
    "\n",
    "# 引数に横軸：プロセス数orベンチマーククラス, 縦軸：関数名となっているデータフレームを取る\n",
    "# 返値として\n",
    "# rowData:プロセス数のリスト もしくは プロセス数のリスト (引数に由来)\n",
    "# 各種関数名：実行回数のリスト\n",
    "# 以上のような関係の辞書を返す\n",
    "\n",
    "def return_dict_Data(DataFrame):\n",
    "    columns = DataFrame.columns.to_numpy()\n",
    "    index = DataFrame.index.to_numpy()\n",
    "    # 返値となる辞書return_dictに引数のデータフレームの列名(プロセス数orベンチマーククラス)を格納\n",
    "    return_dict = {\"rowData\":columns}\n",
    "    for index_name in index:\n",
    "        return_dict[index_name] = DataFrame.T[index_name].to_numpy()\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "\n",
    "### 引数はx軸となる値のリスト, y軸となる値のリスト, 関数名の文字列, 訓練データでMAPEを算出するかどうかの真偽\n",
    "### 返り値は次のようなリスト\n",
    "### [<関数名の文字列>, <線形モデルのMAPE値>, <対数モデルのMAPE値>, <反比例モデルのMAPE値>, <分岐モデルのMAPE値>]\n",
    "def return_Mape_row_list(x :list, y :list, function_name :str, test_ratio=0.3, train=False):\n",
    "\n",
    "    # 変数：model_lin\n",
    "    # 線形モデル\n",
    "    model_lin = ModelLin(x, y, test_ratio=test_ratio)\n",
    "    if(test_ratio == 0):\n",
    "        model_lin.train_x = model_lin.raw_x\n",
    "        model_lin.train_y = model_lin.raw_y\n",
    "    model_lin.calc_lr()\n",
    "    model_lin.calc_mape_score()\n",
    "    model_lin.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_log\n",
    "    # 対数モデル\n",
    "    model_log = ModelLog10(x, y, test_ratio=test_ratio)\n",
    "    if(test_ratio == 0):\n",
    "        model_log.train_x = model_log.raw_x\n",
    "        model_log.train_y = model_log.raw_y\n",
    "    model_log.calc_lr()\n",
    "    model_log.calc_mape_score()\n",
    "    model_log.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_ip\n",
    "    # 反比例モデル\n",
    "    model_ip = ModelIP(x, y, test_ratio=test_ratio)\n",
    "    if(test_ratio == 0):\n",
    "        model_ip.train_y = model_ip.raw_y\n",
    "        model_ip.train_x = model_ip.raw_x\n",
    "    model_ip.calc_lr()\n",
    "    model_ip.calc_mape_score()\n",
    "    model_ip.calc_mape_score_InTrain()\n",
    "\n",
    "    # 変数：model_branch\n",
    "    # 特異点付き条件分岐モデル\n",
    "    model_branch = ModelBranch(x, y, test_ratio=test_ratio)\n",
    "    if(test_ratio == 0):\n",
    "        model_branch.train_x = model_branch.raw_x\n",
    "        model_branch.train_y = model_branch.raw_y\n",
    "    model_branch.calc_lr()\n",
    "    model_branch.calc_mape_score()\n",
    "    model_branch.calc_mape_score_InTrain()\n",
    "\n",
    "    if(train==True):\n",
    "        lin_score = model_lin.mape_score_InTrain\n",
    "        log_score = model_log.mape_score_InTrain\n",
    "        ip_score = model_ip.mape_score_InTrain\n",
    "        branch_score = model_branch.mape_score_InTrain\n",
    "    else:\n",
    "        lin_score = model_lin.mape_score\n",
    "        log_score = model_log.mape_score\n",
    "        ip_score = model_ip.mape_score\n",
    "        branch_score = model_branch.mape_score\n",
    "\n",
    "    # 変数：return_list\n",
    "    # 返り値となるリスト\n",
    "    return_list = [function_name, lin_score, log_score, ip_score, branch_score]\n",
    "    return(return_list)\n",
    "\n",
    "### 引数は「return_dict_DataFrame()」の返値, テストとして何割のデータを用いるかの割合, 訓練データでMAPEを算出するかの真偽\n",
    "### 返り値は行・列がモデル名・関数名で要素がMAPE値となっているDataFrame\n",
    "def return_MapeTable_per_benchmark(dict_data :dict, test_ratio, train=False):\n",
    "\n",
    "    # 変数：_names\n",
    "    # 引数の辞書のプロセス数もしくはベンチマーククラスの文字列のリスト\n",
    "    _names = dict_data['rowData']\n",
    "    # 変数：function_names\n",
    "    # 引数の辞書の関数名の文字列のリスト\n",
    "    function_names = list(dict_data.keys())\n",
    "    function_names.remove('rowData')\n",
    "\n",
    "    # リスト変数：before_DataFrame_list\n",
    "    # 最終的にDataFrameとする元となるリスト\n",
    "    before_DataFrame_list = []\n",
    "    collumn_names = [\"function name\", \"Linear model\", \"Log10 model\", \"Inverse model\", \"Branch model\"]\n",
    "    for function_name in function_names:\n",
    "        if(does_include_nan(dict_data[function_name])):\n",
    "            continue\n",
    "        before_DataFrame_list.append(return_Mape_row_list(x = _names,y = dict_data[function_name],function_name = function_name, test_ratio=test_ratio, train=train))\n",
    "    \n",
    "    # 変数：return_df\n",
    "    # 返り値となるリスト\n",
    "    return_df = pd.DataFrame(before_DataFrame_list)\n",
    "    return_df.columns = collumn_names\n",
    "    return_df = return_df.set_index(\"function name\")\n",
    "\n",
    "    return(return_df)\n",
    "\n",
    "\n",
    "### 構造体的に利用可能なクラス MapeData\n",
    "### 各ベンチマークの各モデルごとに作成される。\n",
    "### 要素として、割合, 最大値, 最小値 がある。\n",
    "class MapeData:\n",
    "    def __init__(self):\n",
    "        self.ratio = 0\n",
    "        self.max = np.nan\n",
    "        self.min = np.nan\n",
    "        self.appearance = 0\n",
    "\n",
    "    def printData(self):\n",
    "        print(f\"{self.ratio}({self.min}, {self.max})\")\n",
    "\n",
    "    def return_Data(self):\n",
    "        max_min = \"\"\n",
    "        if(self.min is np.nan):\n",
    "            max_min = \"(NoData)\"\n",
    "        else:\n",
    "            max_min = f\"({self.min}, {self.max})\"\n",
    "        return(f\"{self.ratio}%{max_min}\")\n",
    "\n",
    "### 引数に「return_MapeTable_per_benchmark()」の返り値, ベンチマーク名, (オプショナル)中間データの詳細をとる\n",
    "### 返り値として次のようなリストを返す\n",
    "### [<線形モデルのMAPEに関する奴>, <対数モデルのMAPEに関する奴>, <反比例モデルのMAPEに関する奴>, <ベンチマーク名>]\n",
    "\n",
    "def return_MapeTable_row(MapeDataframe_detail, benchmark_name:str):\n",
    "    \n",
    "    # 引数として渡されたデータフレームの行列名をindex, columnsに格納\n",
    "    columns = MapeDataframe_detail.columns.to_numpy()\n",
    "    index = MapeDataframe_detail.index.to_numpy()\n",
    "\n",
    "    # この関数で返すリストの要素の準備\n",
    "    MapeLin = MapeData()\n",
    "    MapeLog = MapeData()\n",
    "    MapeIP = MapeData()\n",
    "    MapeBr = MapeData()\n",
    "    return_list = [MapeLin, MapeLog, MapeIP, MapeBr, benchmark_name ]\n",
    "\n",
    "    # 返り値のリストの各要素の値を更新\n",
    "    for function_name in index:\n",
    "        MapeData_per_function = MapeDataframe_detail.loc[function_name].to_list()\n",
    "        min_mape = min(MapeData_per_function)\n",
    "        min_mape_index = MapeData_per_function.index(min_mape)\n",
    "        rounded_min_mape = int(min_mape * 10) / 10\n",
    "        return_list[min_mape_index].appearance += 1\n",
    "        if(return_list[min_mape_index].max is np.nan):\n",
    "            return_list[min_mape_index].max = rounded_min_mape\n",
    "            return_list[min_mape_index].min = rounded_min_mape\n",
    "        if(return_list[min_mape_index].min > min_mape):\n",
    "            return_list[min_mape_index].min = rounded_min_mape\n",
    "        elif(return_list[min_mape_index].max < min_mape):\n",
    "            return_list[min_mape_index].max = rounded_min_mape\n",
    "    sum_num = 0\n",
    "    # 集計データから割合を算出\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        sum_num += return_list[i].appearance\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        return_list[i].ratio = int(100 * return_list[i].appearance/sum_num)\n",
    "    # 割合の合計が100になるように調整\n",
    "    exclude_index0_ratios = 0\n",
    "    for i in range(return_list.index(benchmark_name)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        exclude_index0_ratios += return_list[i].ratio\n",
    "    return_list[0].ratio = 100 - exclude_index0_ratios\n",
    "\n",
    "    return(return_list)\n",
    "\n",
    "def save_MapeTable(MapeTable, suffix=\"\"):\n",
    "    tmp_table = MapeTable.copy()\n",
    "    columns = MapeTable.columns.to_numpy()\n",
    "    index = MapeTable.index.to_numpy()\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(len(index)):\n",
    "            tmp_table.iat[j, i] = tmp_table.iat[j, i].return_Data()\n",
    "    tmp_table.to_csv(f\"./tmp_GenerateResources/MapeTable_{str(suffix)}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数はx軸となる値のリスト, y軸となる値のリスト, 関数名の文字列\n",
    "### 返り値は次のようなリスト\n",
    "### [<関数名の文字列>, <線形モデル>, <対数モデル>, <反比例モデル>, <分岐モデル>]\n",
    "def return_Model_row_list(x :list, y :list, function_name :str, test_ratio=0.3, train=False):\n",
    "\n",
    "    # 変数：model_lin\n",
    "    # 線形モデル\n",
    "    model_lin = ModelLin(x, y, test_ratio=test_ratio)\n",
    "    model_lin.calc_lr()\n",
    "    if(train):\n",
    "        model_lin.calc_mape_score_InTrain()\n",
    "    else:\n",
    "        model_lin.calc_mape_score()\n",
    "    \n",
    "\n",
    "    # 変数：model_log\n",
    "    # 対数モデル\n",
    "    model_log = ModelLog10(x, y, test_ratio=test_ratio)\n",
    "    model_log.calc_lr()\n",
    "    if(train):\n",
    "        model_log.calc_mape_score_InTrain()\n",
    "    else:\n",
    "        model_log.calc_mape_score()\n",
    "    \n",
    "\n",
    "    # 変数：model_ip\n",
    "    # 反比例モデル\n",
    "    model_ip = ModelIP(x, y, test_ratio=test_ratio)\n",
    "    model_ip.calc_lr()\n",
    "    if(train):\n",
    "        model_ip.calc_mape_score_InTrain()\n",
    "    else:\n",
    "        model_ip.calc_mape_score()\n",
    "    \n",
    "\n",
    "    # 変数：model_branch\n",
    "    # 特異点付き条件分岐モデル\n",
    "    model_branch = ModelBranch(x, y, test_ratio=test_ratio)\n",
    "    model_branch.calc_lr()\n",
    "    if(train):\n",
    "        model_branch.calc_mape_score_InTrain()\n",
    "    else:\n",
    "        model_branch.calc_mape_score()\n",
    "    \n",
    "\n",
    "    # 変数：return_list\n",
    "    # 返り値となるリスト\n",
    "    return_list = [function_name, model_lin, model_log, model_ip, model_branch]\n",
    "    return(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、収集するベンチマークのリスト、実行したい学習の割合、固定したいベンチマーククラス\n",
    "### 返値は無し\n",
    "### 実行すると、\"./tmpGenerateResources\" に \"<ベンチマーク名>_FixedClassTrain_<テスト割合>.csv\" という形式でファイルが生成される\n",
    "\n",
    "def GenerateMapeTableFixClass(Benchmarks=[\"cg\"], TestRatios=[\"0.3\"], FixBenchmarksClass=\"C\"):\n",
    "    for test_ratio in TestRatios:\n",
    "        print(f\"test_ratio={test_ratio}\")\n",
    "        print(f\"train_list, test_list = {split_by_ratio(processes_excludeBTSP, test_ratio)} on processes_excludeBTSP\")\n",
    "        print(f\"train_list, test_list = {split_by_ratio(processes_onlyBTSP, test_ratio)} on processes_onlyBTSP\")\n",
    "        print(f\"\\n\")\n",
    "        fixed_class_list = [0] * len(Benchmarks)\n",
    "        for i in range(len(Benchmarks)):\n",
    "            if(Benchmarks[i] == \"bt\" or Benchmarks[i] == \"sp\"):\n",
    "                process = processes_onlyBTSP\n",
    "            else:\n",
    "                process = processes_excludeBTSP\n",
    "            fixed_class_list[i] = return_fixed_class(BenchMark=Benchmarks[i], Processes=processes, FixedBenchMarkClass=FixBenchmarksClass)\n",
    "        fixed_class_DataFrame = [0] * len(fixed_class_list)\n",
    "        for i in range(len(fixed_class_list)):\n",
    "            fixed_class_DataFrame[i] = pd.concat(fixed_class_list[i], axis=1)\n",
    "        for i in range(len(fixed_class_DataFrame)):\n",
    "            dict_data = return_dict_Data(fixed_class_DataFrame[i])\n",
    "            MapeTable_per_benchmark = return_MapeTable_per_benchmark(dict_data, test_ratio=test_ratio, train=True)\n",
    "            MapeTable_per_benchmark.to_csv(f\"./tmp_GenerateResources/{Benchmarks[i]}_FixedClassTrain_{test_ratio}.csv\")\n",
    "# 使用例       \n",
    "# GenerateMapeTableFixClass(Benchmarks=[\"cg\", \"lu\"], TestRatios=[0.3, 0.7], FixBenchmarksClass=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、収集するベンチマークのリスト、実行したい学習の割合、固定したい実行プロセス数\n",
    "### 返値は無し\n",
    "### 実行すると、\"./tmpGenerateResources\" に \"<ベンチマーク名>_FixedProcessTrain_<テスト割合>.csv\" という形式でファイルが生成される\n",
    "\n",
    "def GenerateMapeTableFixProcess(Benchmarks=[\"cg\"], BenchmarkClasses=[\"A\", \"B\", \"C\", \"D\"], BenchmarkClasses_on_num=[1, 4, 16, 256], TestRatios=[\"0.3\"], FixProcess=64):\n",
    "    for test_ratio in TestRatios:\n",
    "        print(f\"test_ratio={test_ratio}\")\n",
    "        print(f\"train_list, test_list = {split_by_ratio(BenchmarkClasses, test_ratio)} on BenchmarkClasses\")\n",
    "        print(f\"\\n\")\n",
    "        fixed_Process_list = [0] * len(benchmarks)\n",
    "        for i in range(len(fixed_Process_list)):\n",
    "            fixed_Process_list[i] = return_fixed_process(BenchMark=Benchmarks[i], BenchMarkClasses=BenchmarkClasses, FixedProcess=FixProcess)\n",
    "        fixed_Process_DataFrame = [0] * len(fixed_Process_list)\n",
    "        for i in range(len(fixed_Process_DataFrame)):\n",
    "            fixed_Process_DataFrame[i] = pd.concat(fixed_Process_list[i], axis=1)\n",
    "            \n",
    "        for i in range(len(fixed_Process_DataFrame)):\n",
    "            dict_data = return_dict_Data(fixed_Process_DataFrame[i])\n",
    "            dict_data['rowData'] = BenchmarkClasses_on_num\n",
    "            try:\n",
    "                MapeTable_per_benchmark=return_MapeTable_per_benchmark(dict_data, test_ratio=test_ratio, train=True)\n",
    "            except:\n",
    "                print(f\"MAPEを算出するのに問題発生@{Benchmarks[i]}\")\n",
    "                continue\n",
    "            MapeTable_per_benchmark.to_csv(f\"./tmp_GenerateResources/{benchmarks[i]}_FixedProcessTrain_{test_ratio}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertBencharkClass_inNPB(Alphabet :str):\n",
    "    if(Alphabet == \"A\"):\n",
    "        return (1)\n",
    "    elif(Alphabet == \"B\"):\n",
    "        return (4)\n",
    "    elif(Alphabet == \"C\"):\n",
    "        return (16)\n",
    "    elif(Alphabet == \"D\"):\n",
    "        return (256)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def ConvertBencharkClass_inNPB_fromNum(number):\n",
    "    number = int(number)\n",
    "    if(number==1):\n",
    "        return(\"A\")\n",
    "    elif(number==4):\n",
    "        return(\"B\")\n",
    "    elif(number==16):\n",
    "        return(\"C\")\n",
    "    elif(number==256):\n",
    "        return(\"D\")\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ConvertBenchmarkClasses(InputList=[\"A\", \"B\", \"C\", \"D\"]):\n",
    "    ReturnList = []\n",
    "    for content in InputList:\n",
    "        ReturnList.append(ConvertBencharkClass_inNPB(content))\n",
    "    return(ReturnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_OptimalProcessesList(BenchmarkName=\"cg\"):\n",
    "    if(BenchmarkName==\"bt\" or BenchmarkName==\"sp\"):\n",
    "        return(processes_onlyBTSP)\n",
    "    else:\n",
    "        return(processes_excludeBTSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、ベンチマーク名、列名、固定する値、プロセスorクラスのどちらで固定するか\n",
    "### 返値は次のような形式の辞書\n",
    "### rowData:[プロセス数]or[ベンチマーククラス]\n",
    "### <関数名>:[実行回数]\n",
    "def returnDictForModelDataFrame(BenchmarkName=\"cg\", rowData=[\"A\", \"B\", \"C\", \"D\"], fix=\"64\", fixed=\"Process\"):\n",
    "    if(fixed==\"Process\"):\n",
    "        FixedProcessList = return_fixed_process(BenchMark=BenchmarkName, BenchMarkClasses=rowData, FixedProcess=fix)\n",
    "        FixedProcessDataFrame = pd.concat(FixedProcessList, axis=1)\n",
    "        DictData = return_dict_Data(FixedProcessDataFrame)\n",
    "    elif(fixed==\"Class\"):\n",
    "        FixedClassList = return_fixed_class(BenchMark=BenchmarkName, Processes=rowData, FixedBenchMarkClass=fix)\n",
    "        FixedClassDataFrame = pd.concat(FixedClassList, axis=1)\n",
    "        DictData = return_dict_Data(FixedClassDataFrame)\n",
    "    return(DictData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、returnDictForModelDataFrame()の返値, rowData, 関数名のリスト, テストに割り当てる割合\n",
    "### 返値はリスト\n",
    "### [<関数名>, <学習済みデータ1>, ... , <学習済みデータn>]\n",
    "def return_ModelDataSourceList(DictData, x_list, Index, test_ratio=0.3):\n",
    "    ModelDataSourceList = []\n",
    "    for FunctionName in Index:\n",
    "        y_list = DictData[FunctionName]\n",
    "        if(does_include_nan(y_list)):\n",
    "            continue\n",
    "        ModelDataSourceList.append(return_Model_row_list(x=x_list, y=y_list, function_name=FunctionName, test_ratio=test_ratio, train=True))\n",
    "    return(ModelDataSourceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、関数名、rowData, テストに割り当てる割合, 固定するプロセス数orクラス, クラスで固定するかプロセス数で固定するか\n",
    "### 返値はDataFrame\n",
    "### 行名は関数名で、列名はモデルの種別\n",
    "### それぞれの要素は学習済みデータ\n",
    "def return_Models_per_Benchmark(BenchmarkName=\"cg\", rowData=[1, 4, 16, 256], TestRate=0.3, fix=\"C\", fixed=\"Class\"):\n",
    "    # 変数：MapeTable\n",
    "    # ベンチマークのMAPE表\n",
    "    # fixedが\"Class\"ならクラスで固定され、fixedが\"Process\"ならプロセス数で固定されたMAPE表を読み込んでいる。\n",
    "    MapeTable = pd.read_csv(f\"./tmp_GenerateResources/{BenchmarkName}_Fixed{fixed}Train_{test_ratio}.csv\")\n",
    "    MapeTable = MapeTable.set_index(\"function name\")\n",
    "    \n",
    "    # 変数：MapeTableColumns, MapTableIndex\n",
    "    # MapeTableの列名・行名\n",
    "    MapeTableColumns = MapeTable.columns.to_numpy()\n",
    "    MapeTableIndex = MapeTable.index.to_numpy()\n",
    "    \n",
    "    # 変数：ModelDataFrame\n",
    "    # MapeTableにおける各関数の学習済みモデルが格納される\n",
    "    checked_rowData=rowData\n",
    "    if(fixed==\"Process\"):\n",
    "        checked_rowData=ConvertBenchmarkClasses(rowData)\n",
    "    DictData = returnDictForModelDataFrame(BenchmarkName, rowData=rowData, fix=fix, fixed=fixed)\n",
    "    ModelDataFrameSourceList = return_ModelDataSourceList(DictData=DictData, x_list=checked_rowData, Index=MapeTableIndex, test_ratio=TestRate)\n",
    "    ModelDataFrameSourceListCollumnsName = [\"FunctionName\", \"ModelLin\", \"ModelLog\", \"ModelIp\", \"ModelBranch\"]\n",
    "    ModelDataFrame = pd.DataFrame(ModelDataFrameSourceList)\n",
    "    ModelDataFrame.columns = ModelDataFrameSourceListCollumnsName\n",
    "    ModelDataFrame = ModelDataFrame.set_index(\"FunctionName\")\n",
    "    return(ModelDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引数は、読み込んだベンチマークごとのMAPE表, 各関数の全てのモデルにおける学習済みモデル\n",
    "### 返値は辞書\n",
    "### キーは<関数名>でバリューは<学習済みモデル>\n",
    "def return_BestModelsDict(MapeTable, ModelDataFrame):\n",
    "    BestModelsDict = {}\n",
    "    ModelNames = ModelDataFrame.columns.to_list()\n",
    "    ModelDataFrameIndexNameList = ModelDataFrame.index.to_numpy()\n",
    "    for FunctionName in ModelDataFrameIndexNameList:\n",
    "        MapeInFunction = MapeTable.loc[FunctionName].to_list()\n",
    "        SmallestModelIndex = MapeInFunction.index(min(MapeInFunction))\n",
    "        SmallestModelName = ModelNames[SmallestModelIndex]\n",
    "        BestModelsDict[FunctionName] = ModelDataFrame.at[FunctionName, SmallestModelName]\n",
    "    return BestModelsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BenchmarkClasses =[\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "### 引数は\n",
    "### 返値は辞書\n",
    "### キーは<関数名>でバリューは<学習済みモデル>の辞書\n",
    "### 学習済みモデルのデータフレームの取得に失敗した場合はFalseを返す\n",
    "def generateBestModelDict(TestRatio=0.3, BenchmarkName=\"ft\", fixed=\"Class\", fix=\"C\", rowData=[1, 2, 4, 8, 16, 32, 64, 128, 256]):\n",
    "    \n",
    "    # 変数：MapeTable\n",
    "    # ベンチマークのMAPE表\n",
    "    if(fixed==\"Class\"):\n",
    "        file_name = f\"{BenchmarkName}_FixedClassTrain_{TestRatio}.csv\"\n",
    "    else:\n",
    "        file_name = f\"{BenchmarkName}_FixedProcessTrain_{TestRatio}.csv\"\n",
    "    file_path = f\"./tmp_GenerateResources/{file_name}\"\n",
    "    MapeTable = pd.read_csv(file_path)\n",
    "    MapeTable = MapeTable.set_index(\"function name\")\n",
    "#     try:\n",
    "#         ModelDataFrame = return_Models_per_Benchmark(BenchmarkName=benchmark, rowData=processes, TestRate=test_ratio, fix=fix, fixed=fixed)\n",
    "#     except:\n",
    "#         print(f\"\\n全てのモデル形式で学習済みモデルを作成しているor集めている最中に問題が発生しました@{benchmark}\\n\")\n",
    "#         return False\n",
    "    ModelDataFrame = return_Models_per_Benchmark(BenchmarkName=benchmark, rowData=rowData, TestRate=test_ratio, fix=fix, fixed=fixed)\n",
    "\n",
    "    BestModelsDict = return_BestModelsDict(MapeTable=MapeTable, ModelDataFrame=ModelDataFrame)\n",
    "    return(BestModelsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、テストの割合のリスト, rowData\n",
    "# 返値はrowDataのtrainのリストを文字列化したもののリスト\n",
    "def return_StringTrainList(TestRatio=[0.3, 0.5], rowData=[1,2,4,8]):\n",
    "    returnList = []\n",
    "    for test_ratio in TestRatio:\n",
    "        train_list, test_list = split_by_ratio(base_list=rowData, test_ratio=test_ratio)\n",
    "        returnList.append(f\"{train_list}\")\n",
    "    return(returnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数は、関数名, 予測値, ベンチマーク名, ベンチマーククラス, 実行プロセス数\n",
    "# 返値は予測値と実測値の誤差率(= 予測値/実測値 * 100)\n",
    "def return_ErrorRate(FunctionName=\"CG\", PredictNum=256, BenchmarkName=\"cg\", BenchmarkClass=\"D\", Process=256):\n",
    "    target_csv = pd.read_csv(f\"./csv_files/pprof_{BenchmarkName}{BenchmarkClass}{Process}.csv\")\n",
    "    target_csv = target_csv.set_index(\"Name\")\n",
    "    try:\n",
    "        RealNum = target_csv.loc[FunctionName, \"#Call\"]\n",
    "    except:\n",
    "        print(f\"該当する関数はありませんでした@{Benchmakname}@{FunctionName}\")\n",
    "        RealNum = False\n",
    "    if(RealNum != False):\n",
    "        returnNum = abs(RealNum-PredictNum)/RealNum*100\n",
    "        return(returnNum)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 予測値を返す関数\n",
    "### 引数は、学習済みモデル, 予測したいプロセス数もしくはベンチマーククラスを数値化したもの\n",
    "\n",
    "def return_Predicted(LearnedModel, num):\n",
    "    Input = np.reshape(num, (-1, 1))\n",
    "    PredictedData = LearnedModel.predict(Input)\n",
    "    PredictedList = PredictedData.tolist()\n",
    "    Predict = PredictedList[0][0]\n",
    "    return(Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### return_FixedClassModelDF\n",
    "##### 引数で指定されたベンチマークでベンチマーククラスを固定した際の各関数について最適な学習済みモデルを返す関数\n",
    "def return_FixedClassModelDF(benchmark=\"cg\", FixClass=\"C\"):\n",
    "    ProcessExcludeBTSP = [1,2,4,8,16,32,64,128,256]\n",
    "    ProcessIncludeBTSP = [1, 4, 16, 64, 256]\n",
    "    TestRates = [0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    if(benchmark == \"bt\" or benchmark == \"sp\"):\n",
    "        processes=ProcessIncludeBTSP\n",
    "        TestRates = [0, 0.2, 0.4, 0.6, 0.8]\n",
    "    else:\n",
    "        processes=ProcessExcludeBTSP\n",
    "\n",
    "    RawDataFrameSource = return_fixed_class(BenchMark=benchmark, Processes=processes,FixedBenchMarkClass=FixClass)\n",
    "    RawDataFrame = pd.concat(RawDataFrameSource, axis=1)\n",
    "\n",
    "    x_list = RawDataFrame.columns.tolist()\n",
    "    FunctionNames = RawDataFrame.index.tolist()\n",
    "    ModelsInBenchmark = {}\n",
    "    for FunctionName in FunctionNames:\n",
    "        BestModelsPerFunction = [0] * len(TestRates)\n",
    "        y_list = RawDataFrame.loc[FunctionName].tolist()\n",
    "        for test_ratio in TestRates:\n",
    "            x_list_splited = split_by_ratio(x_list, test_ratio)[0]\n",
    "            y_list_splited = split_by_ratio(y_list, test_ratio)[0]\n",
    "            if(does_include_nan(y_list)):\n",
    "                continue\n",
    "            Models = return_Model_row_list(x=x_list_splited, y=y_list_splited, function_name=FunctionName, test_ratio=0, train=True)[1:]\n",
    "            MapeScoreInTrains = [0] * len(Models)\n",
    "            for i in range(len(Models)):\n",
    "                MapeScoreInTrains[i] = Models[i].mape_score_InTrain\n",
    "            BestModelsPerFunction[TestRates.index(test_ratio)] = Models[MapeScoreInTrains.index(min(MapeScoreInTrains))]\n",
    "        if(0 in BestModelsPerFunction):\n",
    "            continue\n",
    "        ModelsInBenchmark[FunctionName] = BestModelsPerFunction\n",
    "\n",
    "    ModelDF = pd.DataFrame.from_dict(ModelsInBenchmark, orient='index')\n",
    "    ModelDFcolumns = []\n",
    "    for test_ratio in TestRates:\n",
    "        ModelDFcolumns.append(f\"{split_by_ratio(x_list, test_ratio)[0]}\")\n",
    "    ModelDF.columns = ModelDFcolumns\n",
    "    return(ModelDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### return_FixedProcessModelDF\n",
    "##### 引数で指定されたベンチマークで実行プロセス数を固定した際の各関数について最適な学習済みモデルを返す関数\n",
    "def return_FixedProcessModelDF(benchmark=\"cg\", FixProcess=\"64\"):\n",
    "    BenchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "    TestRates = [0, 0.4, 0.5, 0.8]\n",
    "    BenchmarkClassesOnNum = ConvertBenchmarkClasses(BenchmarkClasses)\n",
    "\n",
    "    RawDataFrameSource = return_fixed_process(BenchMark=benchmark, BenchMarkClasses=BenchmarkClasses ,FixedProcess=FixProcess)\n",
    "    RawDataFrame = pd.concat(RawDataFrameSource, axis=1)\n",
    "    x_list_base = RawDataFrame.columns.tolist()\n",
    "    x_list = ConvertBenchmarkClasses(copy.deepcopy(x_list_base))\n",
    "    FunctionNames = RawDataFrame.index.tolist()\n",
    "    ModelsInBenchmark = {}\n",
    "    for FunctionName in FunctionNames:\n",
    "        BestModelsPerFunction = [0] * len(TestRates)\n",
    "        y_list = RawDataFrame.loc[FunctionName].tolist()\n",
    "        for test_ratio in TestRates:\n",
    "            x_list_splited = split_by_ratio(x_list, test_ratio)[0]\n",
    "            y_list_splited = split_by_ratio(y_list, test_ratio)[0]\n",
    "            if(does_include_nan(y_list)):\n",
    "                continue\n",
    "            Models = return_Model_row_list(x=x_list_splited, y=y_list_splited, function_name=FunctionName, test_ratio=0, train=True)[1:]\n",
    "            MapeScoreInTrains = [0] * len(Models)\n",
    "            for i in range(len(Models)):\n",
    "                MapeScoreInTrains[i] = Models[i].mape_score_InTrain\n",
    "            BestModelsPerFunction[TestRates.index(test_ratio)] = Models[MapeScoreInTrains.index(min(MapeScoreInTrains))]\n",
    "        if(0 in BestModelsPerFunction):\n",
    "            continue\n",
    "        ModelsInBenchmark[FunctionName] = BestModelsPerFunction\n",
    "\n",
    "    ModelDF = pd.DataFrame.from_dict(ModelsInBenchmark, orient='index')\n",
    "    ModelDFcolumns = []\n",
    "    for test_ratio in TestRates:\n",
    "        ModelDFcolumns.append(f\"{split_by_ratio(x_list_base, test_ratio)[0]}\")\n",
    "    ModelDF.columns = ModelDFcolumns\n",
    "    return(ModelDF)\n",
    "\n",
    "ModelDF = return_FixedProcessModelDF(benchmark=\"cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_MapeTableFixedClass(benchmark = \"cg\"):\n",
    "    DirPath = \"./tmp_GenerateResources/\"\n",
    "    try:\n",
    "        ModelDFfixedClass = return_FixedClassModelDF(benchmark=benchmark)\n",
    "    except:\n",
    "        print(f\"return_FixedClassModelDF({benchmark})の実行に失敗しました\")\n",
    "        return -1\n",
    "    ModelDFfixedClass_LowestMape = ModelDFfixedClass\n",
    "    index = ModelDFfixedClass_LowestMape.index.tolist()\n",
    "    columns = ModelDFfixedClass_LowestMape.columns.tolist()\n",
    "    for column in columns:\n",
    "        for row in index:\n",
    "            ModelDFfixedClass_LowestMape.at[row, column] = ModelDFfixedClass.at[row, column].mape_score_InTrain\n",
    "    average = ModelDFfixedClass_LowestMape.mean()\n",
    "    average.name = \"Average\"\n",
    "    ModelDFfixedClass_LowestMape.append(average)\n",
    "    return ModelDFfixedClass_LowestMape\n",
    "\n",
    "def return_MapeTableFixedProcess(benchmark = \"cg\"):\n",
    "    DirPath = \"./tmp_GenerateResources/\"\n",
    "    try:\n",
    "        ModelDFfixedProcess = return_FixedProcessModelDF(benchmark=benchmark)\n",
    "    except:\n",
    "        print(f\"return_FixedProcessModelDF({benchmark})の実行に失敗しました\")\n",
    "        return -1\n",
    "    ModelDFfixedProcess_LowestMape = ModelDFfixedProcess\n",
    "    index = ModelDFfixedProcess_LowestMape.index.tolist()\n",
    "    columns = ModelDFfixedProcess_LowestMape.columns.tolist()\n",
    "    for column in columns:\n",
    "        for row in index:\n",
    "            ModelDFfixedProcess_LowestMape.at[row, column] = ModelDFfixedProcess.at[row, column].mape_score_InTrain\n",
    "    average = ModelDFfixedProcess_LowestMape.mean()\n",
    "    average.name = \"Average\"\n",
    "    ModelDFfixedProcess_LowestMape.append(average)\n",
    "    return ModelDFfixedProcess_LowestMape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### return_MapeTableFixed<Class or Process>\n",
    "### 引数で渡されたベンチマークについて、<クラス or プロセス>を固定したMAPE表となったデータフレームを返す\n",
    "\n",
    "def return_MapeTableFixedClass(benchmark = \"cg\", FixClass=\"C\"):\n",
    "    DirPath = \"./tmp_GenerateResources/\"\n",
    "    try:\n",
    "        ModelDFfixedClass = return_FixedClassModelDF(benchmark=benchmark, FixClass=FixClass)\n",
    "    except:\n",
    "        print(f\"return_FixedClassModelDF({benchmark})の実行に失敗しました\")\n",
    "        return -1\n",
    "    ModelDFfixedClass_LowestMape = ModelDFfixedClass\n",
    "    index = ModelDFfixedClass_LowestMape.index.tolist()\n",
    "    columns = ModelDFfixedClass_LowestMape.columns.tolist()\n",
    "    for column in columns:\n",
    "        for row in index:\n",
    "            ModelDFfixedClass_LowestMape.at[row, column] = ModelDFfixedClass.at[row, column].mape_score_InTrain\n",
    "    average = ModelDFfixedClass_LowestMape.mean()\n",
    "    average.name = \"Average\"\n",
    "    ModelDFfixedClass_LowestMape = ModelDFfixedClass_LowestMape.append(average)\n",
    "    return ModelDFfixedClass_LowestMape\n",
    "\n",
    "def return_MapeTableFixedProcess(benchmark = \"cg\", FixProcess=\"64\"):\n",
    "    DirPath = \"./tmp_GenerateResources/\"\n",
    "    try:\n",
    "        ModelDFfixedProcess = return_FixedProcessModelDF(benchmark=benchmark, FixProcess=FixProcess)\n",
    "    except:\n",
    "        print(f\"return_FixedProcessModelDF({benchmark})の実行に失敗しました\")\n",
    "        return -1\n",
    "    ModelDFfixedProcess_LowestMape = ModelDFfixedProcess\n",
    "    index = ModelDFfixedProcess_LowestMape.index.tolist()\n",
    "    columns = ModelDFfixedProcess_LowestMape.columns.tolist()\n",
    "    for column in columns:\n",
    "        for row in index:\n",
    "            ModelDFfixedProcess_LowestMape.at[row, column] = ModelDFfixedProcess.at[row, column].mape_score_InTrain\n",
    "    average = ModelDFfixedProcess_LowestMape.mean()\n",
    "    average.name = \"Average\"\n",
    "    ModelDFfixedProcess_LowestMape = ModelDFfixedProcess_LowestMape.append(average)\n",
    "    return ModelDFfixedProcess_LowestMape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mape表をCSVとして保存する関数\n",
    "### 引数はbenchmark:ベンチマーク名, FixClass:固定するベンチマーククラス, FixProcess:固定するプロセス数, DirPath:保存するディレクトリ\n",
    "def SaveMapeTables(FixClass=\"B\", FixProcess=256, DirPath = \"./tmp_GenerateResources/\"):\n",
    "    for benchmark in benchmarks:\n",
    "        MapeTableFixedClass = return_MapeTableFixedClass(benchmark, FixClass=\"B\")\n",
    "        MapeTableFixedProcess = return_MapeTableFixedProcess(benchmark, FixProcess=256)\n",
    "        FixedClassFileName = f\"MapeTableFixedClass_{benchmark}{FixClass}.csv\"\n",
    "        FixedProcessFileName = f\"MapeTableFixedProcess_{benchmark}{FixProcess}.csv\"\n",
    "    #     print(f\"FixedClassFileName={FixedClassFileName}, MapeTableFixedProcess={FixedProcessFileName}\")\n",
    "        if(type(MapeTableFixedClass) is pd.core.frame.DataFrame):\n",
    "            print(f\"FixedClassFileName={FixedClassFileName}\")\n",
    "            MapeTableFixedClass.to_csv(f\"{DirPath}{FixedClassFileName}\")\n",
    "        if(type(MapeTableFixedProcess) is pd.core.frame.DataFrame):\n",
    "            print(f\"FixedProcessFileName={FixedProcessFileName}\")\n",
    "            MapeTableFixedProcess.to_csv(f\"{DirPath}{FixedProcessFileName}\")\n",
    "\n",
    "\n",
    "# SaveMapeTables(FixClass=\"C\", FixProcess=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### クラス：TimeData\n",
    "### 下記の関数(= return_TimeDataList())のために作成された\n",
    "### メソッドreturn_AllData()で値のすべてを辞書形式で受けてることができる\n",
    "class TimeData:\n",
    "    def __init__(self, benchmark=\"cg\", process=\"32\", BenchmarkClass=\"C\", time=-1):\n",
    "        self.benchmark = benchmark\n",
    "        self.process = process\n",
    "        self.BenchmarkClass = BenchmarkClass\n",
    "        self.time = time\n",
    "    def return_AllData(self):\n",
    "        return {\"benchmark\":self.benchmark, \"process\":self.process, \"BenchmarkClass\":self.BenchmarkClass, \"time\":self.time}\n",
    "\n",
    "### 返値に独自クラスTimeDataのリストを返す\n",
    "### 引数に実行プロセス数を取る\n",
    "### 返値のリストの要素は引数のリストのプロセス数で実行されたベンチマークの\n",
    "### ベンチマーク名・実行プロセス数・ベンチマーククラス・実行時間が記録されたTimeDataクラスのインスタンス\n",
    "def return_TimeDataList(process = 256):\n",
    "    return_list = []\n",
    "    with open(f\"./toGetProfile/toGetTime/TimeWith{process}.txt\") as f:\n",
    "        line_count = 0\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            if(line_count%3 == 1):\n",
    "                benchmark = line[1:3].lower()\n",
    "                # print(f\"benchmark={benchmark}, len(benchmark)={len(benchmark)}\")\n",
    "                Data = TimeData(benchmark=benchmark, process=process)\n",
    "            if(line_count%3 == 2):\n",
    "                BenchmarkClass = line[-2]\n",
    "                # print(f\"BenchmarkClass={BenchmarkClass}, len(BenchmarkClass)={len(BenchmarkClass)}\")\n",
    "                Data.BenchmarkClass = BenchmarkClass\n",
    "            if(line_count%3 == 0):\n",
    "                Time = line[-25:]\n",
    "                Time = Time.strip()\n",
    "                # print(f\"Time={Time}\")\n",
    "                Data.time = Time\n",
    "                return_list.append(Data)\n",
    "    return(return_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 関数:FillCSV()\n",
    "### 引数は,benchmark:ベンチマーク名, process:実行プロセス数, BenchmarkClass:ベンチマーククラス, time:実行時間\n",
    "### 引数として渡された値を適切なCSVに保存する\n",
    "def FillCSV(benchmark = \"cg\",process = 256,BenchmarkClass = \"A\",time = \"0.04\"):\n",
    "    time=float(time)\n",
    "    CSVFilename = f\"./csv_files/ExecTime@{benchmark}.csv\"\n",
    "    DataFrame = pd.read_csv(CSVFilename, index_col=0)\n",
    "    DataFrame.at[BenchmarkClass, process] = time\n",
    "    DataFrame.to_csv(CSVFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 誤差率の表を作成する関数\n",
    "### 引数は,benchmark:関数名, predict_class:予測したいクラス, predict_process:予測したいプロセス数, FixProcess:固定する実行プロセス数, FixClass:固定するベンチマーククラス\n",
    "### 返り値はなし\n",
    "def GenerateErrorRateTable(benchmark = \"cg\",predict_class = \"D\",predict_process = 256,FixProcess = 64,FixClass = \"B\"):\n",
    "    \n",
    "    DirName = \"./table_LatexForm/\"\n",
    "    FileSuffix = f\".table\"\n",
    "    FilePrefix = f\"ErrorRateTable@{benchmark}Fixed\"    \n",
    "    \n",
    "    try:\n",
    "        ErrorRateFixedProcessDF = return_ErrorRateFixedProcessDF(benchmark=benchmark, FixProcess=FixProcess, predict_class=predict_class)\n",
    "    except:\n",
    "        print(\"実行プロセスを固定した際の誤差率の表を取得するのに失敗したので、CSVとして保存できませんでした。\")\n",
    "    ErrorRateFixedProcessDF.to_latex(f\"{DirName}{FilePrefix}{FixProcess}{FileSuffix}\")\n",
    "    try:\n",
    "        ErrorRateFixedClassDF = return_ErrorRateFixedClassDF(benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "    except:\n",
    "        print(\"ベンチマーククラスを固定した際の誤差率の表を取得するのに失敗したので、CSVとして保存できませんでした。\")\n",
    "    ErrorRateFixedClassDF.to_latex(f\"{DirName}{FilePrefix}{FixClass}{FileSuffix}\")\n",
    "\n",
    "### 誤差率のデータフレームを返す関数\n",
    "### return_ErrorRateFixedProcessDF(), return_ErrorRateFixedClassDF()\n",
    "def return_ErrorRateFixedProcessDF(benchmark=\"cg\", FixProcess=64, predict_class=\"D\"):\n",
    "    FixProcessDF = return_FixedProcessModelDF(benchmark=benchmark, FixProcess=256)\n",
    "    # print(FixProcessDF)\n",
    "    for index in FixProcessDF.index.tolist():\n",
    "        for column in FixProcessDF.columns.tolist():\n",
    "            PredictNum = return_Predicted(FixProcessDF.at[index, column], ConvertBencharkClass_inNPB(predict_class))\n",
    "            FixProcessDF.at[index, column] = int(return_ErrorRate(FunctionName=index, PredictNum=PredictNum, BenchmarkName=benchmark, BenchmarkClass=predict_class, Process=FixProcess)*1000)/1000\n",
    "    return (FixProcessDF)\n",
    "def return_ErrorRateFixedClassDF(benchmark=\"cg\", FixClass=\"B\", predict_process=256):\n",
    "    FixClassDF = return_FixedClassModelDF(benchmark, FixClass=FixClass)\n",
    "    # print(FixClassDF)\n",
    "    for index in FixClassDF.index.tolist():\n",
    "        for column in FixClassDF.columns.tolist():\n",
    "            PredictNum = return_Predicted(FixClassDF.at[index, column], predict_process)\n",
    "            FixClassDF.at[index, column] = int(return_ErrorRate(FunctionName=index, PredictNum=PredictNum, BenchmarkName=benchmark, BenchmarkClass=FixClass, Process=predict_process)*1000)/1000\n",
    "    return(FixClassDF)\n",
    "\n",
    "# for benchmark in benchmarks:\n",
    "#     GenerateErrorRateTable(benchmark=benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes_onlyBTSP = [1, 4, 16, 64, 256]\n",
    "processes_excludeBTSP = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "### 関数：reutrn_MapeTableRowDataframe_FixedClass\n",
    "### 引数：benchmark=<ベンチマーク名>, FixedClass=<固定するクラス>, test_ratio=<テストに用いる割合>\n",
    "### 返値：引数の指定通りの設定で、各モデルの採用割合, 最大値, 最小値が保持された独自クラスを要素としたデータフレーム\n",
    "def reutrn_MapeTableRowDataframe_FixedClass(benchmark = \"cg\",FixedClass = \"B\",test_ratio = 0.3,enableTrain = True):\n",
    "\n",
    "    if(benchmark == \"bt\" or benchmark == \"sp\"):\n",
    "        process = processes_onlyBTSP\n",
    "    else:\n",
    "        process = processes_excludeBTSP\n",
    "\n",
    "    FixedClassList = return_fixed_class(BenchMark = benchmark, Processes=process, FixedBenchMarkClass = FixedClass)\n",
    "    FixedClassDataFrame = pd.concat(FixedClassList, axis=1)\n",
    "\n",
    "    DictData = return_dict_Data(FixedClassDataFrame)\n",
    "    MapeTablePerBenchmark = return_MapeTable_per_benchmark(DictData, test_ratio=test_ratio, train=enableTrain)\n",
    "    MapeTableRow = return_MapeTable_row(MapeTablePerBenchmark, benchmark)\n",
    "    MapeTableSource = MapeTableRow\n",
    "    MapeTableSourceColumnName = [\"線形モデル\", \"対数モデル\", \"反比例モデル\", \"分岐モデル\", \"ベンチマーク名\"]\n",
    "    MapeTable = pd.DataFrame(MapeTableSource)\n",
    "    MapeTable = MapeTable.T\n",
    "    MapeTable.columns = MapeTableSourceColumnName\n",
    "    MapeTable = MapeTable.set_index('ベンチマーク名')\n",
    "    return (MapeTable)\n",
    "\n",
    "\n",
    "### 関数：return_MapeTableDataframe_FixedClass()\n",
    "### 引数：FixedClass=<固定するベンチマーククラス>, test_ratio=<テストに用いる割合>\n",
    "### 返値：引数の指定通りの設定で全てのベンチマークに関する、各モデルの採用割合、最大値、最小値が保持された独自クラスを要素としたデータフレーム\n",
    "def return_MapeTableDataframe_FixedClass(FixedClass=\"B\",test_ratio=0.3):\n",
    "    MapeTableRowDataframes_list = []\n",
    "    for benchmark in benchmarks:\n",
    "        MapeTableRowDataframe = reutrn_MapeTableRowDataframe_FixedClass(benchmark=benchmark, FixedClass=FixedClass, test_ratio=test_ratio, enableTrain=True)\n",
    "        MapeTableRowDataframes_list.append(MapeTableRowDataframe)\n",
    "    MapeTableDataframe = pd.concat(MapeTableRowDataframes_list)\n",
    "    return(MapeTableDataframe)\n",
    "\n",
    "### 関数：return_MapeTableOnlyStrDataframe()\n",
    "### 引数：return_MapeTableDataframe_Fixed<Class or Process>()の返値\n",
    "### 返値：文字列を要素としたMape表のデータフレーム\n",
    "def return_MapeTableOnlyStrDataframe(input_MapeTableDF):\n",
    "    index = input_MapeTableDF.index\n",
    "    columns = input_MapeTableDF.columns\n",
    "    return_DF = pd.DataFrame().reindex_like(input_MapeTableDF).astype('str')\n",
    "    for i in index:\n",
    "        for j in columns:\n",
    "            contentStr = input_MapeTableDF.at[i, j].return_Data()\n",
    "            return_DF.at[i, j] = contentStr\n",
    "    return(return_DF)\n",
    "\n",
    "# 利用例\n",
    "# input_MapeTableDF = return_MapeTableDataframe_FixedClass()\n",
    "# print(return_MapeTableOnlyStrDataframe(input_MapeTableDF=input_MapeTableDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 関数：reutrn_MapeTableRowDataframe_FixedProcess\n",
    "### 引数：benchmark=<ベンチマーク名>, FixedProcess=<固定する実行プロセス>, test_ratio=<テストに用いる割合>\n",
    "### 返値：引数の指定通りの設定で、各モデルの採用割合, 最大値, 最小値が保持された独自クラスを要素としたデータフレーム\n",
    "def reutrn_MapeTableRowDataframe_FixedProcess(benchmark = \"cg\",FixedProcess = 256,test_ratio = 0.3,enableTrain = True):\n",
    "\n",
    "    # classes[2:]としているのは、変換関数(ConvertBenchmarkClasses)がベンチマーククラスS, Wの数値化に対応していないため\n",
    "    classes_inFunc = classes[2:]\n",
    "    classes_onNum = ConvertBenchmarkClasses(classes_inFunc)\n",
    "\n",
    "    FixedProcessList = return_fixed_process(BenchMark = benchmark, BenchMarkClasses=classes_inFunc, FixedProcess = FixedProcess)\n",
    "    FixedProcessDataFrame = pd.concat(FixedProcessList, axis=1)\n",
    "\n",
    "    DictData = return_dict_Data(FixedProcessDataFrame)\n",
    "    DictData['rowData'] = classes_onNum\n",
    "    MapeTablePerBenchmark = return_MapeTable_per_benchmark(DictData, test_ratio=test_ratio, train=enableTrain)\n",
    "    MapeTableRow = return_MapeTable_row(MapeTablePerBenchmark, benchmark)\n",
    "    MapeTableSource = MapeTableRow\n",
    "    MapeTableSourceColumnName = [\"線形モデル\", \"対数モデル\", \"反比例モデル\", \"分岐モデル\", \"ベンチマーク名\"]\n",
    "    MapeTable = pd.DataFrame(MapeTableSource)\n",
    "    MapeTable = MapeTable.T\n",
    "    MapeTable.columns = MapeTableSourceColumnName\n",
    "    MapeTable = MapeTable.set_index('ベンチマーク名')\n",
    "    return (MapeTable)\n",
    "\n",
    "### 関数：return_MapeTableDataframe_FixedProcess()\n",
    "### 引数：FixedProcess=<固定する実行プロセス数>, test_ratio=<テストに用いる割合>\n",
    "### 返値：引数の指定通りの設定で全てのベンチマークに関する、各モデルの採用割合、最大値、最小値が保持された独自クラスを要素としたデータフレーム\n",
    "def return_MapeTableDataframe_FixedProcess(FixedProcess=256,test_ratio=0.3):\n",
    "    MapeTableRowDataframes_list = []\n",
    "    for benchmark in benchmarks:\n",
    "        MapeTableRowDataframe = reutrn_MapeTableRowDataframe_FixedProcess(benchmark=benchmark, FixedProcess=FixedProcess, test_ratio=test_ratio, enableTrain=True)\n",
    "        MapeTableRowDataframes_list.append(MapeTableRowDataframe)\n",
    "    MapeTableDataframe = pd.concat(MapeTableRowDataframes_list)\n",
    "    return(MapeTableDataframe)\n",
    "\n",
    "# 利用例\n",
    "# input_MapeTableDF = return_MapeTableDataframe_FixedProcess()\n",
    "# print(return_MapeTableOnlyStrDataframe(input_MapeTableDF=input_MapeTableDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 関数：return_ErrorRateFixed<Class or Process>DFwithAverage()\n",
    "### 引数：benchmark:ベンチマーク名, FixProcess:固定するプロセス数,  predict_class:予測するクラス\n",
    "### 引数：benchmark:ベンチマーク名, FixClass:固定するクラス数,  predict_process:予測するプロセス数\n",
    "### 返り値はエラー率の平均が付与されたエラー率の表\n",
    "def return_ErrorRateFixedProcessDFwithAverate(benchmark=\"cg\", FixProcess=64, predict_class=\"D\"):\n",
    "    ErrorRateFixedProcessDF = return_ErrorRateFixedProcessDF(benchmark=benchmark, FixProcess=FixProcess, predict_class=predict_class)\n",
    "    average = ErrorRateFixedProcessDF.mean()\n",
    "    average.name = \"Average\"\n",
    "    return(ErrorRateFixedProcessDF.append(average))\n",
    "def return_ErrorRateFixedClassDFwithAverage(benchmark=\"cg\", FixClass=\"B\", predict_process=256):\n",
    "    ErrorRateFixedClsssDF = return_ErrorRateFixedClassDF(benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "    average = ErrorRateFixedClsssDF.mean()\n",
    "    average.name = \"Average\"\n",
    "    return(ErrorRateFixedClsssDF.append(average))\n",
    "\n",
    "### 関数：convert_StrListToIntList()\n",
    "### 引数：要素はすべて整数なリストのプリント出力\n",
    "### 返り値：数値のリスト\n",
    "def convert_StrListToIntList(InputList:list):\n",
    "    InputList = InputList[1:-1]\n",
    "    ReturnList = [int(x.strip()) for x in InputList.split(',')]\n",
    "    return(ReturnList)\n",
    "\n",
    "### 関数：convert_StrListToStrList()\n",
    "### 引数：要素はすべて一文字のアルファベットなリストのプリント出力\n",
    "### 返り値：アルファベットのリスト\n",
    "def convert_StrListToStrList(InputList:list):\n",
    "    InputList = InputList[1:-1]\n",
    "    InputList = InputList.replace('\\'', '')\n",
    "    ReturnList = [x.strip() for x in InputList.split(',')]\n",
    "    return(ReturnList)\n",
    "\n",
    "### 関数：return_ExecTime()\n",
    "### 引数：ベンチマーク名, ベンチマーククラス, 実行プロセス数\n",
    "### 返り値：引数に該当するベンチマークの実行時間\n",
    "def return_ExecTime(benchmark=\"cg\", BenchmarkClass=\"B\", Process=256):\n",
    "    FileDir = \"./csv_files/\"\n",
    "    FileName = f\"ExecTime@{benchmark}.csv\"\n",
    "    ExecTimeDF = pd.read_csv(f\"{FileDir}{FileName}\", index_col=0)\n",
    "    TargetNum = ExecTimeDF.at[BenchmarkClass, f\"{Process}\"]\n",
    "    if(np.isnan(TargetNum)):\n",
    "        TargetNum=-1\n",
    "    return(TargetNum)\n",
    "\n",
    "### 関数:return_FixClassCost()\n",
    "### 引数:benchmark=ベンチマーク名, ProcessList=実行プロセスのリスト, BenchmarkClass=ベンチマーククラス\n",
    "### 返り値：引数の条件に当てはまるコスト\n",
    "def return_FixClassCost(benchmark=\"cg\", ProcessList = [1, 2, 4], BenchmarkClass =\"B\"):\n",
    "    cost = 0\n",
    "    for process in ProcessList:\n",
    "        ExecTime = return_ExecTime(benchmark=benchmark, BenchmarkClass=BenchmarkClass, Process=process)\n",
    "        if(ExecTime < 0):\n",
    "            return(-1)\n",
    "        cost += ExecTime * process\n",
    "    return(cost)\n",
    "\n",
    "### 関数:return_FixProcessCost()\n",
    "### 引数:benchmark=ベンチマーク名, BenchmarkClassList=ベンチマーククラスのリスト, Process=実行プロセス\n",
    "### 返り値：引数の条件に当てはまるコスト\n",
    "def return_FixProcessCost(benchmark=\"cg\", BenchmarkClassList = [\"A\", \"B\", \"C\"], Process=256):\n",
    "    cost = 0\n",
    "    for BenchmarkClass in BenchmarkClassList:\n",
    "        ExecTime = return_ExecTime(benchmark=benchmark, BenchmarkClass=BenchmarkClass, Process=Process)\n",
    "        if(ExecTime < 0):\n",
    "            return(-1)\n",
    "        cost += ExecTime * Process\n",
    "    return(cost)\n",
    "\n",
    "def return_ErrorRateFixedClassAverageAndCosts(benchmark=\"cg\", FixClass=\"B\", predict_process=256):\n",
    "    ErrorRateFixedClassDFwithAverage = return_ErrorRateFixedClassDFwithAverage( benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "    ErrorRateFixedClassAverageSeries = ErrorRateFixedClassDFwithAverage.loc[\"Average\"]\n",
    "    ErrorRateFixedClassAverageDF = pd.DataFrame(ErrorRateFixedClassAverageSeries)\n",
    "    index = ErrorRateFixedClassAverageDF.index.tolist()\n",
    "    columns = ErrorRateFixedClassAverageDF.columns.tolist()\n",
    "    listedIndex = []\n",
    "    for i in index:\n",
    "        listedIndex.append(convert_StrListToIntList(i))\n",
    "    Costs = []\n",
    "    for i in listedIndex:\n",
    "        Costs.append(return_FixClassCost(benchmark=benchmark, ProcessList=i, BenchmarkClass=FixClass))\n",
    "    ErrorRateFixedClassAverageDF[\"PredictCosts\"] = Costs\n",
    "    return(ErrorRateFixedClassAverageDF)\n",
    "\n",
    "\n",
    "def return_ErrorRateFixedProcessAverageAndCosts(benchmark=\"cg\", FixProcess=256, predict_class=\"D\"):\n",
    "    ErrorRateFixedProcessDFwithAverage = return_ErrorRateFixedProcessDFwithAverate(benchmark=benchmark, FixProcess=FixProcess, predict_class=predict_class)\n",
    "    ErrorRateFixedProcessAverageSeries = ErrorRateFixedProcessDFwithAverage.loc[\"Average\"]\n",
    "    ErrorRateFixedProcessAverageDF = pd.DataFrame(ErrorRateFixedProcessAverageSeries)\n",
    "    index = ErrorRateFixedProcessAverageDF.index.tolist()\n",
    "    columns = ErrorRateFixedProcessAverageDF.columns.tolist()\n",
    "    listedIndex = []\n",
    "    for i in index:\n",
    "        listedIndex.append(convert_StrListToStrList(i))\n",
    "    Costs = []\n",
    "    for i in listedIndex:\n",
    "        Costs.append(return_FixProcessCost(benchmark=benchmark, BenchmarkClassList=i, Process=FixProcess))\n",
    "    ErrorRateFixedProcessAverageDF[\"PredictCosts\"] = Costs\n",
    "    return(ErrorRateFixedProcessAverageDF)\n",
    "\n",
    "# benchmark=\"cg\"\n",
    "# FixProcess=64\n",
    "# predict_class=\"D\"\n",
    "# FixClass=\"B\"\n",
    "# predict_process=256\n",
    "# BenchmarkClass=\"B\"\n",
    "# Process=64\n",
    "\n",
    "def return_ErrorRateFixedClass_AveragePredictCostRealCost(benchmark=\"cg\", FixClass=\"B\", predict_process=256):\n",
    "    ErrorRateFixedClass = return_ErrorRateFixedClassAverageAndCosts(benchmark=benchmark, FixClass=FixClass, predict_process=predict_process)\n",
    "    index = ErrorRateFixedClass.index.tolist()\n",
    "    columns = ErrorRateFixedClass.columns.tolist()\n",
    "    RealClass=FixClass\n",
    "    RealProcess = predict_process\n",
    "    RealTime = return_ExecTime(benchmark=benchmark, BenchmarkClass=RealClass, Process=RealProcess)\n",
    "    RealCost = RealProcess * RealTime\n",
    "    RealCostList = [RealCost] * len(index)\n",
    "    ErrorRateFixedClass[\"RealCost\"] = RealCostList\n",
    "    return(ErrorRateFixedClass)\n",
    "\n",
    "def return_ErrorRateFixedProcess_AveragePredictCostRealCost(benchmark=\"cg\", FixProcess=256, predict_class=\"D\"):\n",
    "    ErrorRateFixedProcess = return_ErrorRateFixedProcessAverageAndCosts(benchmark=benchmark, FixProcess=FixProcess, predict_class=predict_class)\n",
    "    index = ErrorRateFixedProcess.index.tolist()\n",
    "    columns = ErrorRateFixedProcess.columns.tolist()\n",
    "    RealClass= predict_class\n",
    "    RealProcess = FixProcess\n",
    "    RealTime = return_ExecTime(benchmark=benchmark, BenchmarkClass=RealClass, Process=RealProcess)\n",
    "    RealCost = RealProcess * RealTime\n",
    "    RealCostList = [RealCost] * len(index)\n",
    "    ErrorRateFixedProcess[\"RealCost\"] = RealCostList\n",
    "    return(ErrorRateFixedProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateMapeTable(FixedClass=\"B\", FixedProcess=\"64\", test_ratio=0.3):\n",
    "    \n",
    "    DirName = \"./table_LatexForm/\"\n",
    "    FileSuffix = f\"Train0{int(test_ratio*10)}.table\"\n",
    "    FilePrefix = \"MapeTableFixed\"\n",
    "    \n",
    "    input_MapeTableDF = return_MapeTableDataframe_FixedProcess(FixedProcess=FixedProcess, test_ratio=test_ratio)\n",
    "    return_MapeTableDF = return_MapeTableOnlyStrDataframe(input_MapeTableDF=input_MapeTableDF)\n",
    "    return_MapeTableDF.to_latex(f\"{DirName}{FilePrefix}{FixedProcess}{FileSuffix}\")\n",
    "    input_MapeTableDF = return_MapeTableDataframe_FixedClass(FixedClass=FixedClass,test_ratio=test_ratio)\n",
    "    return_MapeTableDF = return_MapeTableOnlyStrDataframe(input_MapeTableDF=input_MapeTableDF)\n",
    "    return_MapeTableDF.to_latex(f\"{DirName}{FilePrefix}{FixedClass}{FileSuffix}\")\n",
    "\n",
    "# MAPE表を生成する例   \n",
    "# GenerateMapeTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 関数：ConvertIndexNameToNumOfProfile()\n",
    "### 引数：行名が使用したプロファイル, 列名が平均誤差率・予測コスト・実測コストとなっているデータフレーム\n",
    "### 返値：\n",
    "def ConvertIndexNameToNumOfProfile(inputDF, Fixed=\"Process\"):\n",
    "    index = inputDF.index.tolist()\n",
    "    # ConvertedIndexには入力データフレームの行名から使用されたプロファイルの数を格納している。\n",
    "    ConvertedIndex = [0] * len(index)\n",
    "    for i in range(len(index)):\n",
    "        if(Fixed==\"Process\"):\n",
    "            ConvertedIndex[i] = len(convert_StrListToStrList(index[i]))\n",
    "        elif(Fixed==\"Class\"):\n",
    "            ConvertedIndex[i] = len(convert_StrListToIntList(index[i]))\n",
    "    returnDF = inputDF.copy(deep=True)\n",
    "    returnDF.index = ConvertedIndex\n",
    "    return(returnDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenGraphAveragePerProfileNum(benchmarks=[\"cg\"], Fixed=\"Process\", Fix=64, Predict=\"D\", EnableTitle=False, EnableScatter=False, SaveGraph=False):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for benchmark in benchmarks:\n",
    "        if(Fixed==\"Process\"):\n",
    "            FixedDF = return_ErrorRateFixedProcess_AveragePredictCostRealCost(benchmark=benchmark, FixProcess=Fix, predict_class=Predict)\n",
    "            GraphTitle = f\"ベンチマーク{benchmarks}で実行プロセス数を固定\"\n",
    "        else:\n",
    "            FixedDF = return_ErrorRateFixedClass_AveragePredictCostRealCost(benchmark=benchmark, FixClass=Fix, predict_process=Predict)\n",
    "            GraphTitle = f\"ベンチマーク{benchmarks}でベンチマーククラスを固定\"\n",
    "        ConvertedIndexFixedDF = ConvertIndexNameToNumOfProfile(FixedDF, Fixed=Fixed)\n",
    "        x = ConvertedIndexFixedDF.index.tolist()\n",
    "        y = ConvertedIndexFixedDF[\"Average\"].tolist()\n",
    "        plt.plot(x,y,label=f\"{benchmark.upper()}\", marker='o')\n",
    "        if(EnableScatter):\n",
    "            plt.scatter(x, y)\n",
    "        plt.legend()\n",
    "    plt.xlabel(\"使用したプロファイル数\")\n",
    "    plt.ylabel(\"平均絶対誤差率 [%]\")\n",
    "    plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    if(EnableTitle):\n",
    "        plt.title(GraphTitle)\n",
    "    if(Fixed==\"Process\"):\n",
    "        plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.subplots_adjust(right=0.9)\n",
    "    plt.yscale('log')\n",
    "    if(SaveGraph):\n",
    "        filename = f\"Fix{Fixed}AverageError.pdf\"\n",
    "        plt.savefig(f\"./tmp_GenerateResources/{filename}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "##使用例\n",
    "# GenGraphAveragePerProfileNum(benchmarks=benchmarks, Fixed=\"Process\", Fix=64, Predict=\"D\")\n",
    "# GenGraphAveragePerProfileNum(benchmarks=benchmarks, Fixed=\"Class\", Fix=\"B\", Predict=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for benchmark in benchmarks:\n",
    "#     print(return_ErrorRateFixedClass_AveragePredictCostRealCost(benchmark=benchmark, FixClass=\"B\", predict_process=256))\n",
    "def GenGraphTotalTimePerProfileNum(benchmark=\"cg\", Fixed=\"Process\", Fix=64, Predict=\"D\", EnableTitle=False, SaveGraph=False):\n",
    "    if(Fixed==\"Process\"):\n",
    "        FixedDF = return_ErrorRateFixedProcess_AveragePredictCostRealCost(benchmark=benchmark, FixProcess=Fix, predict_class=Predict)\n",
    "        GraphTitle = f\"ベンチマーク{benchmark}で実行プロセス数を固定\"\n",
    "    else:\n",
    "        FixedDF = return_ErrorRateFixedClass_AveragePredictCostRealCost(benchmark=benchmark, FixClass=Fix, predict_process=Predict)\n",
    "        GraphTitle = f\"ベンチマーク{benchmark}でベンチマーククラスを固定\"\n",
    "    ConvertedIndexFixedDF = ConvertIndexNameToNumOfProfile(inputDF=FixedDF, Fixed=\"Process\")\n",
    "    x = ConvertedIndexFixedDF.index.tolist()\n",
    "    y = ConvertedIndexFixedDF[\"PredictCosts\"]\n",
    "    RealCost = ConvertedIndexFixedDF[\"RealCost\"]\n",
    "    plt.figure()\n",
    "    plt.plot(x, y, marker=\"o\", color=\"blue\", label=\"予測にかかるコスト\")\n",
    "    plt.plot(x, RealCost, color=\"red\", label=\"実測にかかるコスト\")\n",
    "    plt.xlabel(\"使用したプロファイル数\")\n",
    "    plt.ylabel(\"コスト [秒]\")\n",
    "    plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "#     plt.legend(bbox_to_anchor=(0, -0.15), loc='upper left', borderaxespad=0)\n",
    "    plt.legend(loc='best', borderaxespad=0)\n",
    "    if(EnableTitle):\n",
    "        plt.title(GraphTitle)\n",
    "    if(SaveGraph):\n",
    "        filename = f\"Fix{Fixed}AllTime@{benchmark.upper()}.pdf\"\n",
    "        plt.savefig(f\"./tmp_GenerateResources/{filename}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "### 使用例\n",
    "# GenGraphTotalTimePerProfileNum(EnableTitle=True)\n",
    "# GenGraphTotalTimePerProfileNum(benchmark=benchmark, Fixed=\"Class\", Fix=\"C\", Predict=256, EnableTitle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ErrorRateAverageAndCosts(benchmark = \"cg\", Fix = \"B\", Predict = 256, Fixed = \"Class\"):\n",
    "    if(Fixed==\"Class\"):\n",
    "        FixDF = return_ErrorRateFixedClassAverageAndCosts(benchmark=benchmark, FixClass=Fix, predict_process=Predict)\n",
    "    else:\n",
    "        FixDF = return_ErrorRateFixedProcessAverageAndCosts(benchmark=benchmark, FixProcess=Fix, predict_class=Predict)\n",
    "    index = FixDF.index.tolist()\n",
    "    NewIndex = []\n",
    "    for i in index:\n",
    "        if(Fixed==\"Class\"):\n",
    "            NewIndex.append(len(convert_StrListToIntList(i)))\n",
    "        else:\n",
    "            NewIndex.append(len(convert_StrListToStrList(i)))\n",
    "    FixDF = FixDF.reset_index()\n",
    "    FixDF['index'] = NewIndex\n",
    "    FixDF = FixDF.rename(columns={'index': '使用したプロファイル数', 'Average': '平均誤差率', 'PredictCosts':\"コスト\"})\n",
    "    FixDF = FixDF.set_index('使用したプロファイル数')\n",
    "    return(FixDF)\n",
    "\n",
    "# # 使用例\n",
    "# sampleDF = return_ErrorRateAverageAndCosts(benchmark=\"cg\", Fix=\"B\", Predict=256, Fixed=\"Class\")\n",
    "# print(sampleDF)\n",
    "# sampleDF  = return_ErrorRateAverageAndCosts(benchmark=\"cg\", Fix=64, Predict=\"D\", Fixed=\"Process\")\n",
    "# print(sampleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数：BestModelsInDF()\n",
    "# 引数：return_FixedClassModelDF()の返値\n",
    "# 返値：引数で渡されたDFの要素のモデル名を要素としたDF\n",
    "def BestModelsInDF(inputDF):\n",
    "    returnDF = pd.DataFrame()\n",
    "    returnDF = returnDF.reindex_like(inputDF).astype('str')\n",
    "    index = returnDF.index.tolist()\n",
    "    columns = returnDF.columns.tolist()\n",
    "    for i in index:\n",
    "        for j in columns:\n",
    "            returnDF.at[i, j] = inputDF.at[i, j].ModelName()\n",
    "    return(returnDF)\n",
    "# ### 使用例\n",
    "# benchmark = \"mg\"\n",
    "# FixClass = \"B\"\n",
    "# sampleDF = return_FixedClassModelDF(benchmark = benchmark, FixClass = FixClass)\n",
    "# print(BestModelsInDF(inputDF=sampleDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実データを取得する関数\n",
    "\n",
    "benchmark = \"cg\"\n",
    "benchmarkClasses = [\"A\", \"B\", \"C\", \"D\"]\n",
    "FixedProcess = 64\n",
    "process_onlyBTSP    = [1, 4, 16, 64, 256]\n",
    "process_excludeBTSP = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "functionName = \".TAU_application\"\n",
    "\n",
    "def returnRawDF(Benchmark = \"cg\", functionName = \".TAU_application\" , fix=\"Process\", benchmarkClass = [\"A\", \"B\", \"C\", \"D\"], FixedProcess = 64, Processes = [1,2,4,8,16,32,64,128,256], FixedBenchmarkClass=\"B\"):\n",
    "    if(fix == \"Process\"):\n",
    "        fixed = return_fixed_process(BenchMark=Benchmark, BenchMarkClasses=benchmarkClass, FixedProcess=FixedProcess)\n",
    "    else:\n",
    "        fixed = return_fixed_class(BenchMark=Benchmark, Processes=Processes, FixedBenchMarkClass=FixedBenchmarkClass)\n",
    "    summaryRawData = pd.concat(fixed, axis=1)\n",
    "    return summaryRawData.loc[[functionName]]\n",
    "\n",
    "# returnRawDF(fix=\"Process\")\n",
    "# returnRawDF(fix=\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマークごとの生データを取得する関数\n",
    "# 引数は returnRawDF() と基本的に同等\n",
    "# 異なる部分は functionName が存在しないこと\n",
    "\n",
    "def returnRawDFperBenchmark(Benchmark = \"cg\", fix=\"Process\", benchmarkClass = [\"A\", \"B\", \"C\", \"D\"], FixedProcess = 64, Processes = [1,2,4,8,16,32,64,128,256], FixedBenchmarkClass=\"B\"):\n",
    "    if(fix == \"Process\"):\n",
    "        fixed = return_fixed_process(BenchMark=Benchmark, BenchMarkClasses=benchmarkClass, FixedProcess=FixedProcess)\n",
    "    else:\n",
    "        fixed = return_fixed_class(BenchMark=Benchmark, Processes=Processes, FixedBenchMarkClass=FixedBenchmarkClass)\n",
    "    summaryRawData = pd.concat(fixed, axis=1)\n",
    "    return summaryRawData\n",
    "\n",
    "# # 使用例\n",
    "# returnRawDFperBenchmark(fix=\"Process\")\n",
    "# returnRawDFperBenchmark(fix=\"Classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数：inputDF  はreturnRawDF()の返り値\n",
    "# 引数：repeated はDATA列を繰り返す回数\n",
    "# 引数：includeLastData は学習に最後の値を使うかどうかを指定\n",
    "\n",
    "def generateInputFileForExtraP(inputDF = returnRawDF(fix=\"Class\"), repeated = 3, includeLastData = False, fileName = \"tmp_functionName.txt\"):\n",
    "\n",
    "    # https://github.com/extra-p/extrap/blob/master/docs/examples/input.txt\n",
    "    # が入力例のテキストファイルとなっており、これに対応するファイルを作成する。\n",
    "    functionNames = inputDF.index.tolist()\n",
    "    for functionName in functionNames:\n",
    "        tmpfile = open(fileName, 'w+')\n",
    "        columns = inputDF.columns.tolist()\n",
    "        tmpfile.write(\"PARAMETER p\\n\")\n",
    "        tmpfile.write(\"POINTS\")\n",
    "        \n",
    "        if(includeLastData == False):\n",
    "            columns = columns[:-1]\n",
    "        \n",
    "        for column in columns:\n",
    "            tmpfile.write(f\" ({column})\")\n",
    "        tmpfile.write(\"\\n\\n\")\n",
    "\n",
    "        tmpfile.write(f\"REGION {functionName}\\n\")\n",
    "        tmpfile.write(\"METRIC functionCall\\n\")\n",
    "\n",
    "        for column in columns:\n",
    "\n",
    "            tmpfile.write(\"DATA\")\n",
    "            content = float(inputDF.at[functionName, column])\n",
    "\n",
    "            for i in range(repeated):\n",
    "                tmpfile.write(f\" {content}\")\n",
    "            tmpfile.write(\"\\n\")\n",
    "\n",
    "        tmpfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数：input_x  は横軸に相当する値のリスト\n",
    "# 引数：input_y  は縦軸に相当する値のリスト\n",
    "# 引数：repeated はDATA列を繰り返す回数\n",
    "# 引数：includeFirstData は学習に最初の値を使うかどうかを指定\n",
    "# 引数：includeLastData は学習に最初の値を使うかどうかを指定\n",
    "\n",
    "def generateInputFileForExtraPfromLists(input_x=[1, 2, 4, 8, 16, 32, 64, 128, 256], input_y=[1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], repeated = 3, includeFirstData=False, includeLastData = False, fileName = \"tmp_functionName.txt\"):\n",
    "\n",
    "    # https://github.com/extra-p/extrap/blob/master/docs/examples/input.txt\n",
    "    # が入力例のテキストファイルとなっており、これに対応するファイルを作成する。\n",
    "    tmpfile = open(fileName, 'w+')\n",
    "    tmpfile.write(\"PARAMETER p\\n\")\n",
    "    tmpfile.write(\"POINTS\")\n",
    "\n",
    "    if(includeLastData == False):\n",
    "        input_x = input_x[:-1]\n",
    "        input_y = input_y[:-1]\n",
    "    if(includeFirstData == False):\n",
    "        input_x = input_x[1:]\n",
    "        input_y = input_y[1:]\n",
    "\n",
    "    for x in input_x:\n",
    "        tmpfile.write(f\" ({x})\")\n",
    "    tmpfile.write(\"\\n\\n\")\n",
    "\n",
    "    tmpfile.write(f\"REGION {functionName}\\n\")\n",
    "    tmpfile.write(\"METRIC functionCall\\n\")\n",
    "\n",
    "    for y in input_y:\n",
    "\n",
    "        tmpfile.write(\"DATA\")\n",
    "        content = float(y)\n",
    "\n",
    "        for i in range(repeated):\n",
    "            tmpfile.write(f\" {content}\")\n",
    "        tmpfile.write(\"\\n\")\n",
    "\n",
    "    tmpfile.close()\n",
    "\n",
    "##### 使用例 #####\n",
    "##準備##\n",
    "# inputDF = returnRawDF(fix=\"Class\")\n",
    "# input_x = inputDF.columns.tolist()\n",
    "# functionNames = inputDF.index.tolist()\n",
    "# functionName = functionNames[0]\n",
    "# input_y = []\n",
    "# for x in input_x:\n",
    "#     input_y.append(inputDF.at[functionName, x])\n",
    "##使用##\n",
    "# generateInputFileForExtraPfromLists(input_x=input_x, input_y=input_y, repeated=3, includeFirstData=False, includeLastData=False, fileName=\"tmp_functionName.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの共通部分となるクラス\n",
    "# すべての引数はただのリスト。クラスの初期化時に\"\"np.reshape()\"\"を実行する\n",
    "class ModelBase2:\n",
    "    def __init__(self, train_x, train_y, target_x = [], target_y = [], benchmark_name=\"benchmark_name\", function_name=\"function_name\"):\n",
    "        self.benchmark_name = benchmark_name\n",
    "        self.function_name = function_name\n",
    "        \n",
    "        self.train_x  = np.reshape(train_x, (-1, 1))\n",
    "        self.train_y  = np.reshape(train_y, (-1, 1))\n",
    "        self.target_x = np.reshape(target_x, (-1, 1))\n",
    "        self.target_x = np.reshape(target_x, (-1, 1))\n",
    "\n",
    "# 線形モデルでロバスト回帰を行う\n",
    "# 作成したModelBase2を継承\n",
    "class ModelLin_rob(ModelBase2):\n",
    "    def calc_hr(self):\n",
    "        self.hr = HuberRegressor()\n",
    "        self.hr.fit(self.train_x, self.train_y)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted = self.lr.predict(self.target_x)\n",
    "        self.mape_score = float(mape_score(self.target_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted = self.lr.predict(self.train_x)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def predict(self, num):\n",
    "        predicted = self.hr.predict(num)\n",
    "        return(predicted)\n",
    "    \n",
    "    def ModelName(self):\n",
    "        return(\"ModelLin_rob\")\n",
    "\n",
    "# 反比例モデルでロバスト回帰を行う\n",
    "# ModelBase2を継承\n",
    "def ip_func(x):\n",
    "    return 1/x\n",
    "\n",
    "class ModelIp_rob(ModelBase2):\n",
    "    \n",
    "    def calc_hr(self):\n",
    "        self.transformer_ip = sp.FunctionTransformer(func=ip_func, inverse_func=ip_func)\n",
    "        y_train_ip = self.transformer_ip.transform(self.train_y)        \n",
    "        self.hr = HuberRegressor()\n",
    "        self.hr.fit(self.train_x, y_train_ip)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_y_predicted_ip = self.hr.predict(self.test_x)\n",
    "        test_y_predicted = self.transformer_ip.inverse_transform(test_y_predicted_ip)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_y_predicted_ip = self.hr.predict(self.train_x)\n",
    "        train_y_predicted = self.transformer_ip.inverse_transform(train_y_predicted_ip)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def predict(self, num):\n",
    "        predicted_ip = self.hr.predict(num)\n",
    "        predicted = self.transformer_ip.inverse_transform(predicted_ip)\n",
    "        return(predicted)\n",
    "    \n",
    "    def ModelName(self):\n",
    "        return(\"ModelIP_rob\")\n",
    "\n",
    "# 対数モデルでロバスト回帰を行う\n",
    "# ModelBase2を継承\n",
    "def inverter_log10_func(x):\n",
    "    return 10**x\n",
    "\n",
    "class ModelLog10_rob(ModelBase2):\n",
    "\n",
    "    def calc_hr(self):\n",
    "        self.transformer_log10 = sp.FunctionTransformer(func=np.log10, inverse_func=inverter_log10_func)\n",
    "        x_train_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        y_train_log10 = self.transformer_log10.transform(self.train_y)\n",
    "\n",
    "        self.hr = HuberRegressor()\n",
    "        self.hr.fit(x_train_log10, y_train_log10)\n",
    "\n",
    "    def calc_mape_score(self):\n",
    "        test_x_log10 = self.transformer_log10.transform(self.test_x)\n",
    "        test_y_predicted_log10 = self.hr.predict(test_x_log10)\n",
    "        test_y_predicted = self.transformer_log10.inverse_transform(test_y_predicted_log10)\n",
    "        self.mape_score = float(mape_score(self.test_y, test_y_predicted))\n",
    "\n",
    "    def calc_mape_score_InTrain(self):\n",
    "        train_x_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        train_y_predicted_log10 = self.hr.predict(train_x_log10)\n",
    "        train_y_predicted = self.transformer_log10.inverse_transform(train_y_predicted_log10)\n",
    "        self.mape_score_InTrain = float(mape_score(self.train_y, train_y_predicted))\n",
    "\n",
    "    def predict(self, num):\n",
    "        num_log10 = self.transformer_log10.transform(num)\n",
    "        predicted_log10 = self.hr.predict(num_log10)\n",
    "        predicted = self.transformer_log10.inverse_transform(predicted_log10)\n",
    "        return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLog10_rob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反比例モデルmk2\n",
    "# ModelBaseを継承した反比例モデルは yの逆数をとっていた。\n",
    "# しかし、それでは意図したモデルとならないことが判明した。\n",
    "# そのため、xの逆数をとる、反比例モデルがコレ。\n",
    "\n",
    "class ModelIp_mk2(ModelBase2):\n",
    "\n",
    "    def calc_lr(self):\n",
    "        self.transformer_ip = sp.FunctionTransformer(func=ip_func, inverse_func=ip_func)\n",
    "        x_train_ip = self.transformer_ip.transform(self.train_x)\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(x_train_ip, self.train_y)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num = np.reshape(num, (-1, 1))\n",
    "        numConverted = self.transformer_ip.transform(num)\n",
    "        predicted = self.lr.predict(numConverted)\n",
    "        return(predicted)\n",
    "    \n",
    "    def return_coef_(self):\n",
    "        return self.lr.coef_\n",
    "    \n",
    "    def return_intercept_(self):\n",
    "        return self.lr.intercept_\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelIP\")\n",
    "\n",
    "# # 使用例\n",
    "# modelIpMk2 = ModelIp_mk2(train_x=train_x, train_y=train_y, target_x=target_x, target_y=target_y)\n",
    "# modelIpMk2.calc_lr()\n",
    "# plot_y = modelIpMk2.predict(plot_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対数モデルmk2\n",
    "# ModelBaseを継承した対数モデルはどこかに不具合がある。\n",
    "# ModelBase2を継承して、改修した対数モデルがこのモデル。\n",
    "\n",
    "class ModelLog10_mk2(ModelBase2):\n",
    "\n",
    "    def calc_lr(self):\n",
    "        self.transformer_log10 = sp.FunctionTransformer(func=np.log10, inverse_func=inverter_log10_func)\n",
    "        x_train_log10 = self.transformer_log10.transform(self.train_x)\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(x_train_log10, self.train_y)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num = np.reshape(num, (-1, 1))\n",
    "        numConverted = self.transformer_log10.transform(num)\n",
    "        predicted = self.lr.predict(numConverted)\n",
    "        return(predicted)\n",
    "    \n",
    "    def return_coef_(self):\n",
    "        return self.lr.coef_\n",
    "    \n",
    "    def return_intercept_(self):\n",
    "        return self.lr.intercept_\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLog\")\n",
    "\n",
    "# # 使用例\n",
    "# modelLog10Mk2 = ModelIp_mk2(train_x=train_x, train_y=train_y, target_x=target_x, target_y=target_y)\n",
    "# modelLog10Mk2.calc_lr()\n",
    "# plot_y = modelLog10Mk2.predict(plot_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形モデルmk2\n",
    "# ModelBase2を継承して、改修したモデル。\n",
    "\n",
    "class ModelLin_mk2(ModelBase2):\n",
    "\n",
    "    def calc_lr(self):\n",
    "        self.lr = LinearRegression()\n",
    "        self.lr.fit(self.train_x, self.train_y)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num = np.reshape(num, (-1, 1))\n",
    "        predicted = self.lr.predict(num)\n",
    "        return(predicted)\n",
    "    \n",
    "    def return_coef_(self):\n",
    "        return self.lr.coef_\n",
    "    \n",
    "    def return_intercept_(self):\n",
    "        return self.lr.intercept_\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelLin\")\n",
    "\n",
    "# 分岐モデルmk2\n",
    "# ModelBase2を継承して、改修したモデル\n",
    "\n",
    "class ModelBranch_mk2(ModelBase2):\n",
    "    def calc_lr(self):\n",
    "        self.t = np.ndarray.argmax(self.train_y)\n",
    "        self.t_num = self.train_x[self.t]\n",
    "        if (self.t == 0 or self.t == len(self.train_y) - 1):\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.train_x, self.train_y)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.train_x, self.train_y)\n",
    "        else:\n",
    "            self.train_x_1 = self.train_x[:self.t]\n",
    "            self.train_x_2 = self.train_x[self.t:]\n",
    "            self.train_y_1 = self.train_y[:self.t]\n",
    "            self.train_y_2 = self.train_y[self.t:]\n",
    "            self.lr1 = LinearRegression()\n",
    "            self.lr1.fit(self.train_x_1, self.train_y_1)\n",
    "            self.lr2 = LinearRegression()\n",
    "            self.lr2.fit(self.train_x_2, self.train_y_2)\n",
    "\n",
    "    def predict(self, num):\n",
    "        num = np.reshape(num, (-1, 1))\n",
    "        num_t = np.ndarray.argmax(num)\n",
    "        num_t_max = num[num_t]\n",
    "        k = np.abs(np.asarray(num) - self.t_num).argmin()\n",
    "        if (num_t_max < self.train_x[self.t] or k == 0):\n",
    "            predicted = self.lr1.predict(num)\n",
    "            return(predicted)\n",
    "        else:\n",
    "            num_1 = num[:k]\n",
    "            num_2 = num[k:]\n",
    "            predicted_1 = self.lr1.predict(num_1)\n",
    "            predicted_2 = self.lr2.predict(num_2)\n",
    "            predicted = np.concatenate([predicted_1, predicted_2])\n",
    "            return(predicted)\n",
    "\n",
    "    def ModelName(self):\n",
    "        return(\"ModelBranch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectFunctionNamesPerBenchmark(benchmarkName = \"cg\", benchmarkClasses = [\"A\",\"B\",\"C\"], processes = [1,2,4,8], baseDir = \"./csv_files/\"):\n",
    "    dataframesList = []\n",
    "    for benchmarkClass in benchmarkClasses:\n",
    "        for process in processes:\n",
    "            # ファイル名を決める\n",
    "            fileName = (\"pprof_\"+benchmarkName+benchmarkClass+str(process)+\".csv\")\n",
    "            # ファイルのパスを決める\n",
    "            filePath = baseDir + fileName\n",
    "            # ファイルが存在して、空データではないという条件で\n",
    "            if (os.path.exists(filePath) and os.stat(filePath).st_size != 0):\n",
    "                DF = pd.read_csv(filePath)\n",
    "                len_indice = DF.shape[0]\n",
    "                len_columns = DF.shape[1]\n",
    "\n",
    "                DF = DF.rename(columns={'Name': 'functionName', '#Call': 'call'})\n",
    "                DF['benchmarkClass'] = [ConvertBencharkClass_inNPB(benchmarkClass)] * len_indice\n",
    "                DF['benchmarkName'] = [benchmarkName] * len_indice\n",
    "                DF['process'] = [str(process)] * len_indice\n",
    "                dataframesList.append(DF)\n",
    "    returnDF = pd.concat(dataframesList)\n",
    "    return returnDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験結果を集計するためのデータフレームのカラムの名称のリストを返す関数\n",
    "def return_numOfColumns(dataType=False):\n",
    "    returnList = []\n",
    "    returnDict = {}\n",
    "    # ベンチマーク名\n",
    "    returnList.append(\"benchmarkName\")\n",
    "    returnDict[\"benchmarkName\"] = str\n",
    "    # 関数名\n",
    "    returnList.append(\"functionName\")\n",
    "    returnDict[\"functionName\"] = str\n",
    "    # 使用データ(数値化されたリスト)\n",
    "    returnList.append(\"usedData\")\n",
    "    returnDict[\"usedData\"] = object\n",
    "    # 使用データ数\n",
    "    returnList.append(\"numOfData\")\n",
    "    returnDict[\"numOfData\"] = \"int16\"\n",
    "    # 固定したもの(\"Process\" or \"Class\")\n",
    "    returnList.append(\"ProcessOrClass\")\n",
    "    returnDict[\"ProcessOrClass\"] = str\n",
    "    # 固定したもの(プロセス数(数値)or問題サイズ(数値))\n",
    "    returnList.append(\"fixed\")\n",
    "    returnDict[\"fixed\"] = \"float32\"\n",
    "    # 予測対象プロセス数\n",
    "    returnList.append(\"targetNumOfProcess\")\n",
    "    returnDict[\"targetNumOfProcess\"] = \"int16\"\n",
    "    # 予測対象問題サイズ（数値）\n",
    "    returnList.append(\"targetNumOfProblemSize\")\n",
    "    returnDict[\"targetNumOfProblemSize\"] = \"float32\"\n",
    "    # 予測対象問題サイズ\n",
    "    returnList.append(\"targetProblemSize\")\n",
    "    returnDict[\"targetProblemSize\"] = str\n",
    "    # 予測対象関数コール回数\n",
    "    returnList.append(\"targetNumOfFunctionCall\")\n",
    "    returnDict[\"targetNumOfFunctionCall\"] = \"float32\"\n",
    "    # 線形モデルのオブジェクト\n",
    "    returnList.append(\"objectLinModel\")\n",
    "    returnDict[\"objectLinModel\"] = object\n",
    "    # 線形モデルのMAPE\n",
    "    returnList.append(\"MAPEOfLinModel\")\n",
    "    returnDict[\"MAPEOfLinModel\"] = \"float32\"\n",
    "    # 反比例モデルのオブジェクト\n",
    "    returnList.append(\"objectIpModel\")\n",
    "    returnDict[\"objectIpModel\"] = object\n",
    "    # 反比例モデルのMAPE\n",
    "    returnList.append(\"MAPEOfIpModel\")\n",
    "    returnDict[\"MAPEOfIpModel\"] = \"float32\"\n",
    "    # 対数モデルのオブジェクト\n",
    "    returnList.append(\"objectLogModel\")\n",
    "    returnDict[\"objectLogModel\"] = object\n",
    "    # 対数モデルのMAPE\n",
    "    returnList.append(\"MAPEOfLogModel\")\n",
    "    returnDict[\"MAPEOfLogModel\"] = \"float32\"\n",
    "    # 線形飽和モデルのオブジェクト\n",
    "    returnList.append(\"objectBranchModel\")\n",
    "    returnDict[\"objectBranchModel\"] = object\n",
    "    # 線形飽和モデルのMAPE\n",
    "    returnList.append(\"MAPEOfBranchModel\")\n",
    "    returnDict[\"MAPEOfBranchModel\"] = \"float32\"\n",
    "    # 最も相対誤差の小さいモデル名\n",
    "    returnList.append(\"objectBestModelName\")\n",
    "    returnDict[\"objectBestModelName\"] = object\n",
    "    # 最小の相対誤差\n",
    "    returnList.append(\"relativeErrorOfBestModel\")\n",
    "    returnDict[\"relativeErrorOfBestModel\"] = \"float32\"\n",
    "    if(dataType==True):\n",
    "        return(returnDict)\n",
    "    else:\n",
    "        return(returnList)\n",
    "\n",
    "# 使用例\n",
    "# columnNames = return_numOfColumns()\n",
    "# df_sample = pd.DataFrame(columns=columnNames)\n",
    "# df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマーク名・関数名・プロセス数・問題サイズを指定することで、その条件での関数コール回数を取得する関数\n",
    "\n",
    "def returnSpecificData(benchmarkName=\"cg\", functionName=\".TAU_application\", process=256, benchmarkClass=\"D\"):\n",
    "    targetRawDF = returnRawDF(Benchmark=benchmarkName, functionName=functionName, benchmarkClass=[benchmarkClass], FixedProcess=process, Processes=[process], FixedBenchmarkClass=benchmarkClass)\n",
    "    return targetRawDF.iat[0, 0]\n",
    "# returnSpecificData(benchmarkName=\"mg\", functionName=\"BUBBLE\", process=256, benchmarkClass=\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_numOfColumns()でのカラム名としてのモデル名、モデルのメソッドModelName()が返すモデル名を相互的なキー・バリューとした辞書を返す関数\n",
    "def returnDictModelNames():\n",
    "    returnDict = {}\n",
    "    # カラム名をキー・モデルが返すモデル名をバリュー\n",
    "    returnDict[\"objectLinModel\"] = \"ModelLin\"\n",
    "    returnDict[\"objectIpModel\"] = \"ModelIp\"\n",
    "    returnDict[\"objectLogModel\"] = \"ModelLog\"\n",
    "    returnDict[\"objectBranchModel\"] = \"ModelBranch\"\n",
    "    # カラム名をキー・モデルが返すモデル名をバリュー\n",
    "    returnDict[\"ModelLin\"] = \"objectLinModel\"\n",
    "    returnDict[\"ModelIp\"] = \"objectIpModel\"\n",
    "    returnDict[\"ModelLog\"] = \"objectLogModel\"\n",
    "    returnDict[\"ModelBranch\"] = \"objectBranchModel\"\n",
    "    \n",
    "    returnDict(returnDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を集計するためのDFに挿入するSeriesを作成する関数\n",
    "def returnSeriesOfData(benchmarkName=\"benhmarkName\", functionName=\"functionName\", raw_x = [1,2,3], raw_y=[1,2,3], fix=\"Class\", fixed=\"B\", targetProcess=256, targetProblemSize=\"B\"):\n",
    "    \n",
    "    dataSeries = pd.Series(index=return_numOfColumns(), dtype=object)\n",
    "    dataSeries[\"benchmarkName\"] = benchmarkName\n",
    "    dataSeries[\"functionName\"] = functionName\n",
    "    dataSeries[\"usedData\"] = raw_y\n",
    "    dataSeries[\"numOfData\"] = len(raw_y)\n",
    "    dataSeries[\"ProcessOrClass\"] = fix\n",
    "    dataSeries[\"fixed\"] = ConvertBencharkClass_inNPB(fixed)\n",
    "    targetNumOfProcess = 256\n",
    "    dataSeries[\"targetNumOfProcess\"] = 256\n",
    "    targetProblemsize = fixed\n",
    "    targetNumOfProblemSize = ConvertBencharkClass_inNPB(targetProblemsize)\n",
    "    dataSeries[\"targetNumOfProblemSize\"] = targetNumOfProblemSize\n",
    "    dataSeries[\"targetProblemSize\"] = targetProblemsize\n",
    "\n",
    "    dataSeries[\"targetNumOfFunctionCall\"] = returnSpecificData(\n",
    "        benchmarkName=benchmarkName, functionName=functionName, process=targetNumOfProcess, benchmarkClass=targetProblemsize)\n",
    "\n",
    "    # MAPE の算出には mape_score()を用いる\n",
    "    # mape_score()の返り値の単位は％\n",
    "\n",
    "    raw_x\n",
    "    raw_y\n",
    "    # 線形モデル\n",
    "    modelLin = ModelLin_mk2(train_x=raw_x, train_y=raw_y)\n",
    "    modelLin.calc_lr()\n",
    "    predicted_y = modelLin.predict(raw_x)\n",
    "    dataSeries[\"objectLinModel\"] = modelLin\n",
    "    dataSeries[\"MAPEOfLinModel\"] = mape_score(predicted_y, raw_y)\n",
    "    # 反比例モデル\n",
    "    modelIp = ModelIp_mk2(train_x=raw_x, train_y=raw_y)\n",
    "    modelIp.calc_lr()\n",
    "    predicted_y = modelIp.predict(raw_x)\n",
    "    dataSeries[\"objectIpModel\"] = modelIp\n",
    "    dataSeries[\"MAPEOfIpModel\"] = mape_score(predicted_y, raw_y)\n",
    "    # 対数モデル\n",
    "    modelLog = ModelLog10_mk2(train_x=raw_x, train_y=raw_y)\n",
    "    modelLog.calc_lr()\n",
    "    predicted_y = modelLog.predict(raw_x)\n",
    "    dataSeries[\"objectLogModel\"] = modelLog\n",
    "    dataSeries[\"MAPEOfLogModel\"] = mape_score(predicted_y, raw_y)\n",
    "    # 分岐モデル\n",
    "    modelBranch = ModelBranch_mk2(train_x=raw_x, train_y=raw_y)\n",
    "    modelBranch.calc_lr()\n",
    "    predicted_y = modelBranch.predict(raw_x)\n",
    "    dataSeries[\"objectBranchModel\"] = modelBranch\n",
    "    dataSeries[\"MAPEOfBranchModel\"] = mape_score(predicted_y, raw_y)\n",
    "    # 最適なモデルのモデルのモデル名・MAPE値の算出\n",
    "    listToCalcBestModel = {}\n",
    "    listToCalcBestModel[dataSeries[\"objectLinModel\"].ModelName()] = dataSeries[\"MAPEOfLinModel\"]\n",
    "    listToCalcBestModel[dataSeries[\"objectIpModel\"].ModelName()] = dataSeries[\"MAPEOfIpModel\"]\n",
    "    listToCalcBestModel[dataSeries[\"objectLogModel\"].ModelName()] = dataSeries[\"MAPEOfLogModel\"]\n",
    "    listToCalcBestModel[dataSeries[\"objectBranchModel\"].ModelName()] = dataSeries[\"MAPEOfBranchModel\"]\n",
    "    minMAPE = min(listToCalcBestModel.values())\n",
    "    dataSeries[\"MAPEOfBestModel\"] = minMAPE\n",
    "    dataSeries[\"objectBestModelName\"] = [k for k, v in listToCalcBestModel.items() if v == minMAPE][0]\n",
    "    # relativeErrorOfBestModelへのデータ格納処理\n",
    "    # これには、学習したモデルから対象となる関数コール回数を予測し、予測値と実測値の相対誤差を入れる\n",
    "    \n",
    "    \n",
    "    return(dataSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数noNaNDFのデータに基づいて各関数での予測精度などを保持したDFを返す関数\n",
    "# 引数詳細\n",
    "# benchmark:ベンチマーク名\n",
    "# noNaNDF:生の実験データから NaN が含まれる関数の行を削除したDF\n",
    "# targetNumOfProcess:対象となるプロセス数を格納した変数\n",
    "# targetProblemSize:対象となる問題サイズを格納した変数\n",
    "# fix:\"Process\" or \"Class\"\n",
    "def return_calculatedDF(benchmark :str, noNaNDF, targetNumOfProcess=256, targetProblemSize=\"B\", fix=\"Class\"):\n",
    "\n",
    "    # 取得した実験データから NaN が含まれない関数名のリスト\n",
    "    functionNames = noNaNDF.index.tolist()\n",
    "    # プロセス数のリスト\n",
    "    processes = noNaNDF.columns.tolist()\n",
    "    # 集計するためのDF\n",
    "    calculatedDF = pd.DataFrame(columns=return_numOfColumns())\n",
    "    calculatedDF = calculatedDF.astype(return_numOfColumns(dataType=True))\n",
    "\n",
    "\n",
    "    for functionName in functionNames:\n",
    "        for i in reversed(range(1, len(processes))):\n",
    "\n",
    "            indexSeparator = i+1\n",
    "            raw_x = noNaNDF.loc[functionName].tolist()[:indexSeparator]\n",
    "            raw_y = processes[:indexSeparator]\n",
    "            targetNumOfProcess = 256\n",
    "            targetProblemSize = fixedBenchmarkClass\n",
    "            dataSeries = returnSeriesOfData(benchmarkName=benchmark, functionName=functionName, raw_x=raw_x, raw_y=raw_y,\n",
    "                                            fix=fix, fixed=fixedBenchmarkClass, targetProcess=targetNumOfProcess, targetProblemSize=targetProblemSize)\n",
    "            calculatedDF = calculatedDF.append(dataSeries, ignore_index=True)\n",
    "    return(calculatedDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
